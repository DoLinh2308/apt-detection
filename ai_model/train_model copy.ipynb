{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e184fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Current RAM Usage: 0.07 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import IPython\n",
    "\n",
    "# ‚úÖ Function to Monitor RAM Usage\n",
    "def check_ram():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_usage = process.memory_info().rss / 1024 ** 3  # Convert to GB\n",
    "    print(f\"üîç Current RAM Usage: {ram_usage:.2f} GB\")\n",
    "\n",
    "# ‚úÖ Function to Clean RAM\n",
    "def clean_ram():\n",
    "    gc.collect()  # ‚úÖ Free Unused Memory\n",
    "    IPython.display.clear_output(wait=True)  # ‚úÖ Clear Output (Optional)\n",
    "    check_ram()  # ‚úÖ Show Updated RAM Usage\n",
    "\n",
    "# Example Usage:\n",
    "# Run this after every large operation (like merging datasets, model training)\n",
    "check_ram()  # Before cleaning\n",
    "clean_ram()  # After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6525e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features Used for Training:\n",
      "['Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Protocol_0', 'Protocol_6', 'Protocol_17']\n",
      "\n",
      "‚úÖ Feature names saved to feature_list.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = \"dataset/working1/cleaned_dataset.csv\"  # Change this path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Get the feature names (excluding the label column)\n",
    "feature_names = df.columns.tolist()\n",
    "feature_names.remove(\"Label\")\n",
    "\n",
    "print(\"‚úÖ Features Used for Training:\")\n",
    "print(feature_names)\n",
    "\n",
    "# Save feature names to a text file (to transfer to Ubuntu VM)\n",
    "with open(\"dataset/working1/feature_list.txt\", \"w\") as f:\n",
    "    for feature in feature_names:\n",
    "        f.write(feature + \"\\n\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature names saved to feature_list.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1914df",
   "metadata": {},
   "source": [
    "NORMALIZE DATA AND SPLIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang t·∫£i Dataset t·ª´: dataset/working2/cleaned_dataset.csv ...\n",
      "‚úÖ Dataset ƒë√£ t·∫£i! Shape: (5881823, 73)\n",
      "\n",
      "‚úÖ C·ªôt 'Label' ƒë√£ ƒë∆∞·ª£c m√£ h√≥a sang d·∫°ng s·ªë.\n",
      "Ph√¢n b·ªë c√°c l·ªõp sau khi m√£ h√≥a v√† lo·∫°i b·ªè NaN (n·∫øu c√≥):\n",
      "Label\n",
      "0     5087280\n",
      "1      144535\n",
      "2      217401\n",
      "3        1730\n",
      "4       41406\n",
      "5      145199\n",
      "6          55\n",
      "7        9908\n",
      "8          53\n",
      "9      139341\n",
      "10      94048\n",
      "11        555\n",
      "12        228\n",
      "13         84\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîÑ ƒêang chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm th·ª≠...\n",
      "‚úÖ Chia d·ªØ li·ªáu ho√†n t·∫•t!\n",
      "   K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (df_train): (4705458, 73)\n",
      "   K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm th·ª≠ (df_test): (1176365, 73)\n",
      "\n",
      "üîÑ ƒêang chu·∫©n h√≥a c√°c c·ªôt s·ªë h·ªçc...\n",
      "‚úÖ Chu·∫©n h√≥a ho√†n t·∫•t!\n",
      "‚úÖ Scaler ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: dataset/working2\\scaler.pkl\n",
      "\n",
      "üîç Ph√¢n b·ªë nh√£n trong T·∫≠p Hu·∫•n luy·ªán TR∆Ø·ªöC khi x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng:\n",
      "0     4069824\n",
      "1      115628\n",
      "2      173921\n",
      "3        1384\n",
      "4       33125\n",
      "5      116159\n",
      "6          44\n",
      "7        7926\n",
      "8          43\n",
      "9      111473\n",
      "10      75238\n",
      "11        444\n",
      "12        182\n",
      "13         67\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üöÄ √Åp d·ª•ng ph∆∞∆°ng ph√°p x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng: enhanced_smote...\n",
      "Chi·∫øn l∆∞·ª£c SMOTE (s·ªë m·∫´u m·ª•c ti√™u): {3: 25000, 6: 25000, 8: 25000, 11: 25000, 12: 25000, 13: 25000}\n",
      "\n",
      "‚úÖ X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng b·∫±ng 'enhanced_smote' ho√†n t·∫•t!\n",
      "   K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán SAU khi resample: X=(4853294, 72), y=(4853294,)\n",
      "\n",
      "üîç Ph√¢n b·ªë nh√£n trong T·∫≠p Hu·∫•n luy·ªán SAU khi x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng:\n",
      "Label\n",
      "0     4069824\n",
      "1      115628\n",
      "2      173921\n",
      "3       25000\n",
      "4       33125\n",
      "5      116159\n",
      "6       25000\n",
      "7        7926\n",
      "8       25000\n",
      "9      111473\n",
      "10      75238\n",
      "11      25000\n",
      "12      25000\n",
      "13      25000\n",
      "Name: count, dtype: int64\n",
      "   Ph·∫°m vi ƒë·∫∑c tr∆∞ng sau khi resample: 0.0000 ƒë·∫øn 1.0000\n",
      "\n",
      "‚úÖ D·ªØ li·ªáu hu·∫•n luy·ªán (ƒë√£ c√¢n b·∫±ng) ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: dataset/working2\\train_balanced_by_enhanced_smote.csv\n",
      "‚úÖ D·ªØ li·ªáu ki·ªÉm th·ª≠ (ƒë√£ chu·∫©n h√≥a) ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: dataset/working2\\test1.csv\n",
      "\n",
      "üîç Ph√¢n b·ªë nh√£n trong T·∫≠p Hu·∫•n luy·ªán ƒë√£ l∆∞u (df_train_balanced):\n",
      "Label\n",
      "0     4069824\n",
      "1      115628\n",
      "2      173921\n",
      "3       25000\n",
      "4       33125\n",
      "5      116159\n",
      "6       25000\n",
      "7        7926\n",
      "8       25000\n",
      "9      111473\n",
      "10      75238\n",
      "11      25000\n",
      "12      25000\n",
      "13      25000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîç Ph√¢n b·ªë nh√£n trong T·∫≠p Ki·ªÉm th·ª≠ ƒë√£ l∆∞u (df_test_scaled):\n",
      "Label\n",
      "0     1017456\n",
      "1       28907\n",
      "2       43480\n",
      "3         346\n",
      "4        8281\n",
      "5       29040\n",
      "6          11\n",
      "7        1982\n",
      "8          10\n",
      "9       27868\n",
      "10      18810\n",
      "11        111\n",
      "12         46\n",
      "13         17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ D·ªçn d·∫πp b·ªô nh·ªõ ho√†n t·∫•t.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import joblib # ƒê·ªÉ l∆∞u v√† t·∫£i scaler\n",
    "import os # ƒê·ªÉ l√†m vi·ªác v·ªõi ƒë∆∞·ªùng d·∫´n file\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import c√°c ph∆∞∆°ng ph√°p x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng t·ª´ imblearn\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# --- Thi·∫øt l·∫≠p chung ---\n",
    "cleaned_file_path = \"/content/drive/MyDrive/ai_model/cleaned_dataset.csv\"\n",
    "output_data_dir = \"/content/drive/MyDrive/ai_model/working2\"\n",
    "os.makedirs(output_data_dir, exist_ok=True)\n",
    "scaler_path = os.path.join(output_data_dir, \"scaler.pkl\")\n",
    "test_data_output_path = os.path.join(output_data_dir, \"test1.csv\")\n",
    "\n",
    "# --- 1. T·∫£i D·ªØ li·ªáu ƒê√£ L√†m S·∫°ch ---\n",
    "print(f\"üìÇ ƒêang t·∫£i Dataset t·ª´: {cleaned_file_path} ...\")\n",
    "try:\n",
    "    df = pd.read_csv(cleaned_file_path)\n",
    "    print(f\"‚úÖ Dataset ƒë√£ t·∫£i! Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file '{cleaned_file_path}'. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªñI kh√¥ng x√°c ƒë·ªãnh khi t·∫£i d·ªØ li·ªáu: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. M√£ h√≥a C·ªôt Nh√£n ---\n",
    "label_mapping = {\n",
    "    \"Benign\": 0, \"Bot\": 1, \"DDOS attack-HOIC\": 2, \"DDOS attack-LOIC-UDP\": 3,\n",
    "    \"DoS attacks-GoldenEye\": 4, \"DoS attacks-Hulk\": 5, \"DoS attacks-SlowHTTPTest\": 6,\n",
    "    \"DoS attacks-Slowloris\": 7, \"FTP-BruteForce\": 8, \"Infilteration\": 9, # C√≥ th·ªÉ l√† \"Infiltration\"\n",
    "    \"SSH-Bruteforce\": 10, \"Brute Force -Web\": 11, \"Brute Force -XSS\": 12, \"SQL Injection\": 13\n",
    "}\n",
    "if \"Label\" not in df.columns:\n",
    "    print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y c·ªôt 'Label' trong DataFrame. C√°c c·ªôt hi·ªán c√≥: {df.columns.tolist()}\")\n",
    "    exit()\n",
    "df[\"Label\"] = df[\"Label\"].map(label_mapping)\n",
    "if df[\"Label\"].isnull().any():\n",
    "    print(\"\\n‚ö†Ô∏è C·∫¢NH B√ÅO: C√≥ gi√° tr·ªã NaN trong c·ªôt 'Label' sau khi map. C√°c h√†ng n√†y s·∫Ω b·ªã lo·∫°i b·ªè.\")\n",
    "    df.dropna(subset=[\"Label\"], inplace=True)\n",
    "df[\"Label\"] = df[\"Label\"].astype(int)\n",
    "print(\"\\n‚úÖ C·ªôt 'Label' ƒë√£ ƒë∆∞·ª£c m√£ h√≥a sang d·∫°ng s·ªë.\")\n",
    "print(\"Ph√¢n b·ªë c√°c l·ªõp sau khi m√£ h√≥a v√† lo·∫°i b·ªè NaN (n·∫øu c√≥):\")\n",
    "print(df[\"Label\"].value_counts().sort_index())\n",
    "\n",
    "# --- 3. Chia D·ªØ li·ªáu Train-Test ---\n",
    "print(\"\\nüîÑ ƒêang chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm th·ª≠...\")\n",
    "if df.empty or \"Label\" not in df.columns:\n",
    "    print(\"‚ùå L·ªñI: DataFrame r·ªóng ho·∫∑c thi·∫øu c·ªôt 'Label' tr∆∞·ªõc khi chia train-test.\")\n",
    "    exit()\n",
    "\n",
    "# Ki·ªÉm tra xem t·∫•t c·∫£ c√°c l·ªõp c√≥ ƒë·ªß m·∫´u ƒë·ªÉ stratify kh√¥ng\n",
    "label_counts_for_split = df[\"Label\"].value_counts()\n",
    "if (label_counts_for_split < 2).any() and (label_counts_for_split < (1/0.2)): # 0.2 l√† test_size\n",
    "    print(f\"‚ö†Ô∏è C·∫¢NH B√ÅO: M·ªôt s·ªë l·ªõp c√≥ √≠t h∆°n 2 m·∫´u ho·∫∑c kh√¥ng ƒë·ªß m·∫´u cho stratify v·ªõi test_size=0.2: \\n{label_counts_for_split[label_counts_for_split < 2]}\")\n",
    "    print(\"   C√¢n nh·∫Øc vi·ªác g·ªôp c√°c l·ªõp hi·∫øm n√†y ho·∫∑c lo·∫°i b·ªè ch√∫ng n·∫øu kh√¥ng th·ªÉ stratify.\")\n",
    "    # Ho·∫∑c th·ª≠ kh√¥ng d√πng stratify n·∫øu b·∫Øt bu·ªôc, nh∆∞ng kh√¥ng khuy·∫øn kh√≠ch cho d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng\n",
    "    # X = df.drop(columns=[\"Label\"])\n",
    "    # y = df[\"Label\"]\n",
    "    # X_train_df, X_test_df, y_train_series, y_test_series = train_test_split(X, y, test_size=0.2, random_state=20) # B·ªè stratify\n",
    "    # print(\"   ƒê√£ th·ª≠ chia kh√¥ng d√πng stratify.\")\n",
    "    # T·∫°m th·ªùi d·ª´ng n·∫øu c√≥ l·ªõp qu√° √≠t m·∫´u g√¢y l·ªói stratify\n",
    "    if (label_counts_for_split < 2).any():\n",
    "         print(\"   M·ªôt s·ªë l·ªõp c√≥ √≠t h∆°n 2 m·∫´u, kh√¥ng th·ªÉ stratify. D·ª´ng ch∆∞∆°ng tr√¨nh.\")\n",
    "         exit()\n",
    "\n",
    "\n",
    "try:\n",
    "    X = df.drop(columns=[\"Label\"])\n",
    "    y = df[\"Label\"]\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=20, stratify=y)\n",
    "    X_train_df = df_train.drop(columns=[\"Label\"])\n",
    "    y_train_series = df_train[\"Label\"]\n",
    "    X_test_df = df_test.drop(columns=[\"Label\"])\n",
    "    y_test_series = df_test[\"Label\"]\n",
    "\n",
    "    print(f\"‚úÖ Chia d·ªØ li·ªáu ho√†n t·∫•t!\")\n",
    "    print(f\"   K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (df_train): {df_train.shape}\")\n",
    "    print(f\"   K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm th·ª≠ (df_test): {df_test.shape}\")\n",
    "except ValueError as e_split:\n",
    "    print(f\"‚ùå L·ªñI khi chia train-test (c√≥ th·ªÉ do stratify v·ªõi l·ªõp qu√° √≠t m·∫´u): {e_split}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. Chu·∫©n h√≥a D·ªØ li·ªáu S·ªë h·ªçc ---\n",
    "print(\"\\nüîÑ ƒêang chu·∫©n h√≥a c√°c c·ªôt s·ªë h·ªçc...\")\n",
    "numeric_cols = X_train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "if not numeric_cols:\n",
    "    print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y c·ªôt s·ªë h·ªçc n√†o trong X_train_df ƒë·ªÉ chu·∫©n h√≥a.\")\n",
    "    exit()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_df[numeric_cols])\n",
    "X_train_scaled_df = X_train_df.copy()\n",
    "X_test_scaled_df = X_test_df.copy()\n",
    "X_train_scaled_df[numeric_cols] = scaler.transform(X_train_df[numeric_cols])\n",
    "X_test_scaled_df[numeric_cols] = scaler.transform(X_test_df[numeric_cols])\n",
    "print(\"‚úÖ Chu·∫©n h√≥a ho√†n t·∫•t!\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"‚úÖ Scaler ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {scaler_path}\")\n",
    "df_train_scaled = X_train_scaled_df.copy()\n",
    "df_train_scaled[\"Label\"] = y_train_series.values\n",
    "df_test_scaled = X_test_scaled_df.copy()\n",
    "df_test_scaled[\"Label\"] = y_test_series.values\n",
    "\n",
    "# --- 5. X·ª≠ l√Ω M·∫•t c√¢n b·∫±ng D·ªØ li·ªáu ---\n",
    "X_train_features_np = df_train_scaled.drop(columns=[\"Label\"]).values\n",
    "y_train_labels_np = df_train_scaled[\"Label\"].values\n",
    "\n",
    "# --- TR·ª∞C QUAN H√ìA PH√ÇN PH·ªêI L·ªöP TR∆Ø·ªöC V√Ä SAU KHI C√ÇN B·∫∞NG D·ªÆ LI·ªÜU ---\n",
    "print(\"\\nüìä ƒêang t·∫°o bi·ªÉu ƒë·ªì tr·ª±c quan h√≥a ph√¢n ph·ªëi l·ªõp...\")\n",
    "\n",
    "# Gi·∫£ ƒë·ªãnh r·∫±ng 'label_mapping' ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü ph·∫ßn tr∆∞·ªõc c·ªßa script c·ªßa b·∫°n\n",
    "# V√≠ d·ª•: label_mapping = { \"Benign\": 0, \"Bot\": 1, ... }\n",
    "# T·∫°o inverse_label_mapping ƒë·ªÉ l·∫•y t√™n l·ªõp cho bi·ªÉu ƒë·ªì\n",
    "if 'label_mapping' in locals() or 'label_mapping' in globals():\n",
    "    inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: Bi·∫øn 'label_mapping' kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y. Bi·ªÉu ƒë·ªì s·∫Ω s·ª≠ d·ª•ng nh√£n s·ªë.\")\n",
    "    # T·∫°o m·ªôt √°nh x·∫° gi·∫£ n·∫øu kh√¥ng c√≥, ƒë·ªÉ code kh√¥ng b·ªã l·ªói\n",
    "    # L·∫•y t·∫•t c·∫£ c√°c nh√£n duy nh·∫•t t·ª´ c·∫£ hai t·∫≠p d·ªØ li·ªáu ƒë·ªÉ t·∫°o √°nh x·∫° gi·∫£\n",
    "    unique_labels_before = pd.Series(y_train_labels_np).unique()\n",
    "    unique_labels_after = df_train_balanced[\"Label\"].unique()\n",
    "    all_unique_labels = sorted(list(set(list(unique_labels_before) + list(unique_labels_after))))\n",
    "    inverse_label_mapping = {label: f\"L·ªõp {label}\" for label in all_unique_labels}\n",
    "\n",
    "\n",
    "# 1. Tr·ª±c quan h√≥a Ph√¢n ph·ªëi L·ªõp TR∆Ø·ªöC khi √°p d·ª•ng SMOTE (tr√™n t·∫≠p hu·∫•n luy·ªán g·ªëc)\n",
    "#    'y_train_labels_np' l√† Series/array ch·ª©a nh√£n c·ªßa t·∫≠p hu·∫•n luy·ªán g·ªëc (ƒë√£ m√£ h√≥a s·ªë)\n",
    "train_label_counts_before_smote = pd.Series(y_train_labels_np).value_counts().sort_index()\n",
    "train_label_names_before_smote = train_label_counts_before_smote.index.map(inverse_label_mapping)\n",
    "\n",
    "plt.figure(figsize=(14, 8)) # K√≠ch th∆∞·ªõc bi·ªÉu ƒë·ªì\n",
    "sns.barplot(x=train_label_names_before_smote, y=train_label_counts_before_smote.values, palette=\"viridis\")\n",
    "plt.title('Ph√¢n ph·ªëi c√°c l·ªõp trong t·∫≠p Hu·∫•n luy·ªán TR∆Ø·ªöC khi X·ª≠ l√Ω M·∫•t c√¢n b·∫±ng', fontsize=16)\n",
    "plt.xlabel('Lo·∫°i T·∫•n C√¥ng / Benign', fontsize=14)\n",
    "plt.ylabel('S·ªë l∆∞·ª£ng M·∫´u', fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "# L∆∞u bi·ªÉu ƒë·ªì (t√πy ch·ªçn)\n",
    "path_before_smote_plot = os.path.join(output_data_dir, f\"distribution_before_balancing_{balancing_method_choice}.png\")\n",
    "plt.savefig(path_before_smote_plot, dpi=150)\n",
    "print(f\"‚úÖ Bi·ªÉu ƒë·ªì TR∆Ø·ªöC khi c√¢n b·∫±ng ƒë√£ l∆∞u t·∫°i: {path_before_smote_plot}\")\n",
    "plt.show()\n",
    "print(\"\\nüîç Ph√¢n b·ªë nh√£n trong T·∫≠p Hu·∫•n luy·ªán TR∆Ø·ªöC khi x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng:\")\n",
    "current_class_counts_train = pd.Series(y_train_labels_np).value_counts().sort_index()\n",
    "print(current_class_counts_train)\n",
    "\n",
    "balancing_method_choice = 'enhanced_smote'\n",
    "X_resampled_np = X_train_features_np.copy()\n",
    "y_resampled_np = y_train_labels_np.copy()\n",
    "print(f\"\\nüöÄ √Åp d·ª•ng ph∆∞∆°ng ph√°p x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng: {balancing_method_choice}...\")\n",
    "\n",
    "# Tham s·ªë k_neighbors chung cho c√°c ph∆∞∆°ng ph√°p d·ª±a tr√™n SMOTE\n",
    "# SMOTE y√™u c·∫ßu s·ªë m·∫´u c·ªßa m·ªôt l·ªõp ph·∫£i > k_neighbors ƒë·ªÉ c√≥ th·ªÉ oversample l·ªõp ƒë√≥.\n",
    "# N·∫øu m·ªôt l·ªõp c√≥ s·ªë m·∫´u <= k_neighbors, n√≥ s·∫Ω kh√¥ng ƒë∆∞·ª£c SMOTE.\n",
    "DEFAULT_K_NEIGHBORS = 5\n",
    "\n",
    "if balancing_method_choice == 'enhanced_smote':\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    target_samples_smote = {}\n",
    "    minority_classes_to_boost = [3, 6, 8, 9, 11, 12, 13]\n",
    "    desired_count_for_minorities = 25000\n",
    "\n",
    "    for cls_label in minority_classes_to_boost:\n",
    "        num_samples_class = current_class_counts_train.get(cls_label, 0)\n",
    "        if num_samples_class > 0 and num_samples_class < desired_count_for_minorities:\n",
    "            if num_samples_class > DEFAULT_K_NEIGHBORS: # ƒê·ªß m·∫´u ƒë·ªÉ SMOTE v·ªõi k_neighbors hi·ªán t·∫°i\n",
    "                target_samples_smote[cls_label] = desired_count_for_minorities\n",
    "            else:\n",
    "                print(f\"L∆∞u √Ω (SMOTE): L·ªõp {cls_label} c√≥ {num_samples_class} m·∫´u, kh√¥ng ƒë·ªß cho k_neighbors={DEFAULT_K_NEIGHBORS}. \"\n",
    "                      f\"L·ªõp n√†y s·∫Ω kh√¥ng ƒë∆∞·ª£c oversample b·ªüi SMOTE v·ªõi k_neighbors n√†y.\")\n",
    "\n",
    "    if not target_samples_smote:\n",
    "        print(\"Kh√¥ng c√≥ l·ªõp n√†o ph√π h·ª£p ƒë·ªÉ SMOTE theo chi·∫øn l∆∞·ª£c hi·ªán t·∫°i. D·ªØ li·ªáu gi·ªØ nguy√™n.\")\n",
    "    else:\n",
    "        print(f\"Chi·∫øn l∆∞·ª£c SMOTE (s·ªë m·∫´u m·ª•c ti√™u): {target_samples_smote}\")\n",
    "        smote_sampler = SMOTE(sampling_strategy=target_samples_smote, random_state=42, k_neighbors=DEFAULT_K_NEIGHBORS)\n",
    "        X_resampled_np, y_resampled_np = smote_sampler.fit_resample(X_train_features_np, y_train_labels_np)\n",
    "\n",
    "elif balancing_method_choice == 'adasyn':\n",
    "    from imblearn.over_sampling import ADASYN\n",
    "    target_samples_adasyn = {}\n",
    "    minority_classes_to_boost = [3, 6, 8, 9, 11, 12, 13]\n",
    "    desired_count_for_minorities = 25000\n",
    "\n",
    "    for cls_label in minority_classes_to_boost:\n",
    "        num_samples_class = current_class_counts_train.get(cls_label, 0)\n",
    "        if num_samples_class > 0 and num_samples_class < desired_count_for_minorities:\n",
    "            # ADASYN c≈©ng d·ª±a tr√™n k-NN (tham s·ªë n_neighbors), t∆∞∆°ng t·ª± k_neighbors c·ªßa SMOTE\n",
    "            if num_samples_class > DEFAULT_K_NEIGHBORS: # ADASYN m·∫∑c ƒë·ªãnh n_neighbors=5\n",
    "                target_samples_adasyn[cls_label] = desired_count_for_minorities\n",
    "            else:\n",
    "                print(f\"L∆∞u √Ω (ADASYN): L·ªõp {cls_label} c√≥ {num_samples_class} m·∫´u, kh√¥ng ƒë·ªß cho n_neighbors={DEFAULT_K_NEIGHBORS}. \"\n",
    "                      f\"L·ªõp n√†y s·∫Ω kh√¥ng ƒë∆∞·ª£c oversample b·ªüi ADASYN v·ªõi n_neighbors n√†y.\")\n",
    "\n",
    "    if not target_samples_adasyn:\n",
    "        print(\"Kh√¥ng c√≥ l·ªõp n√†o ph√π h·ª£p ƒë·ªÉ ADASYN. D·ªØ li·ªáu gi·ªØ nguy√™n.\")\n",
    "    else:\n",
    "        print(f\"Chi·∫øn l∆∞·ª£c ADASYN (s·ªë m·∫´u m·ª•c ti√™u): {target_samples_adasyn}\")\n",
    "        adasyn_sampler = ADASYN(sampling_strategy=target_samples_adasyn, random_state=42, n_neighbors=DEFAULT_K_NEIGHBORS)\n",
    "        X_resampled_np, y_resampled_np = adasyn_sampler.fit_resample(X_train_features_np, y_train_labels_np)\n",
    "\n",
    "elif balancing_method_choice == 'borderline_smote':\n",
    "    from imblearn.over_sampling import BorderlineSMOTE\n",
    "    target_samples_bsmote = {}\n",
    "    minority_classes_to_boost = [3, 6, 8, 9, 11, 12, 13]\n",
    "    desired_count_for_minorities = 25000\n",
    "\n",
    "    for cls_label in minority_classes_to_boost:\n",
    "        num_samples_class = current_class_counts_train.get(cls_label, 0)\n",
    "        if num_samples_class > 0 and num_samples_class < desired_count_for_minorities:\n",
    "            # BorderlineSMOTE c√≥ k_neighbors (cho SMOTE) v√† m_neighbors (ƒë·ªÉ x√°c ƒë·ªãnh borderline)\n",
    "            # C·∫£ hai ƒë·ªÅu c·∫ßn s·ªë m·∫´u > gi√° tr·ªã c·ªßa ch√∫ng. Gi·ªØ m·ªëc DEFAULT_K_NEIGHBORS cho c·∫£ hai.\n",
    "            if num_samples_class > DEFAULT_K_NEIGHBORS :\n",
    "                target_samples_bsmote[cls_label] = desired_count_for_minorities\n",
    "            else:\n",
    "                print(f\"L∆∞u √Ω (BorderlineSMOTE): L·ªõp {cls_label} c√≥ {num_samples_class} m·∫´u, kh√¥ng ƒë·ªß cho k_neighbors/m_neighbors={DEFAULT_K_NEIGHBORS}. \"\n",
    "                      f\"L·ªõp n√†y s·∫Ω kh√¥ng ƒë∆∞·ª£c oversample.\")\n",
    "    if not target_samples_bsmote:\n",
    "        print(\"Kh√¥ng c√≥ l·ªõp n√†o ph√π h·ª£p ƒë·ªÉ BorderlineSMOTE. D·ªØ li·ªáu gi·ªØ nguy√™n.\")\n",
    "    else:\n",
    "        print(f\"Chi·∫øn l∆∞·ª£c BorderlineSMOTE (s·ªë m·∫´u m·ª•c ti√™u): {target_samples_bsmote}\")\n",
    "        bsmote_sampler = BorderlineSMOTE(sampling_strategy=target_samples_bsmote, random_state=42,\n",
    "                                         k_neighbors=DEFAULT_K_NEIGHBORS, m_neighbors=min(10, DEFAULT_K_NEIGHBORS*2), kind='borderline-1')\n",
    "        X_resampled_np, y_resampled_np = bsmote_sampler.fit_resample(X_train_features_np, y_train_labels_np)\n",
    "\n",
    "elif balancing_method_choice in ['smoteenn', 'smotetomek']:\n",
    "    # Chi·∫øn l∆∞·ª£c SMOTE b√™n trong cho c√°c ph∆∞∆°ng ph√°p k·∫øt h·ª£p\n",
    "    smote_strategy_for_combine = {}\n",
    "    minority_classes_to_boost_combine = [3, 6, 8, 9, 11, 12, 13]\n",
    "    desired_count_for_minorities_combine = 30000\n",
    "\n",
    "    for cls_label in minority_classes_to_boost_combine:\n",
    "        num_samples_class = current_class_counts_train.get(cls_label, 0)\n",
    "        if num_samples_class > 0 and num_samples_class < desired_count_for_minorities_combine:\n",
    "            if num_samples_class > DEFAULT_K_NEIGHBORS:\n",
    "                 smote_strategy_for_combine[cls_label] = desired_count_for_minorities_combine\n",
    "            else:\n",
    "                print(f\"L∆∞u √Ω ({balancing_method_choice}): L·ªõp {cls_label} c√≥ {num_samples_class} m·∫´u, kh√¥ng ƒë·ªß cho SMOTE b√™n trong v·ªõi k_neighbors={DEFAULT_K_NEIGHBORS}. \"\n",
    "                      f\"L·ªõp n√†y s·∫Ω kh√¥ng ƒë∆∞·ª£c SMOTE trong b∆∞·ªõc ƒë·∫ßu c·ªßa {balancing_method_choice}.\")\n",
    "\n",
    "    if not smote_strategy_for_combine:\n",
    "        print(f\"Kh√¥ng c√≥ l·ªõp n√†o ph√π h·ª£p cho b∆∞·ªõc SMOTE c·ªßa {balancing_method_choice}. D·ªØ li·ªáu gi·ªØ nguy√™n.\")\n",
    "    else:\n",
    "        print(f\"Chi·∫øn l∆∞·ª£c SMOTE cho {balancing_method_choice}: {smote_strategy_for_combine}\")\n",
    "        if balancing_method_choice == 'smoteenn':\n",
    "            from imblearn.combine import SMOTEENN\n",
    "            combine_sampler = SMOTEENN(sampling_strategy=smote_strategy_for_combine, random_state=42, smote=SMOTE(k_neighbors=DEFAULT_K_NEIGHBORS, random_state=42))\n",
    "        else: # smotetomek\n",
    "            from imblearn.combine import SMOTETomek\n",
    "            combine_sampler = SMOTETomek(sampling_strategy=smote_strategy_for_combine, random_state=42, smote=SMOTE(k_neighbors=DEFAULT_K_NEIGHBORS, random_state=42))\n",
    "        X_resampled_np, y_resampled_np = combine_sampler.fit_resample(X_train_features_np, y_train_labels_np)\n",
    "\n",
    "elif balancing_method_choice == 'random_undersampler':\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    rus_strategy = {}\n",
    "    class_counts_sorted_rus = current_class_counts_train.sort_values(ascending=False)\n",
    "    if len(class_counts_sorted_rus) > 1:\n",
    "        majority_class_label = class_counts_sorted_rus.index[0]\n",
    "        second_majority_count = class_counts_sorted_rus.iloc[1]\n",
    "        target_majority_count = max(int(second_majority_count * 3), 500000)\n",
    "        if current_class_counts_train.get(majority_class_label, 0) > target_majority_count:\n",
    "             rus_strategy[majority_class_label] = target_majority_count\n",
    "    if not rus_strategy:\n",
    "        print(\"Kh√¥ng c·∫ßn Random Undersampling. D·ªØ li·ªáu gi·ªØ nguy√™n.\")\n",
    "    else:\n",
    "        print(f\"Chi·∫øn l∆∞·ª£c RandomUnderSampler: {rus_strategy}\")\n",
    "        rus_sampler = RandomUnderSampler(sampling_strategy=rus_strategy, random_state=42)\n",
    "        X_resampled_np, y_resampled_np = rus_sampler.fit_resample(X_train_features_np, y_train_labels_np)\n",
    "\n",
    "elif balancing_method_choice == 'none':\n",
    "    print(\"Kh√¥ng √°p d·ª•ng ph∆∞∆°ng ph√°p resampling n√†o.\")\n",
    "else:\n",
    "    print(f\"L·ª±a ch·ªçn ph∆∞∆°ng ph√°p '{balancing_method_choice}' kh√¥ng h·ª£p l·ªá. D·ªØ li·ªáu gi·ªØ nguy√™n.\")\n",
    "\n",
    "print(f\"\\n‚úÖ X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng b·∫±ng '{balancing_method_choice}' ho√†n t·∫•t!\")\n",
    "print(f\"   K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán SAU khi resample: X={X_resampled_np.shape}, y={y_resampled_np.shape}\")\n",
    "\n",
    "df_train_balanced = pd.DataFrame(X_resampled_np, columns=X_train_df.columns) # S·ª≠ d·ª•ng t√™n c·ªôt t·ª´ X_train_df\n",
    "df_train_balanced[\"Label\"] = y_resampled_np\n",
    "\n",
    "print(\"\\nüîç Ph√¢n b·ªë nh√£n trong T·∫≠p Hu·∫•n luy·ªán SAU khi x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng:\")\n",
    "print(df_train_balanced[\"Label\"].value_counts().sort_index())\n",
    "if X_resampled_np.size > 0 :\n",
    "    min_val_resampled = df_train_balanced.iloc[:, :-1].values.min()\n",
    "    max_val_resampled = df_train_balanced.iloc[:, :-1].values.max()\n",
    "    print(f\"   Ph·∫°m vi ƒë·∫∑c tr∆∞ng sau khi resample: {min_val_resampled:.4f} ƒë·∫øn {max_val_resampled:.4f}\")\n",
    "\n",
    "# --- 6. L∆∞u D·ªØ li·ªáu ƒê√£ X·ª≠ l√Ω ---\n",
    "output_train_balanced_file_path = os.path.join(output_data_dir, f\"train_balanced_by_{balancing_method_choice}.csv\")\n",
    "try:\n",
    "    df_train_balanced.to_csv(output_train_balanced_file_path, index=False)\n",
    "    print(f\"\\n‚úÖ D·ªØ li·ªáu hu·∫•n luy·ªán (ƒë√£ c√¢n b·∫±ng) ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_train_balanced_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªñI khi l∆∞u file hu·∫•n luy·ªán ƒë√£ c√¢n b·∫±ng: {e}\")\n",
    "try:\n",
    "    df_test_scaled.to_csv(test_data_output_path, index=False)\n",
    "    print(f\"‚úÖ D·ªØ li·ªáu ki·ªÉm th·ª≠ (ƒë√£ chu·∫©n h√≥a) ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {test_data_output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªñI khi l∆∞u file ki·ªÉm th·ª≠: {e}\")\n",
    "\n",
    "# 2. Tr·ª±c quan h√≥a Ph√¢n ph·ªëi L·ªõp SAU khi √°p d·ª•ng SMOTE (tr√™n t·∫≠p hu·∫•n luy·ªán ƒë√£ c√¢n b·∫±ng)\n",
    "#    'df_train_balanced' l√† DataFrame ch·ª©a t·∫≠p hu·∫•n luy·ªán sau khi √°p d·ª•ng k·ªπ thu·∫≠t c√¢n b·∫±ng\n",
    "if not df_train_balanced.empty:\n",
    "    train_label_counts_after_smote = df_train_balanced['Label'].value_counts().sort_index()\n",
    "    train_label_names_after_smote = train_label_counts_after_smote.index.map(inverse_label_mapping)\n",
    "\n",
    "    plt.figure(figsize=(14, 8)) # K√≠ch th∆∞·ªõc bi·ªÉu ƒë·ªì\n",
    "    sns.barplot(x=train_label_names_after_smote, y=train_label_counts_after_smote.values, palette=\"magma\")\n",
    "    plt.title(f'Ph√¢n ph·ªëi c√°c l·ªõp trong t·∫≠p Hu·∫•n luy·ªán SAU khi √°p d·ª•ng {balancing_method_choice.upper()}', fontsize=16)\n",
    "    plt.xlabel('Lo·∫°i T·∫•n C√¥ng / Benign', fontsize=14)\n",
    "    plt.ylabel('S·ªë l∆∞·ª£ng M·∫´u (Sau Resampling)', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    # L∆∞u bi·ªÉu ƒë·ªì (t√πy ch·ªçn)\n",
    "    path_after_smote_plot = os.path.join(output_data_dir, f\"distribution_after_balancing_{balancing_method_choice}.png\")\n",
    "    plt.savefig(path_after_smote_plot, dpi=150)\n",
    "    print(f\"‚úÖ Bi·ªÉu ƒë·ªì SAU khi c√¢n b·∫±ng ƒë√£ l∆∞u t·∫°i: {path_after_smote_plot}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataFrame 'df_train_balanced' r·ªóng, kh√¥ng th·ªÉ t·∫°o bi·ªÉu ƒë·ªì sau khi c√¢n b·∫±ng.\")\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t tr·ª±c quan h√≥a ph√¢n ph·ªëi l·ªõp.\")\n",
    "\n",
    "# --- 7. X√°c minh l·∫°i Ph√¢n b·ªë Nh√£n ---\n",
    "print(\"\\nüîç Ph√¢n b·ªë nh√£n trong T·∫≠p Hu·∫•n luy·ªán ƒë√£ l∆∞u (df_train_balanced):\")\n",
    "print(df_train_balanced[\"Label\"].value_counts().sort_index())\n",
    "print(\"\\nüîç Ph√¢n b·ªë nh√£n trong T·∫≠p Ki·ªÉm th·ª≠ ƒë√£ l∆∞u (df_test_scaled):\")\n",
    "print(df_test_scaled[\"Label\"].value_counts().sort_index())\n",
    "\n",
    "# --- 8. D·ªçn d·∫πp B·ªô nh·ªõ ---\n",
    "del df, X, y, df_train, df_test\n",
    "del X_train_df, y_train_series, X_test_df, y_test_series\n",
    "del X_train_scaled_df, X_test_scaled_df, df_train_scaled\n",
    "del X_train_features_np, y_train_labels_np\n",
    "del X_resampled_np, y_resampled_np, df_train_balanced, df_test_scaled\n",
    "gc.collect()\n",
    "print(\"\\n‚úÖ D·ªçn d·∫πp b·ªô nh·ªõ ho√†n t·∫•t.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8267e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "label_mapping = {\n",
    "    \"Benign\": 0,\n",
    "    \"Bot\": 1,\n",
    "    \"DDOS attack-HOIC\": 2,\n",
    "    \"DDOS attack-LOIC-UDP\": 3,\n",
    "    \"DoS attacks-GoldenEye\": 4,\n",
    "    \"DoS attacks-Hulk\": 5,\n",
    "    \"DoS attacks-SlowHTTPTest\": 6,\n",
    "    \"DoS attacks-Slowloris\": 7,\n",
    "    \"FTP-BruteForce\": 8,\n",
    "    \"Infilteration\": 9,\n",
    "    \"SSH-Bruteforce\": 10,\n",
    "    \"Brute Force -Web\": 11,\n",
    "    \"Brute Force -XSS\": 12,\n",
    "    \"SQL Injection\": 13\n",
    "}\n",
    "torch.save(label_mapping, \"dataset/working1/label_mapping.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b0a49",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7807f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAM and GPU memory cleared!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Clear Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ RAM and GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb04290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Current RAM Usage: 0.51 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import IPython\n",
    "\n",
    "# ‚úÖ Function to Monitor RAM Usage\n",
    "def check_ram():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_usage = process.memory_info().rss / 1024 ** 3  # Convert to GB\n",
    "    print(f\"üîç Current RAM Usage: {ram_usage:.2f} GB\")\n",
    "\n",
    "# ‚úÖ Function to Clean RAM\n",
    "def clean_ram():\n",
    "    gc.collect()  # ‚úÖ Free Unused Memory\n",
    "    IPython.display.clear_output(wait=True)  # ‚úÖ Clear Output (Optional)\n",
    "    check_ram()  # ‚úÖ Show Updated RAM Usage\n",
    "\n",
    "# Example Usage:\n",
    "# Run this after every large operation (like merging datasets, model training)\n",
    "check_ram()  # Before cleaning\n",
    "clean_ram()  # After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04389d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ ƒêang t·∫£i d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω...\n",
      "  - K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán: (4719765, 73)\n",
      "  - K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm th·ª≠: (1176365, 73)\n",
      "\n",
      "üè∑Ô∏è ƒêang t·∫£i b·∫£n ƒë·ªì nh√£n...\n",
      "  - T·∫£i th√†nh c√¥ng b·∫£n ƒë·ªì nh√£n cho 14 l·ªõp.\n",
      "  - C√°c l·ªõp: ['Benign', 'Bot', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DoS attacks-GoldenEye', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'DoS attacks-Slowloris', 'FTP-BruteForce', 'Infilteration', 'SSH-Bruteforce', 'Brute Force -Web', 'Brute Force -XSS', 'SQL Injection']\n",
      "\n",
      "‚öôÔ∏è ƒêang chu·∫©n b·ªã d·ªØ li·ªáu (chuy·ªÉn ƒë·ªïi sang NumPy)...\n",
      "  - K√≠ch th∆∞·ªõc X_train: (4719765, 72), y_train: (4719765,)\n",
      "  - K√≠ch th∆∞·ªõc X_test: (1176365, 72), y_test: (1176365,)\n",
      "\n",
      "üî• Ki·ªÉm tra ph·∫°m vi gi√° tr·ªã d·ªØ li·ªáu (sau khi t·∫£i):\n",
      "  - Ph·∫°m vi ƒë·∫∑c tr∆∞ng Train: 0.0000 ƒë·∫øn 1.0000\n",
      "  - Ph·∫°m vi ƒë·∫∑c tr∆∞ng Test: 0.0000 ƒë·∫øn 3.4572\n",
      "  - ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ t·ª´ Pandas DataFrames.\n",
      "\n",
      "========== üå≥ B·∫Øt ƒë·∫ßu Tinh ch·ªânh v√† Hu·∫•n luy·ªán Random Forest ==========\n",
      "\n",
      "‚è≥ B·∫Øt ƒë·∫ßu RandomizedSearchCV cho Random Forest (n_iter=5, cv=2)...\n",
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "6 fits failed out of a total of 10.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 360, in fit\n",
      "    X, y = validate_data(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2961, in validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1370, in check_X_y\n",
      "    X = check_array(\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1055, in check_array\n",
      "    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 839, in _asarray_with_order\n",
      "    array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "numpy._core._exceptions._ArrayMemoryError: Unable to allocate 648. MiB for an array with shape (2359882, 72) and data type float32\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 487, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\joblib\\parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\joblib\\parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\joblib\\parallel.py\", line 1762, in _retrieve\n",
      "    time.sleep(0.01)\n",
      "OSError: [WinError 1450] Insufficient system resources exist to complete the requested service\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 196\u001b[39m\n\u001b[32m    194\u001b[39m start_rf_tuning_time = time.time()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[43mrf_random_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     rf_tuning_time = time.time() - start_rf_tuning_time\n\u001b[32m    198\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Ho√†n th√†nh RandomizedSearchCV cho Random Forest trong \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrf_tuning_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m gi√¢y.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1062\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1060\u001b[39m refit_start_time = time.time()\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbest_estimator_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28mself\u001b[39m.best_estimator_.fit(X, **routed_params.estimator.fit)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    476\u001b[39m trees = [\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    479\u001b[39m ]\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import joblib\n",
    "import os # Th√™m os ƒë·ªÉ t·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "\n",
    "# C√°c thu·∫≠t to√°n v√† c√¥ng c·ª• ƒë√°nh gi√° t·ª´ Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV # TH√äM M·ªöI\n",
    "\n",
    "# Th∆∞ vi·ªán XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Th∆∞ vi·ªán PyTorch (ch·ªâ d√πng ƒë·ªÉ t·∫£i label_mapping n·∫øu b·∫°n ƒë√£ l∆∞u b·∫±ng torch.save)\n",
    "import torch\n",
    "\n",
    "# --- H√†m tr·ª£ gi√∫p ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh ---\n",
    "# H√†m evaluate_model c·ªßa b·∫°n gi·ªØ nguy√™n (nh∆∞ b·∫°n ƒë√£ cung c·∫•p)\n",
    "def evaluate_model(model, X_test, y_test, class_names, model_name, results_dir=\"dataset/working1\"):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° m√¥ h√¨nh, in c√°c ch·ªâ s·ªë, l∆∞u k·∫øt qu·∫£ v√† v·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n.\n",
    "    \"\"\"\n",
    "    # ƒê·∫£m b·∫£o th∆∞ m·ª•c results_dir t·ªìn t·∫°i\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nüîç B·∫Øt ƒë·∫ßu ƒë√°nh gi√° m√¥ h√¨nh: {model_name}...\")\n",
    "    start_eval_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    eval_time = time.time() - start_eval_time\n",
    "    print(f\"  - Th·ªùi gian d·ª± ƒëo√°n: {eval_time:.2f} gi√¢y\")\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    try:\n",
    "        report = classification_report(y_test, y_pred, target_names=class_names, zero_division=0, digits=4)\n",
    "    except ValueError as e:\n",
    "        print(f\"‚ö†Ô∏è C·∫£nh b√°o khi t·∫°o classification report cho {model_name}: {e}\")\n",
    "        try:\n",
    "            report = classification_report(y_test, y_pred, zero_division=0, digits=4)\n",
    "        except Exception as report_err:\n",
    "            print(f\"‚ùå Kh√¥ng th·ªÉ t·∫°o classification report: {report_err}\")\n",
    "            report = \"Kh√¥ng th·ªÉ t·∫°o b√°o c√°o.\"\n",
    "\n",
    "    print(f\"\\nüèÜ ƒê·ªô ch√≠nh x√°c cu·ªëi c√πng c·ªßa {model_name}: {accuracy:.2f}%\")\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i (Classification Report) c·ªßa {model_name}:\\n{report}\")\n",
    "\n",
    "    results_filename = os.path.join(results_dir, f\"{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')}_results.txt\")\n",
    "    try:\n",
    "        with open(results_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"{model_name} - Final Accuracy: {accuracy:.2f}%\\n\\n\")\n",
    "            f.write(\"Classification Report:\\n\")\n",
    "            f.write(report)\n",
    "        print(f\"‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: '{results_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi l∆∞u k·∫øt qu·∫£ cho {model_name}: {e}\")\n",
    "\n",
    "    cm_filename = os.path.join(results_dir, f\"{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')}_confusion_matrix.png\")\n",
    "    try:\n",
    "        plt.figure(figsize=(18, 15)) # TƒÉng k√≠ch th∆∞·ªõc cho d·ªÖ ƒë·ªçc h∆°n\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred, labels=np.arange(len(class_names))) # ƒê·∫£m b·∫£o labels ƒë√∫ng th·ª© t·ª±\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 8})\n",
    "        plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix) - {model_name}', fontsize=16)\n",
    "        plt.ylabel('Nh√£n Th·∫≠t (True Label)', fontsize=14)\n",
    "        plt.xlabel('Nh√£n D·ª± ƒëo√°n (Predicted Label)', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.yticks(rotation=0, fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(cm_filename, dpi=150)\n",
    "        print(f\"üìà Ma tr·∫≠n nh·∫ßm l·∫´n ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: '{cm_filename}'\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi t·∫°o/l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n cho {model_name}: {e}\")\n",
    "        # traceback.print_exc() # B·ªè comment ƒë·ªÉ xem chi ti·∫øt l·ªói n·∫øu c·∫ßn\n",
    "\n",
    "# --- Ch∆∞∆°ng tr√¨nh ch√≠nh ---\n",
    "start_total_time = time.time()\n",
    "gc.collect()\n",
    "\n",
    "# --- Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n ---\n",
    "data_dir = \"dataset/working1\" # Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω c·ªßa b·∫°n\n",
    "results_dir = os.path.join(data_dir, \"tuning_results\") # Th∆∞ m·ª•c m·ªõi ƒë·ªÉ l∆∞u k·∫øt qu·∫£ tuning\n",
    "os.makedirs(results_dir, exist_ok=True) # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "\n",
    "train_file = os.path.join(data_dir, \"train_balanced1.csv\") # File train ƒë√£ qua SMOTE\n",
    "test_file = os.path.join(data_dir, \"test1.csv\") # File test\n",
    "label_map_file = os.path.join(data_dir, \"label_mapping.pth\") # File √°nh x·∫° nh√£n\n",
    "\n",
    "# --- T·∫£i d·ªØ li·ªáu ---\n",
    "print(\"\\nüìÇ ƒêang t·∫£i d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω...\")\n",
    "try:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "    print(f\"  - K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán: {train_data.shape}\")\n",
    "    print(f\"  - K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm th·ª≠: {test_data.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp d·ªØ li·ªáu. {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫£i d·ªØ li·ªáu: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- T·∫£i b·∫£n ƒë·ªì nh√£n (Label Mapping) ---\n",
    "print(\"\\nüè∑Ô∏è ƒêang t·∫£i b·∫£n ƒë·ªì nh√£n...\")\n",
    "try:\n",
    "    label_mapping = torch.load(label_map_file)\n",
    "    # S·∫Øp x·∫øp class_names d·ª±a tr√™n gi√° tr·ªã c·ªßa mapping (0, 1, 2, ...)\n",
    "    class_names = [k for k, v in sorted(label_mapping.items(), key=lambda item: item[1])]\n",
    "    num_classes = len(class_names)\n",
    "    print(f\"  - T·∫£i th√†nh c√¥ng b·∫£n ƒë·ªì nh√£n cho {num_classes} l·ªõp.\")\n",
    "    print(f\"  - C√°c l·ªõp: {class_names}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp b·∫£n ƒë·ªì nh√£n '{label_map_file}'.\")\n",
    "    max_label_train = train_data['Label'].max() if 'Label' in train_data else -1\n",
    "    max_label_test = test_data['Label'].max() if 'Label' in test_data else -1\n",
    "    max_label = int(max(max_label_train, max_label_test))\n",
    "    if max_label >= 0:\n",
    "        class_names = [f\"Lop_{i}\" for i in range(max_label + 1)] # C·∫ßn ƒë·∫£m b·∫£o th·ª© t·ª± ƒë√∫ng\n",
    "        num_classes = len(class_names)\n",
    "        print(f\"‚ö†Ô∏è ƒêang s·ª≠ d·ª•ng t√™n l·ªõp m·∫∑c ƒë·ªãnh: {class_names}\")\n",
    "    else:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ x√°c ƒë·ªãnh s·ªë l·ªõp t·ª´ d·ªØ li·ªáu.\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i b·∫£n ƒë·ªì nh√£n: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh ---\n",
    "print(\"\\n‚öôÔ∏è ƒêang chu·∫©n b·ªã d·ªØ li·ªáu (chuy·ªÉn ƒë·ªïi sang NumPy)...\")\n",
    "try:\n",
    "    X_train_np = train_data.drop(columns=[\"Label\"]).values\n",
    "    y_train_np = train_data[\"Label\"].values\n",
    "    X_test_np = test_data.drop(columns=[\"Label\"]).values\n",
    "    y_test_np = test_data[\"Label\"].values\n",
    "    print(f\"  - K√≠ch th∆∞·ªõc X_train: {X_train_np.shape}, y_train: {y_train_np.shape}\")\n",
    "    print(f\"  - K√≠ch th∆∞·ªõc X_test: {X_test_np.shape}, y_test: {y_test_np.shape}\")\n",
    "except KeyError as e:\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y c·ªôt 'Label' trong DataFrame. {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi chu·∫©n b·ªã d·ªØ li·ªáu NumPy: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nüî• Ki·ªÉm tra ph·∫°m vi gi√° tr·ªã d·ªØ li·ªáu (sau khi t·∫£i):\")\n",
    "min_train, max_train = X_train_np.min(), X_train_np.max()\n",
    "min_test, max_test = X_test_np.min(), X_test_np.max()\n",
    "print(f\"  - Ph·∫°m vi ƒë·∫∑c tr∆∞ng Train: {min_train:.4f} ƒë·∫øn {max_train:.4f}\") # ƒê√£ chu·∫©n h√≥a n√™n th∆∞·ªùng l√† 0-1\n",
    "print(f\"  - Ph·∫°m vi ƒë·∫∑c tr∆∞ng Test: {min_test:.4f} ƒë·∫øn {max_test:.4f}\")   # C√≥ th·ªÉ >1 n·∫øu test c√≥ gi√° tr·ªã ngo√†i kho·∫£ng train\n",
    "\n",
    "del train_data, test_data\n",
    "gc.collect()\n",
    "print(\"  - ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ t·ª´ Pandas DataFrames.\")\n",
    "\n",
    "\n",
    "# --- C·∫§U H√åNH CHUNG CHO RandomizedSearchCV ---\n",
    "# CH√ö √ù: TƒÇNG n_iter v√† cv ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët h∆°n, nh∆∞ng s·∫Ω t·ªën nhi·ªÅu th·ªùi gian h∆°n.\n",
    "N_ITER_SEARCH = 5 # S·ªë l∆∞·ª£ng k·∫øt h·ª£p tham s·ªë ƒë∆∞·ª£c th·ª≠ (TƒÇNG L√äN √≠t nh·∫•t 10-20)\n",
    "CV_FOLDS = 2      # S·ªë fold cho cross-validation (TƒÇNG L√äN 3 ho·∫∑c 5)\n",
    "SCORING_METRIC = 'f1_weighted' # Metric ƒë·ªÉ t·ªëi ∆∞u\n",
    "\n",
    "# --- 1. Tinh ch·ªânh si√™u tham s·ªë v√† Hu·∫•n luy·ªán Random Forest ---\n",
    "print(\"\\n\" + \"=\"*10 + \" üå≥ B·∫Øt ƒë·∫ßu Tinh ch·ªânh v√† Hu·∫•n luy·ªán Random Forest \" + \"=\"*10)\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a kh√¥ng gian si√™u tham s·ªë cho RandomForest\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [100, 150, 200], # Gi·∫£m b·ªõt ƒë·ªÉ ch·∫°y nhanh h∆°n, b·∫°n c√≥ th·ªÉ th√™m 250, 300\n",
    "    'max_depth': [20, 30, None], # None nghƒ©a l√† kh√¥ng gi·ªõi h·∫°n\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None], # None t∆∞∆°ng ƒë∆∞∆°ng v·ªõi t·∫•t c·∫£ c√°c feature\n",
    "    # 'class_weight': [None, 'balanced', 'balanced_subsample'] # SMOTE ƒë√£ x·ª≠ l√Ω, nh∆∞ng c√≥ th·ªÉ th·ª≠ th√™m\n",
    "}\n",
    "\n",
    "rf_base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base_model,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=N_ITER_SEARCH,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring=SCORING_METRIC,\n",
    "    verbose=2, # Hi·ªÉn th·ªã log qu√° tr√¨nh t√¨m ki·∫øm\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # S·ª≠ d·ª•ng t·∫•t c·∫£ CPU cores cho cross-validation\n",
    ")\n",
    "\n",
    "print(f\"\\n‚è≥ B·∫Øt ƒë·∫ßu RandomizedSearchCV cho Random Forest (n_iter={N_ITER_SEARCH}, cv={CV_FOLDS})...\")\n",
    "start_rf_tuning_time = time.time()\n",
    "try:\n",
    "    rf_random_search.fit(X_train_np, y_train_np)\n",
    "    rf_tuning_time = time.time() - start_rf_tuning_time\n",
    "    print(f\"‚úÖ Ho√†n th√†nh RandomizedSearchCV cho Random Forest trong {rf_tuning_time:.2f} gi√¢y.\")\n",
    "    print(f\"üåü Si√™u tham s·ªë t·ªët nh·∫•t cho Random Forest: {rf_random_search.best_params_}\")\n",
    "    print(f\"üåü ƒêi·ªÉm {SCORING_METRIC} t·ªët nh·∫•t t·ª´ cross-validation: {rf_random_search.best_score_:.4f}\")\n",
    "\n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest cu·ªëi c√πng v·ªõi c√°c tham s·ªë t·ªët nh·∫•t\n",
    "    print(\"\\n‚öôÔ∏è Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest cu·ªëi c√πng v·ªõi si√™u tham s·ªë t·ªët nh·∫•t...\")\n",
    "    best_rf_model = rf_random_search.best_estimator_ # L·∫•y m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c fit v·ªõi params t·ªët nh·∫•t tr√™n to√†n b·ªô X_train_np\n",
    "    # Ho·∫∑c b·∫°n c√≥ th·ªÉ kh·ªüi t·∫°o l·∫°i v√† fit:\n",
    "    # best_rf_model = RandomForestClassifier(**rf_random_search.best_params_, random_state=42, n_jobs=-1)\n",
    "    # best_rf_model.fit(X_train_np, y_train_np) # N·∫øu d√πng best_estimator_ th√¨ kh√¥ng c·∫ßn fit l·∫°i\n",
    "\n",
    "    # L∆∞u m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
    "    rf_model_path = os.path.join(results_dir, \"tuned_random_forest_model.pkl\")\n",
    "    joblib.dump(best_rf_model, rf_model_path)\n",
    "    print(f\"üíæ M√¥ h√¨nh Random Forest (ƒë√£ tinh ch·ªânh) ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: '{rf_model_path}'\")\n",
    "\n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "    evaluate_model(best_rf_model, X_test_np, y_test_np, class_names, \"Random Forest (Tuned)\", results_dir=results_dir)\n",
    "\n",
    "except MemoryError:\n",
    "    print(f\"‚ùå L·ªói b·ªô nh·ªõ khi tinh ch·ªânh/hu·∫•n luy·ªán Random Forest.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω Random Forest: {e}\")\n",
    "    # import traceback\n",
    "    # traceback.print_exc()\n",
    "\n",
    "del rf_base_model, rf_random_search\n",
    "if 'best_rf_model' in locals(): del best_rf_model\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# --- 2. Tinh ch·ªânh si√™u tham s·ªë v√† Hu·∫•n luy·ªán XGBoost ---\n",
    "print(\"\\n\" + \"=\"*10 + \" üöÄ B·∫Øt ƒë·∫ßu Tinh ch·ªânh v√† Hu·∫•n luy·ªán XGBoost \" + \"=\"*10)\n",
    "\n",
    "# Ki·ªÉm tra GPU v√† thi·∫øt l·∫≠p tree_method\n",
    "xgb_tree_method = 'hist' # M·∫∑c ƒë·ªãnh l√† CPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"  - Ph√°t hi·ªán GPU NVIDIA v√† CUDA.\")\n",
    "    xgb_tree_method = 'hist' # S·ª≠ d·ª•ng 'hist' v√† s·∫Ω ƒë·∫∑t device='cuda'\n",
    "    print(f\"  - S·∫Ω s·ª≠ d·ª•ng tree_method='{xgb_tree_method}' v√† device='cuda' cho XGBoost.\")\n",
    "else:\n",
    "    print(\"  - Kh√¥ng ph√°t hi·ªán GPU NVIDIA ho·∫∑c CUDA.\")\n",
    "    print(f\"  - S·∫Ω s·ª≠ d·ª•ng tree_method='{xgb_tree_method}' (CPU) cho XGBoost.\")\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a kh√¥ng gian si√™u tham s·ªë cho XGBoost\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': [100, 150, 200], # Gi·∫£m b·ªõt ƒë·ªÉ ch·∫°y nhanh h∆°n\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [6, 8, 10], # Gi·ªØ ·ªü m·ª©c v·ª´a ph·∫£i\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "    # 'scale_pos_weight': # C√≥ th·ªÉ h·ªØu √≠ch n·∫øu v·∫´n c√≤n m·∫•t c√¢n b·∫±ng ƒë√°ng k·ªÉ sau SMOTE cho m·ªôt s·ªë l·ªõp,\n",
    "                          # nh∆∞ng XGBoost x·ª≠ l√Ω ƒëa l·ªõp, n√™n c·∫ßn c·∫©n th·∫≠n.\n",
    "                          # V·ªõi SMOTE, th∆∞·ªùng kh√¥ng c·∫ßn thi·∫øt b·∫±ng.\n",
    "}\n",
    "\n",
    "# C√°c tham s·ªë c·ªë ƒë·ªãnh cho XGBoost\n",
    "xgb_fixed_params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': num_classes,\n",
    "    'random_state': 42,\n",
    "    'eval_metric': 'mlogloss', # Ho·∫∑c 'merror'\n",
    "    'n_jobs': -1, # XGBoost t·ª± qu·∫£n l√Ω threads, n_jobs c·ªßa RandomizedSearchCV s·∫Ω lo CV\n",
    "    'tree_method': xgb_tree_method\n",
    "}\n",
    "if xgb_tree_method == 'hist' and torch.cuda.is_available(): # S·ª≠a l·∫°i theo khuy·∫øn c√°o m·ªõi\n",
    "    xgb_fixed_params['device'] = 'cuda'\n",
    "    # N·∫øu d√πng phi√™n b·∫£n XGBoost c≈© h∆°n v√† 'gpu_hist' v·∫´n ho·∫°t ƒë·ªông, b·∫°n c√≥ th·ªÉ d√πng:\n",
    "    # xgb_fixed_params['tree_method'] = 'gpu_hist'\n",
    "else:\n",
    "    xgb_fixed_params['tree_method'] = 'hist' # ƒê·∫£m b·∫£o l√† 'hist' cho CPU\n",
    "\n",
    "xgb_base_model = xgb.XGBClassifier(**xgb_fixed_params)\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base_model,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=N_ITER_SEARCH,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring=SCORING_METRIC,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1 # n_jobs c·ªßa RandomizedSearchCV\n",
    ")\n",
    "\n",
    "print(f\"\\n‚è≥ B·∫Øt ƒë·∫ßu RandomizedSearchCV cho XGBoost (n_iter={N_ITER_SEARCH}, cv={CV_FOLDS})...\")\n",
    "start_xgb_tuning_time = time.time()\n",
    "try:\n",
    "    xgb_random_search.fit(X_train_np, y_train_np)\n",
    "    xgb_tuning_time = time.time() - start_xgb_tuning_time\n",
    "    print(f\"‚úÖ Ho√†n th√†nh RandomizedSearchCV cho XGBoost trong {xgb_tuning_time:.2f} gi√¢y.\")\n",
    "    print(f\"üåü Si√™u tham s·ªë t·ªët nh·∫•t cho XGBoost: {xgb_random_search.best_params_}\")\n",
    "    print(f\"üåü ƒêi·ªÉm {SCORING_METRIC} t·ªët nh·∫•t t·ª´ cross-validation: {xgb_random_search.best_score_:.4f}\")\n",
    "\n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost cu·ªëi c√πng v·ªõi c√°c tham s·ªë t·ªët nh·∫•t\n",
    "    print(\"\\n‚öôÔ∏è Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost cu·ªëi c√πng v·ªõi si√™u tham s·ªë t·ªët nh·∫•t...\")\n",
    "    # K·∫øt h·ª£p tham s·ªë c·ªë ƒë·ªãnh v√† tham s·ªë t·ªët nh·∫•t t·ª´ t√¨m ki·∫øm\n",
    "    final_xgb_params = {**xgb_fixed_params, **xgb_random_search.best_params_}\n",
    "    best_xgb_model = xgb.XGBClassifier(**final_xgb_params)\n",
    "    \n",
    "    # Ki·ªÉm tra l·∫°i device n·∫øu c·∫ßn thi·∫øt tr∆∞·ªõc khi fit\n",
    "    if 'device' in final_xgb_params and final_xgb_params['device'] == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"‚ö†Ô∏è CUDA kh√¥ng kh·∫£ d·ª•ng, chuy·ªÉn XGBoost sang CPU.\")\n",
    "        final_xgb_params['device'] = 'cpu'\n",
    "        final_xgb_params['tree_method'] = 'hist'\n",
    "        best_xgb_model = xgb.XGBClassifier(**final_xgb_params) # Kh·ªüi t·∫°o l·∫°i v·ªõi params CPU\n",
    "\n",
    "    best_xgb_model.fit(X_train_np, y_train_np, verbose=False)\n",
    "\n",
    "    # L∆∞u m√¥ h√¨nh\n",
    "    xgb_model_path = os.path.join(results_dir, \"tuned_xgboost_model.pkl\")\n",
    "    # Ki·ªÉm tra xem model c√≥ thu·ªôc t√≠nh device kh√¥ng (phi√™n b·∫£n XGBoost m·ªõi)\n",
    "    if hasattr(best_xgb_model, 'device') and best_xgb_model.device == 'cuda:0':\n",
    "         # Chuy·ªÉn model v·ªÅ CPU tr∆∞·ªõc khi l∆∞u b·∫±ng joblib n·∫øu n√≥ ƒëang ·ªü tr√™n GPU\n",
    "         # ƒêi·ªÅu n√†y gi√∫p tr√°nh l·ªói khi t·∫£i model tr√™n m√¥i tr∆∞·ªùng kh√¥ng c√≥ GPU\n",
    "        try:\n",
    "            print(\"  - Chuy·ªÉn model XGBoost v·ªÅ CPU tr∆∞·ªõc khi l∆∞u...\")\n",
    "            best_xgb_model.set_params(device='cpu') # Th·ª≠ c√°ch n√†y\n",
    "            # Ho·∫∑c n·∫øu c√°ch tr√™n kh√¥ng ƒë∆∞·ª£c, b·∫°n c√≥ th·ªÉ c·∫ßn get_params, x√≥a device, r·ªìi t·∫°o l·∫°i\n",
    "            # params = best_xgb_model.get_params()\n",
    "            # params.pop('device', None) # B·ªè device\n",
    "            # params.pop('tree_method', None) # B·ªè tree_method ƒë·ªÉ n√≥ t·ª± ch·ªçn hist\n",
    "            # cpu_model = xgb.XGBClassifier(**params)\n",
    "            # cpu_model.fit(X_train_np, y_train_np) # Fit l·∫°i tr√™n CPU, h∆°i t·ªën tgian\n",
    "            # joblib.dump(cpu_model, xgb_model_path)\n",
    "            joblib.dump(best_xgb_model, xgb_model_path) # L∆∞u model ƒë√£ chuy·ªÉn v·ªÅ CPU (n·∫øu set_params th√†nh c√¥ng)\n",
    "\n",
    "        except Exception as e_cpu:\n",
    "            print(f\"  - ‚ö†Ô∏è Kh√¥ng th·ªÉ chuy·ªÉn model XGBoost v·ªÅ CPU, l∆∞u tr·ª±c ti·∫øp: {e_cpu}\")\n",
    "            joblib.dump(best_xgb_model, xgb_model_path) # V·∫´n c·ªë l∆∞u\n",
    "    else:\n",
    "        joblib.dump(best_xgb_model, xgb_model_path) # L∆∞u model\n",
    "\n",
    "    print(f\"üíæ M√¥ h√¨nh XGBoost (ƒë√£ tinh ch·ªânh) ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: '{xgb_model_path}'\")\n",
    "\n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "    evaluate_model(best_xgb_model, X_test_np, y_test_np, class_names, \"XGBoost (Tuned)\", results_dir=results_dir)\n",
    "\n",
    "except xgb.core.XGBoostError as e:\n",
    "    print(f\"‚ùå L·ªói XGBoost: {e}\")\n",
    "    if \"CUDA\" in str(e) or \"GPU\" in str(e):\n",
    "        print(\"  - L·ªói c√≥ th·ªÉ li√™n quan ƒë·∫øn c√†i ƒë·∫∑t CUDA ho·∫∑c GPU.\")\n",
    "except MemoryError:\n",
    "    print(f\"‚ùå L·ªói b·ªô nh·ªõ khi tinh ch·ªânh/hu·∫•n luy·ªán XGBoost.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω XGBoost: {e}\")\n",
    "    # import traceback\n",
    "    # traceback.print_exc()\n",
    "\n",
    "\n",
    "# --- Ho√†n t·∫•t ---\n",
    "del X_train_np, y_train_np, X_test_np, y_test_np\n",
    "if 'xgb_base_model' in locals(): del xgb_base_model\n",
    "if 'xgb_random_search' in locals(): del xgb_random_search\n",
    "if 'best_xgb_model' in locals(): del best_xgb_model\n",
    "gc.collect()\n",
    "\n",
    "end_total_time = time.time()\n",
    "total_script_time = end_total_time - start_total_time\n",
    "print(f\"\\nüèÅüèÅüèÅ Qu√° tr√¨nh tinh ch·ªânh, hu·∫•n luy·ªán v√† ƒë√°nh gi√° ho√†n t·∫•t trong {total_script_time:.2f} gi√¢y! üèÅüèÅüèÅ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
