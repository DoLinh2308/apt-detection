{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Finetuning M√¥ h√¨nh tr√™n D·ªØ li·ªáu DAPT2020\n",
                "\n",
                "Notebook n√†y th·ª±c hi·ªán vi·ªác tinh ch·ªânh (finetuning) m·ªôt m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc (v√≠ d·ª•: tr√™n CSE-CIC-IDS2018) b·∫±ng c√°ch s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu DAPT2020. \n",
                "\n",
                "**L∆∞u √Ω v·ªÅ Finetuning Random Forest/XGBoost:**\n",
                "Vi·ªác \"finetuning\" tr·ª±c ti·∫øp c√°c m√¥ h√¨nh n√†y kh√¥ng gi·ªëng nh∆∞ m·∫°ng neural. C√°c c√°ch ti·∫øp c·∫≠n c√≥ th·ªÉ bao g·ªìm:\n",
                "1.  Hu·∫•n luy·ªán ti·∫øp m√¥ h√¨nh n·∫øu n√≥ h·ªó tr·ª£ (v√≠ d·ª•: XGBoost c√≥ tham s·ªë `xgb_model` ƒë·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán).\n",
                "2.  Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh m·ªõi tr√™n DAPT2020 nh∆∞ng s·ª≠ d·ª•ng c√°c si√™u tham s·ªë t·ªëi ∆∞u ƒë√£ t√¨m th·∫•y t·ª´ m√¥ h√¨nh tr∆∞·ªõc ƒë√≥.\n",
                "3.  K·∫øt h·ª£p d·ªØ li·ªáu (n·∫øu c√≥ th·ªÉ l√†m cho ƒë·∫∑c tr∆∞ng v√† nh√£n t∆∞∆°ng th√≠ch) v√† hu·∫•n luy·ªán l·∫°i t·ª´ ƒë·∫ßu.\n",
                "\n",
                "Trong notebook n√†y, ch√∫ng ta s·∫Ω t·∫≠p trung v√†o vi·ªác t·∫£i m√¥ h√¨nh XGBoost ƒë√£ hu·∫•n luy·ªán v√† th·ª≠ hu·∫•n luy·ªán ti·∫øp, v√† hu·∫•n luy·ªán l·∫°i Random Forest v·ªõi c·∫•u h√¨nh t∆∞∆°ng t·ª± tr√™n DAPT2020."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Thi·∫øt l·∫≠p M√¥i tr∆∞·ªùng v√† Import Th∆∞ vi·ªán"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üßπ RAM and GPU memory cleared (if GPU was used).\n",
                        "üîç Current RAM Usage: 0.58 GB\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import gc\n",
                "import time\n",
                "import joblib # ƒê·ªÉ t·∫£i/l∆∞u model v√† scaler\n",
                "import torch # ƒê·ªÉ t·∫£i label_mapping n·∫øu ƒë√£ l∆∞u b·∫±ng torch\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "from imblearn.over_sampling import SMOTE # N·∫øu c·∫ßn x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng tr√™n DAPT2020\n",
                "\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "import xgboost as xgb\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n (C·∫¶N ƒêI·ªÄU CH·ªàNH CHO PH√ô H·ª¢P)\n",
                "DAPT2020_DATA_DIR = 'dataset/DAPT-2020/' # V√≠ d·ª•: 'dataset/DAPT2020/'\n",
                "PRETRAINED_MODEL_DIR = 'dataset/working2/' # Th∆∞ m·ª•c ch·ª©a model ƒë√£ hu·∫•n luy·ªán tr√™n CSE-CIC-IDS2018\n",
                "OUTPUT_DIR = 'dataset/working2/finetuned_DAPT2020/' # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ finetune\n",
                "\n",
                "# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# H√†m tr·ª£ gi√∫p (t∆∞∆°ng t·ª± nh∆∞ trong notebook g·ªëc)\n",
                "def check_ram():\n",
                "    import psutil\n",
                "    process = psutil.Process(os.getpid())\n",
                "    ram_usage = process.memory_info().rss / 1024 ** 3\n",
                "    print(f\"üîç Current RAM Usage: {ram_usage:.2f} GB\")\n",
                "\n",
                "def clean_ram():\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()\n",
                "    print(\"üßπ RAM and GPU memory cleared (if GPU was used).\")\n",
                "    check_ram()\n",
                "\n",
                "def evaluate_model_finetuned(model, X_test, y_test, class_names, model_name, results_dir):\n",
                "    print(f\"\\nüîç B·∫Øt ƒë·∫ßu ƒë√°nh gi√° m√¥ h√¨nh ƒë√£ finetune: {model_name}...\")\n",
                "    start_eval_time = time.time()\n",
                "    y_pred = model.predict(X_test)\n",
                "    eval_time = time.time() - start_eval_time\n",
                "    print(f\"   - Th·ªùi gian d·ª± ƒëo√°n: {eval_time:.2f} gi√¢y\")\n",
                "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
                "    report = classification_report(y_test, y_pred, target_names=class_names, zero_division=0)\n",
                "    print(f\"\\nüèÜ ƒê·ªô ch√≠nh x√°c cu·ªëi c√πng c·ªßa {model_name}: {accuracy:.2f}%\")\n",
                "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i (Classification Report) c·ªßa {model_name}:\")\n",
                "    print(report)\n",
                "    results_filename = os.path.join(results_dir, f\"{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')}_results.txt\")\n",
                "    with open(results_filename, 'w', encoding='utf-8') as f:\n",
                "        f.write(f\"{model_name} - Final Accuracy: {accuracy:.2f}%\\n\\n\")\n",
                "        f.write(\"Classification Report:\\n\")\n",
                "        f.write(report)\n",
                "    print(f\"‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: '{results_filename}'\")\n",
                "    cm_filename = os.path.join(results_dir, f\"{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')}_confusion_matrix.png\")\n",
                "    plt.figure(figsize=(max(10, len(class_names)), max(8, len(class_names) * 0.8)))\n",
                "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
                "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
                "                xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 8})\n",
                "    plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - {model_name}', fontsize=14)\n",
                "    plt.ylabel('Nh√£n Th·∫≠t', fontsize=12)\n",
                "    plt.xlabel('Nh√£n D·ª± ƒëo√°n', fontsize=12)\n",
                "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
                "    plt.yticks(rotation=0, fontsize=10)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(cm_filename, dpi=150)\n",
                "    print(f\"üìà Ma tr·∫≠n nh·∫ßm l·∫´n ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: '{cm_filename}'\")\n",
                "    plt.close()\n",
                "\n",
                "clean_ram()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. T·∫£i v√† Ti·ªÅn x·ª≠ l√Ω C∆° b·∫£n D·ªØ li·ªáu DAPT2020\n",
                "\n",
                "**QUAN TR·ªåNG:** B·∫°n c·∫ßn ƒëi·ªÅu ch·ªânh ƒëo·∫°n m√£ d∆∞·ªõi ƒë√¢y ƒë·ªÉ ƒë·ªçc ƒë√∫ng c√°c t·ªáp CSV c·ªßa DAPT2020 v√† th·ª±c hi·ªán c√°c b∆∞·ªõc l√†m s·∫°ch c∆° b·∫£n t∆∞∆°ng t·ª± nh∆∞ ƒë√£ l√†m v·ªõi CSE-CIC-IDS2018.\n",
                "C√°c b∆∞·ªõc bao g·ªìm:\n",
                "- ƒê·ªçc d·ªØ li·ªáu (theo chunk n·∫øu l·ªõn).\n",
                "- X·ª≠ l√Ω gi√° tr·ªã thi·∫øu (NaN), v√¥ c·ª±c (inf).\n",
                "- Lo·∫°i b·ªè d√≤ng tr√πng l·∫∑p, header l·∫∑p l·∫°i.\n",
                "- Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu c∆° b·∫£n."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîÑ B·∫Øt ƒë·∫ßu t·∫£i v√† ti·ªÅn x·ª≠ l√Ω c∆° b·∫£n DAPT2020 t·ª´ file ƒë√£ g·ªôp...\n",
                        "   üìÇ ƒêang ƒë·ªçc file ƒë√£ g·ªôp: dataset/DAPT-2020/merged_cleaned.csv\n",
                        "      ‚úÖ ƒê√£ ƒë·ªçc th√†nh c√¥ng file ƒë√£ g·ªôp. K√≠ch th∆∞·ªõc ban ƒë·∫ßu: (165154, 85)\n",
                        "      - ƒê√£ chu·∫©n h√≥a t√™n c·ªôt.\n",
                        "      ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y c·ªôt 'Label' ƒë·ªÉ ki·ªÉm tra header l·∫∑p l·∫°i.\n",
                        "      - ƒê√£ x·ª≠ l√Ω gi√° tr·ªã v√¥ c·ª±c.\n",
                        "      - ƒêang chuy·ªÉn ƒë·ªïi 78 c·ªôt sang ki·ªÉu s·ªë v√† fill NaN b·∫±ng 0...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\linhv\\AppData\\Local\\Temp\\ipykernel_19084\\1002639517.py:85: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
                        "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
                        "\n",
                        "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
                        "\n",
                        "\n",
                        "  df_dapt[col].fillna(0, inplace=True)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "      - Chuy·ªÉn ƒë·ªïi ki·ªÉu s·ªë v√† fill NaN ho√†n t·∫•t.\n",
                        "      - ƒê√£ lo·∫°i b·ªè 82577 d√≤ng tr√πng l·∫∑p.\n",
                        "‚úÖ DAPT2020 sau khi l√†m s·∫°ch c∆° b·∫£n. K√≠ch th∆∞·ªõc: (82577, 85)\n",
                        "‚úÖ T·∫£i v√† ti·ªÅn x·ª≠ l√Ω c∆° b·∫£n DAPT2020 ho√†n th√†nh trong 2.89 gi√¢y.\n",
                        "üîç Current RAM Usage: 0.64 GB\n"
                    ]
                }
            ],
            "source": [
                "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt n·∫øu ch√∫ng ch∆∞a ƒë∆∞·ª£c import ·ªü ƒë·∫ßu notebook\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import gc # ƒê·ªÉ qu·∫£n l√Ω b·ªô nh·ªõ\n",
                "import time # ƒê·ªÉ ƒëo th·ªùi gian th·ª±c thi\n",
                "\n",
                "print(\"üîÑ B·∫Øt ƒë·∫ßu t·∫£i v√† ti·ªÅn x·ª≠ l√Ω c∆° b·∫£n DAPT2020 t·ª´ file ƒë√£ g·ªôp...\")\n",
                "start_load_time = time.time()\n",
                "\n",
                "# --- ƒê·ªçc d·ªØ li·ªáu DAPT2020 t·ª´ file ƒë√£ g·ªôp ---\n",
                "# C·∫¨P NH·∫¨T ƒê∆Ø·ªúNG D·∫™N V√Ä T√äN FILE CHO ƒê√öNG V·ªöI FILE G·ªòP C·ª¶A B·∫†N\n",
                "# Bi·∫øn DAPT2020_DATA_DIR n√™n ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü cell ƒë·∫ßu ti√™n c·ªßa notebook\n",
                "# V√≠ d·ª•: DAPT2020_DATA_DIR = 'dataset/DAPT2020/'\n",
                "# (Gi·∫£ s·ª≠ DAPT2020_DATA_DIR ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü cell tr∆∞·ªõc ƒë√≥)\n",
                "DAPT2020_MERGED_FILE_PATH = os.path.join(DAPT2020_DATA_DIR, 'merged_cleaned.csv')\n",
                "\n",
                "df_dapt = pd.DataFrame() # Kh·ªüi t·∫°o DataFrame r·ªóng ph√≤ng tr∆∞·ªùng h·ª£p l·ªói\n",
                "\n",
                "if not os.path.exists(DAPT2020_MERGED_FILE_PATH):\n",
                "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file DAPT2020 ƒë√£ g·ªôp t·∫°i: {DAPT2020_MERGED_FILE_PATH}\")\n",
                "    # C√¢n nh·∫Øc vi·ªác d·ª´ng th·ª±c thi ho·∫∑c x·ª≠ l√Ω l·ªói ph√π h·ª£p ·ªü ƒë√¢y n·∫øu file kh√¥ng t·ªìn t·∫°i\n",
                "    # V√≠ d·ª•: df_dapt = None ho·∫∑c raise FileNotFoundError(\"File DAPT2020 g·ªôp kh√¥ng t·ªìn t·∫°i\")\n",
                "else:\n",
                "    print(f\"   üìÇ ƒêang ƒë·ªçc file ƒë√£ g·ªôp: {DAPT2020_MERGED_FILE_PATH}\")\n",
                "    try:\n",
                "        # ƒê·ªçc to√†n b·ªô file CSV ƒë√£ g·ªôp.\n",
                "        # N·∫øu file n√†y v·∫´n R·∫§T L·ªöN (v√≠ d·ª•, >10-15GB v√† b·∫°n kh√¥ng ƒë·ªß RAM),\n",
                "        # b·∫°n c√≥ th·ªÉ c·∫ßn ƒë·ªçc theo chunk ngay c·∫£ khi n√≥ ƒë√£ ƒë∆∞·ª£c g·ªôp.\n",
                "        # df_dapt_chunks = []\n",
                "        # chunk_iterator = pd.read_csv(DAPT2020_MERGED_FILE_PATH, low_memory=False, chunksize=100000)\n",
                "        # for chunk in chunk_iterator:\n",
                "        #     # Th·ª±c hi·ªán c√°c b∆∞·ªõc l√†m s·∫°ch c∆° b·∫£n cho t·ª´ng chunk ·ªü ƒë√¢y n·∫øu c·∫ßn\n",
                "        #     df_dapt_chunks.append(chunk)\n",
                "        # df_dapt = pd.concat(df_dapt_chunks, ignore_index=True)\n",
                "        # del df_dapt_chunks\n",
                "        # gc.collect()\n",
                "\n",
                "        # T·∫°m th·ªùi gi·∫£ ƒë·ªãnh c√≥ th·ªÉ ƒë·ªçc to√†n b·ªô v√†o b·ªô nh·ªõ:\n",
                "        df_dapt = pd.read_csv(DAPT2020_MERGED_FILE_PATH, low_memory=False)\n",
                "        print(f\"      ‚úÖ ƒê√£ ƒë·ªçc th√†nh c√¥ng file ƒë√£ g·ªôp. K√≠ch th∆∞·ªõc ban ƒë·∫ßu: {df_dapt.shape}\")\n",
                "\n",
                "        # --- C√°c b∆∞·ªõc l√†m s·∫°ch c∆° b·∫£n tr√™n DataFrame ƒë√£ t·∫£i ---\n",
                "        # 1. X·ª≠ l√Ω t√™n c·ªôt (v√≠ d·ª•: lo·∫°i b·ªè kho·∫£ng tr·∫Øng ·ªü ƒë·∫ßu/cu·ªëi)\n",
                "        df_dapt.columns = df_dapt.columns.str.strip()\n",
                "        print(\"      - ƒê√£ chu·∫©n h√≥a t√™n c·ªôt.\")\n",
                "\n",
                "        # 2. X√≥a header l·∫∑p l·∫°i (n·∫øu c√≥ trong file g·ªôp - √≠t kh·∫£ nƒÉng nh∆∞ng v·∫´n ki·ªÉm tra)\n",
                "        # C·∫ßn x√°c ƒë·ªãnh c·ªôt ƒë√°ng tin c·∫≠y ƒë·ªÉ ki·ªÉm tra header, v√≠ d·ª• 'Label'\n",
                "        DAPT_LABEL_COLUMN_NAME = 'Label' # THAY TH·∫æ B·∫∞NG T√äN C·ªòT NH√ÉN TH·ª∞C T·∫æ C·ª¶A DAPT2020\n",
                "        if DAPT_LABEL_COLUMN_NAME in df_dapt.columns:\n",
                "            initial_rows = len(df_dapt)\n",
                "            # ƒê·∫£m b·∫£o so s√°nh v·ªõi chu·ªói 'Label' n·∫øu header l√† chu·ªói\n",
                "            df_dapt = df_dapt[df_dapt[DAPT_LABEL_COLUMN_NAME].astype(str) != DAPT_LABEL_COLUMN_NAME]\n",
                "            if len(df_dapt) < initial_rows:\n",
                "                print(f\"      - ƒê√£ lo·∫°i b·ªè {initial_rows - len(df_dapt)} d√≤ng header l·∫∑p l·∫°i (d·ª±a tr√™n c·ªôt '{DAPT_LABEL_COLUMN_NAME}').\")\n",
                "        else:\n",
                "            print(f\"      ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y c·ªôt '{DAPT_LABEL_COLUMN_NAME}' ƒë·ªÉ ki·ªÉm tra header l·∫∑p l·∫°i.\")\n",
                "\n",
                "        # 3. X·ª≠ l√Ω gi√° tr·ªã v√¥ c·ª±c v√† NaN\n",
                "        df_dapt.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "        print(\"      - ƒê√£ x·ª≠ l√Ω gi√° tr·ªã v√¥ c·ª±c.\")\n",
                "\n",
                "        # 4. Chuy·ªÉn c√°c c·ªôt s·ªë h·ªçc ti·ªÅm nƒÉng sang ki·ªÉu s·ªë, l·ªói √©p th√†nh NaN\n",
                "        #    Sau ƒë√≥ fill NaN b·∫±ng 0 cho c√°c c·ªôt s·ªë n√†y.\n",
                "        #    C·∫ßn x√°c ƒë·ªãnh c√°c c·ªôt kh√¥ng ph·∫£i s·ªë c·ªßa DAPT2020\n",
                "        #    (v√≠ d·ª•: 'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', v√† c·ªôt nh√£n)\n",
                "        #    L∆ØU √ù: C·ªôt 'Protocol' v√† 'Timestamp' s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω/lo·∫°i b·ªè ·ªü c√°c b∆∞·ªõc sau n·∫øu c·∫ßn.\n",
                "        potential_non_numeric_dapt = [\n",
                "            'Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port',\n",
                "            'Protocol', 'Timestamp', DAPT_LABEL_COLUMN_NAME # S·ª≠ d·ª•ng bi·∫øn ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n\n",
                "            # Th√™m b·∫•t k·ª≥ c·ªôt n√†o kh√°c b·∫°n bi·∫øt ch·∫Øc ch·∫Øn kh√¥ng ph·∫£i l√† s·ªë ·ªü ƒë√¢y\n",
                "        ]\n",
                "        # Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng t·ªìn t·∫°i kh·ªèi danh s√°ch n√†y ƒë·ªÉ tr√°nh l·ªói\n",
                "        actual_potential_non_numeric = [col for col in potential_non_numeric_dapt if col in df_dapt.columns]\n",
                "\n",
                "        numeric_cols_to_convert = df_dapt.columns.difference(actual_potential_non_numeric, sort=False)\n",
                "\n",
                "        print(f\"      - ƒêang chuy·ªÉn ƒë·ªïi {len(numeric_cols_to_convert)} c·ªôt sang ki·ªÉu s·ªë v√† fill NaN b·∫±ng 0...\")\n",
                "        for col in numeric_cols_to_convert:\n",
                "            if col in df_dapt.columns: # ƒê·∫£m b·∫£o c·ªôt t·ªìn t·∫°i\n",
                "                df_dapt[col] = pd.to_numeric(df_dapt[col], errors='coerce')\n",
                "                # Ch·ªâ fill NaN n·∫øu c·ªôt ƒë√≥ th·ª±c s·ª± c√≥ gi√° tr·ªã NaN sau khi coerce\n",
                "                if df_dapt[col].isnull().any():\n",
                "                    df_dapt[col].fillna(0, inplace=True)\n",
                "        print(\"      - Chuy·ªÉn ƒë·ªïi ki·ªÉu s·ªë v√† fill NaN ho√†n t·∫•t.\")\n",
                "\n",
                "        # 5. X√≥a c√°c d√≤ng ho√†n to√†n r·ªóng (n·∫øu c√≤n sau c√°c b∆∞·ªõc tr√™n)\n",
                "        initial_rows = len(df_dapt)\n",
                "        df_dapt.dropna(how='all', inplace=True)\n",
                "        if len(df_dapt) < initial_rows:\n",
                "            print(f\"      - ƒê√£ lo·∫°i b·ªè {initial_rows - len(df_dapt)} d√≤ng ho√†n to√†n r·ªóng.\")\n",
                "\n",
                "        # 6. X√≥a c√°c d√≤ng tr√πng l·∫∑p t·ªïng th·ªÉ\n",
                "        initial_rows = len(df_dapt)\n",
                "        df_dapt.drop_duplicates(inplace=True)\n",
                "        if len(df_dapt) < initial_rows:\n",
                "            print(f\"      - ƒê√£ lo·∫°i b·ªè {initial_rows - len(df_dapt)} d√≤ng tr√πng l·∫∑p.\")\n",
                "\n",
                "        print(f\"‚úÖ DAPT2020 sau khi l√†m s·∫°ch c∆° b·∫£n. K√≠ch th∆∞·ªõc: {df_dapt.shape}\")\n",
                "\n",
                "    except FileNotFoundError:\n",
                "        print(f\"      ‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp - {DAPT2020_MERGED_FILE_PATH}\")\n",
                "        df_dapt = pd.DataFrame() # ƒê·∫£m b·∫£o df_dapt l√† DataFrame r·ªóng n·∫øu l·ªói\n",
                "    except pd.errors.EmptyDataError:\n",
                "        print(f\"      ‚ö†Ô∏è C·∫£nh b√°o: T·ªáp r·ªóng - {DAPT2020_MERGED_FILE_PATH}\")\n",
                "        df_dapt = pd.DataFrame() # ƒê·∫£m b·∫£o df_dapt l√† DataFrame r·ªóng n·∫øu l·ªói\n",
                "    except Exception as e:\n",
                "        print(f\"      ‚ùå L·ªói khi x·ª≠ l√Ω {DAPT2020_MERGED_FILE_PATH}: {e}\")\n",
                "        df_dapt = pd.DataFrame() # ƒê·∫£m b·∫£o df_dapt l√† DataFrame r·ªóng n·∫øu l·ªói\n",
                "\n",
                "if df_dapt.empty:\n",
                "   print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu DAPT2020 n√†o ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng. Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n v√† n·ªôi dung file.\")\n",
                "   # C√¢n nh·∫Øc vi·ªác d·ª´ng notebook ·ªü ƒë√¢y n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu\n",
                "   # import sys\n",
                "   # sys.exit(\"D·ª´ng th·ª±c thi do kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu DAPT2020.\")\n",
                "\n",
                "load_time = time.time() - start_load_time\n",
                "print(f\"‚úÖ T·∫£i v√† ti·ªÅn x·ª≠ l√Ω c∆° b·∫£n DAPT2020 ho√†n th√†nh trong {load_time:.2f} gi√¢y.\")\n",
                "\n",
                "# G·ªçi h√†m check_ram() ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü cell ƒë·∫ßu ti√™n c·ªßa notebook\n",
                "# (Gi·∫£ s·ª≠ h√†m check_ram() ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü cell \"1. Thi·∫øt l·∫≠p M√¥i tr∆∞·ªùng v√† Import Th∆∞ vi·ªán\")\n",
                "if 'check_ram' in locals() and callable(check_ram):\n",
                "    check_ram()\n",
                "else:\n",
                "    print(\"H√†m check_ram() ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. √Ånh x·∫° ƒê·∫∑c tr∆∞ng v√† Nh√£n, Chu·∫©n h√≥a D·ªØ li·ªáu DAPT2020\n",
                "\n",
                "ƒê√¢y l√† b∆∞·ªõc **C·ª∞C K·ª≤ QUAN TR·ªåNG**.\n",
                "1.  **T·∫£i danh s√°ch ƒë·∫∑c tr∆∞ng v√† scaler** ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng/t·∫°o ra t·ª´ m√¥ h√¨nh hu·∫•n luy·ªán tr√™n CSE-CIC-IDS2018.\n",
                "2.  **ƒê·∫£m b·∫£o DAPT2020 c√≥ c√°c ƒë·∫∑c tr∆∞ng t∆∞∆°ng ·ª©ng.** N·∫øu thi·∫øu, b·∫°n c·∫ßn xem x√©t vi·ªác t·∫°o ra ch√∫ng ho·∫∑c lo·∫°i b·ªè kh·ªèi c·∫£ hai b·ªô d·ªØ li·ªáu (n·∫øu kh√¥ng th·ªÉ t·∫°o).\n",
                "3.  **√Ånh x·∫° nh√£n t·∫•n c√¥ng c·ªßa DAPT2020** sang m·ªôt h·ªá th·ªëng nh√£n nh·∫•t qu√°n. N·∫øu b·∫°n mu·ªën m√¥ h√¨nh c≈© d·ª± ƒëo√°n c√°c l·ªõp c≈©, b·∫°n c·∫ßn √°nh x·∫° c√°c t·∫•n c√¥ng c·ªßa DAPT2020 v√†o c√°c l·ªõp ƒë√≥, ho·∫∑c t·∫°o l·ªõp \"unknown/other attack\". N·∫øu b·∫°n mu·ªën m√¥ h√¨nh h·ªçc c√°c t·∫•n c√¥ng m·ªõi t·ª´ DAPT2020, b·∫°n c·∫ßn ƒë·ªãnh nghƒ©a l·∫°i `label_mapping` v√† c√≥ th·ªÉ c·∫•u tr√∫c l·∫°i l·ªõp output c·ªßa m√¥ h√¨nh (kh√≥ h∆°n v·ªõi RF/XGBoost ƒë√£ l∆∞u).\n",
                "4.  **Chu·∫©n h√≥a d·ªØ li·ªáu DAPT2020** s·ª≠ d·ª•ng scaler ƒë√£ ƒë∆∞·ª£c `fit` tr√™n d·ªØ li·ªáu hu·∫•n luy·ªán CSE-CIC-IDS2018."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîÑ B·∫Øt ƒë·∫ßu √°nh x·∫° ƒë·∫∑c tr∆∞ng, nh√£n v√† chu·∫©n h√≥a DAPT2020...\n",
                        "‚úÖ Scaler t·ª´ CSE-CIC-IDS2018 ƒë√£ ƒë∆∞·ª£c t·∫£i t·ª´: dataset/working2/scaler.pkl\n",
                        "‚úÖ Danh s√°ch 72 ƒë·∫∑c tr∆∞ng k·ª≥ v·ªçng t·ª´ scaler CSE-CIC-IDS2018 ƒë√£ ƒë∆∞·ª£c t·∫£i.\n",
                        "   ƒêang chu·∫©n b·ªã ƒë·∫∑c tr∆∞ng cho DAPT2020...\n",
                        "      M√£ h√≥a One-Hot cho c·ªôt 'Protocol' c·ªßa DAPT2020...\n",
                        "         C√°c c·ªôt Protocol m·ªõi: ['Protocol_0', 'Protocol_6', 'Protocol_17']\n",
                        "      ƒêang √°nh x·∫° nh√£n cho DAPT2020...\n",
                        "         Label mapping c·ªßa CSE-CIC-IDS2018: {'Benign': 0, 'Bot': 1, 'DDOS attack-HOIC': 2, 'DDOS attack-LOIC-UDP': 3, 'DoS attacks-GoldenEye': 4, 'DoS attacks-Hulk': 5, 'DoS attacks-SlowHTTPTest': 6, 'DoS attacks-Slowloris': 7, 'FTP-BruteForce': 8, 'Infilteration': 9, 'SSH-Bruteforce': 10, 'Brute Force -Web': 11, 'Brute Force -XSS': 12, 'SQL Injection': 13}\n",
                        "‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y c·ªôt nh√£n 'Label' trong DAPT2020.\n",
                        "      ‚ö†Ô∏è ƒê·∫∑c tr∆∞ng k·ª≥ v·ªçng 'CWE Flag Count' kh√¥ng c√≥ trong DAPT2020 ƒë√£ x·ª≠ l√Ω. S·∫Ω th√™m c·ªôt n√†y v·ªõi gi√° tr·ªã 0.\n",
                        "      Lo·∫°i b·ªè 16 c·ªôt t·ª´ DAPT2020 kh√¥ng c√≥ trong danh s√°ch ƒë·∫∑c tr∆∞ng k·ª≥ v·ªçng: ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Timestamp']...\n"
                    ]
                },
                {
                    "ename": "KeyError",
                    "evalue": "'Stage'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
                        "\u001b[31mKeyError\u001b[39m: 'Stage'",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 147\u001b[39m\n\u001b[32m    143\u001b[39m     df_dapt_processed.drop(columns=cols_to_drop_from_dapt, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m, errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# S·∫Øp x·∫øp l·∫°i c√°c c·ªôt c·ªßa DAPT2020 theo ƒë√∫ng th·ª© t·ª± c·ªßa expected_features_cse\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# v√† th√™m c·ªôt Label v√†o cu·ªëi\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m label_column_dapt = \u001b[43mdf_dapt_processed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mStage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m df_dapt_processed = df_dapt_processed[expected_features_cse] \u001b[38;5;66;03m# ƒê·∫£m b·∫£o ƒë√∫ng th·ª© t·ª± v√† ch·ªâ c√°c c·ªôt c·∫ßn thi·∫øt\u001b[39;00m\n\u001b[32m    149\u001b[39m df_dapt_processed[\u001b[33m'\u001b[39m\u001b[33mStage\u001b[39m\u001b[33m'\u001b[39m] = label_column_dapt\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:5819\u001b[39m, in \u001b[36mDataFrame.pop\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   5778\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: Hashable) -> Series:\n\u001b[32m   5779\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5780\u001b[39m \u001b[33;03m    Return item and drop from frame. Raise KeyError if not found.\u001b[39;00m\n\u001b[32m   5781\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5817\u001b[39m \u001b[33;03m    3  monkey        NaN\u001b[39;00m\n\u001b[32m   5818\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:947\u001b[39m, in \u001b[36mNDFrame.pop\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: Hashable) -> Series | Any:\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    948\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m[item]\n\u001b[32m    950\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
                        "\u001b[31mKeyError\u001b[39m: 'Stage'"
                    ]
                }
            ],
            "source": [
                "print(\"üîÑ B·∫Øt ƒë·∫ßu √°nh x·∫° ƒë·∫∑c tr∆∞ng, nh√£n v√† chu·∫©n h√≥a DAPT2020...\")\n",
                "\n",
                "# --- 3.1 T·∫£i Scaler v√† Danh s√°ch ƒê·∫∑c tr∆∞ng t·ª´ m√¥ h√¨nh CSE-CIC-IDS2018 --- \n",
                "try:\n",
                "    PRETRAINED_SCALER_PATH = os.path.join(PRETRAINED_MODEL_DIR, 'scaler.pkl')\n",
                "    scaler_cse = joblib.load(PRETRAINED_SCALER_PATH)\n",
                "    print(f\"‚úÖ Scaler t·ª´ CSE-CIC-IDS2018 ƒë√£ ƒë∆∞·ª£c t·∫£i t·ª´: {PRETRAINED_SCALER_PATH}\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file scaler t·∫°i '{PRETRAINED_SCALER_PATH}'. H√£y ƒë·∫£m b·∫£o n√≥ ƒë√£ ƒë∆∞·ª£c l∆∞u t·ª´ notebook hu·∫•n luy·ªán g·ªëc.\")\n",
                "    # exit()\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå L·ªói khi t·∫£i scaler: {e}\")\n",
                "    # exit()\n",
                "\n",
                "try:\n",
                "    # Gi·∫£ s·ª≠ scaler.pkl l∆∞u tr·ªØ th√¥ng tin v·ªÅ c√°c feature_names m√† n√≥ ƒë√£ ƒë∆∞·ª£c fit\n",
                "    # N·∫øu scaler c·ªßa b·∫°n l√† MinMaxScaler t·ª´ sklearn, n√≥ c√≥ thu·ªôc t√≠nh 'feature_names_in_'\n",
                "    if hasattr(scaler_cse, 'feature_names_in_'):\n",
                "        expected_features_cse = scaler_cse.feature_names_in_.tolist()\n",
                "        print(f\"‚úÖ Danh s√°ch {len(expected_features_cse)} ƒë·∫∑c tr∆∞ng k·ª≥ v·ªçng t·ª´ scaler CSE-CIC-IDS2018 ƒë√£ ƒë∆∞·ª£c t·∫£i.\")\n",
                "        # print(expected_features_cse) # B·ªè comment ƒë·ªÉ xem\n",
                "    else:\n",
                "        # N·∫øu scaler kh√¥ng c√≥ feature_names_in_, b·∫°n c·∫ßn t·∫£i danh s√°ch ƒë·∫∑c tr∆∞ng t·ª´ file ri√™ng\n",
                "        FEATURE_LIST_PATH = os.path.join(PRETRAINED_MODEL_DIR, 'feature_list.txt') # Gi·∫£ s·ª≠ b·∫°n ƒë√£ l∆∞u n√≥\n",
                "        with open(FEATURE_LIST_PATH, 'r') as f:\n",
                "            expected_features_cse = [line.strip() for line in f]\n",
                "        print(f\"‚úÖ Danh s√°ch {len(expected_features_cse)} ƒë·∫∑c tr∆∞ng k·ª≥ v·ªçng t·ª´ file '{FEATURE_LIST_PATH}' ƒë√£ ƒë∆∞·ª£c t·∫£i.\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file danh s√°ch ƒë·∫∑c tr∆∞ng 'feature_list.txt' t·∫°i '{PRETRAINED_MODEL_DIR}'.\")\n",
                "    print(\"      Vui l√≤ng t·∫°o file n√†y t·ª´ notebook hu·∫•n luy·ªán g·ªëc ho·∫∑c ƒë·∫£m b·∫£o scaler l∆∞u tr·ªØ t√™n ƒë·∫∑c tr∆∞ng.\")\n",
                "    expected_features_cse = [] # Kh·ªüi t·∫°o r·ªóng ƒë·ªÉ tr√°nh l·ªói sau n√†y, nh∆∞ng ƒë√¢y l√† v·∫•n ƒë·ªÅ c·∫ßn s·ª≠a\n",
                "    # exit()\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå L·ªói khi t·∫£i danh s√°ch ƒë·∫∑c tr∆∞ng: {e}\")\n",
                "    expected_features_cse = []\n",
                "    # exit()\n",
                "\n",
                "# --- 3.2 X·ª≠ l√Ω ƒê·∫∑c tr∆∞ng trong DAPT2020 --- \n",
                "if 'df_dapt' not in locals() or df_dapt.empty:\n",
                "    print(\"‚ùå DataFrame DAPT2020 (df_dapt) r·ªóng ho·∫∑c ch∆∞a ƒë∆∞·ª£c t·∫°o. Kh√¥ng th·ªÉ ti·∫øp t·ª•c.\")\n",
                "    # exit()\n",
                "else:\n",
                "    print(\"   ƒêang chu·∫©n b·ªã ƒë·∫∑c tr∆∞ng cho DAPT2020...\")\n",
                "    df_dapt_processed = df_dapt.copy() # L√†m vi·ªác tr√™n b·∫£n sao\n",
                "\n",
                "    # Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c√≥ trong expected_features_cse (sau khi x·ª≠ l√Ω Protocol, Label)\n",
                "    # v√† th√™m c√°c c·ªôt b·ªã thi·∫øu t·ª´ expected_features_cse (ƒëi·ªÅn gi√° tr·ªã 0 ho·∫∑c NaN r·ªìi fill)\n",
                "\n",
                "    # --- 3.2.1 X·ª≠ l√Ω c·ªôt Protocol (One-Hot Encoding) --- \n",
                "    # Gi·∫£ s·ª≠ DAPT2020 c≈©ng c√≥ c·ªôt 'Protocol'. C·∫ßn ƒë·∫£m b·∫£o n√≥ ƒë∆∞·ª£c m√£ h√≥a t∆∞∆°ng t·ª± CSE-CIC-IDS2018.\n",
                "    # C√°c c·ªôt Protocol one-hot k·ª≥ v·ªçng t·ª´ CSE-CIC-IDS2018 (v√≠ d·ª•: 'Protocol_0', 'Protocol_6', 'Protocol_17')\n",
                "    expected_protocol_cols = [col for col in expected_features_cse if col.startswith('Protocol_')]\n",
                "\n",
                "    if 'Protocol' in df_dapt_processed.columns:\n",
                "        print(\"      M√£ h√≥a One-Hot cho c·ªôt 'Protocol' c·ªßa DAPT2020...\")\n",
                "        # Chuy·ªÉn sang category ƒë·ªÉ get_dummies ho·∫°t ƒë·ªông ƒë√∫ng v·ªõi c√°c gi√° tr·ªã s·ªë c·ªßa protocol\n",
                "        df_dapt_processed['Protocol'] = df_dapt_processed['Protocol'].astype('category')\n",
                "        df_dapt_processed = pd.get_dummies(df_dapt_processed, columns=['Protocol'], prefix='Protocol', dtype=np.int8)\n",
                "        print(f\"         C√°c c·ªôt Protocol m·ªõi: {[col for col in df_dapt_processed.columns if col.startswith('Protocol_')]}\")\n",
                "    else:\n",
                "        print(\"      ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'Protocol' trong DAPT2020. S·∫Ω th√™m c√°c c·ªôt Protocol_x k·ª≥ v·ªçng v·ªõi gi√° tr·ªã 0.\")\n",
                "    \n",
                "    # ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c c·ªôt Protocol_x t·ª´ CSE-CIC-IDS2018 ƒë·ªÅu c√≥ trong DAPT2020 (n·∫øu thi·∫øu th√¨ th√™m v√† fill 0)\n",
                "    for p_col in expected_protocol_cols:\n",
                "        if p_col not in df_dapt_processed.columns:\n",
                "            print(f\"         Th√™m c·ªôt Protocol b·ªã thi·∫øu: {p_col} v·ªõi gi√° tr·ªã 0.\")\n",
                "            df_dapt_processed[p_col] = np.int8(0)\n",
                "\n",
                "    # --- 3.2.2 √Ånh x·∫° Nh√£n DAPT2020 --- \n",
                "    # ƒê√¢y l√† ph·∫ßn C·ª∞C K·ª≤ QUAN TR·ªåNG v√† ph·ª• thu·ªôc v√†o m·ª•c ti√™u c·ªßa b·∫°n.\n",
                "    # GI·∫¢ ƒê·ªäNH: B·∫°n mu·ªën √°nh x·∫° c√°c nh√£n c·ªßa DAPT2020 v√†o h·ªá th·ªëng nh√£n c·ªßa CSE-CIC-IDS2018.\n",
                "    # N·∫øu m·ªôt lo·∫°i t·∫•n c√¥ng trong DAPT2020 kh√¥ng c√≥ trong CSE, b·∫°n c√≥ th·ªÉ map n√≥ v√†o 'Benign' ho·∫∑c m·ªôt l·ªõp 'OtherAttack'.\n",
                "    print(\"      ƒêang √°nh x·∫° nh√£n cho DAPT2020...\")\n",
                "    try:\n",
                "        LABEL_MAPPING_CSE_PATH = os.path.join(PRETRAINED_MODEL_DIR, 'label_mapping.pth')\n",
                "        label_mapping_cse = torch.load(LABEL_MAPPING_CSE_PATH)\n",
                "        # ƒê·∫£o ng∆∞·ª£c mapping ƒë·ªÉ l·∫•y t√™n l·ªõp t·ª´ s·ªë\n",
                "        # class_names_cse = [k for k, v in sorted(label_mapping_cse.items(), key=lambda item: item[1])]\n",
                "        print(f\"         Label mapping c·ªßa CSE-CIC-IDS2018: {label_mapping_cse}\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp label_mapping.pth c·ªßa CSE-CIC-IDS2018 t·∫°i {LABEL_MAPPING_CSE_PATH}\")\n",
                "        print(\"      Kh√¥ng th·ªÉ √°nh x·∫° nh√£n DAPT2020 m·ªôt c√°ch ƒë√°ng tin c·∫≠y.\")\n",
                "        label_mapping_cse = {\"Benign\": 0} # Fallback r·∫•t c∆° b·∫£n\n",
                "        # exit()\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå L·ªói khi t·∫£i label_mapping.pth c·ªßa CSE: {e}\")\n",
                "        label_mapping_cse = {\"Benign\": 0}\n",
                "        # exit()\n",
                "\n",
                "    # **** B·∫†N C·∫¶N ƒê·ªäNH NGHƒ®A √ÅNH X·∫† N√ÄY CHO DAPT2020 ****\n",
                "    # V√≠ d·ª•: Gi·∫£ s·ª≠ DAPT2020 c√≥ c√°c nh√£n 'Normal', 'PortScan', 'WebAppAttack'\n",
                "    # v√† b·∫°n mu·ªën map 'PortScan' v√†o m·ªôt l·ªõp t∆∞∆°ng t·ª± trong CSE (n·∫øu c√≥) ho·∫∑c m·ªôt l·ªõp chung.\n",
                "    # ƒê√¢y ch·ªâ l√† V√ç D·ª§, b·∫°n c·∫ßn thay th·∫ø b·∫±ng c√°c nh√£n th·ª±c t·∫ø c·ªßa DAPT2020.\n",
                "    dapt_to_cse_label_map = {\n",
                "        'Normal': label_mapping_cse.get('Benign', 0), # √Ånh x·∫° Normal c·ªßa DAPT2020 v√†o Benign c·ªßa CSE\n",
                "        'Benign': label_mapping_cse.get('Benign', 0),\n",
                "        # V√≠ d·ª• n·∫øu DAPT2020 c√≥ 'PortScan' v√† CSE c√≥ m·ªôt l·ªõp cho DoS/Scan\n",
                "        # 'PortScan_DAPT': label_mapping_cse.get('DoS attacks-Hulk', 0), # V√ç D·ª§ TH√îI!\n",
                "        # N·∫øu kh√¥ng c√≥ l·ªõp t∆∞∆°ng ·ª©ng, c√≥ th·ªÉ t·∫°o m·ªôt l·ªõp m·ªõi (n·∫øu m√¥ h√¨nh h·ªó tr·ª£) ho·∫∑c map v√†o Benign/Other.\n",
                "        # Trong tr∆∞·ªùng h·ª£p finetuning m√¥ h√¨nh ƒë√£ c√≥, vi·ªác th√™m l·ªõp m·ªõi kh√¥ng ƒë∆°n gi·∫£n v·ªõi RF/XGB.\n",
                "    }\n",
                "    # L·∫•y t√™n c·ªôt nh√£n c·ªßa DAPT2020 (v√≠ d·ª•: 'Label' ho·∫∑c 'Attack_Category')\n",
                "    DAPT_LABEL_COLUMN = 'Label' # THAY TH·∫æ N·∫æU C·∫¶N\n",
                "    if DAPT_LABEL_COLUMN not in df_dapt_processed.columns:\n",
                "        print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y c·ªôt nh√£n '{DAPT_LABEL_COLUMN}' trong DAPT2020.\")\n",
                "        # exit()\n",
                "    else:\n",
                "        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ·ªü c·ªôt nh√£n DAPT2020\n",
                "        df_dapt_processed[DAPT_LABEL_COLUMN] = df_dapt_processed[DAPT_LABEL_COLUMN].astype(str).str.strip()\n",
                "        print(f\"         C√°c nh√£n g·ªëc trong DAPT2020: {df_dapt_processed[DAPT_LABEL_COLUMN].unique()}\")\n",
                "        \n",
                "        # √Ånh x·∫° nh√£n, nh·ªØng nh√£n kh√¥ng c√≥ trong map s·∫Ω th√†nh NaN, sau ƒë√≥ fill b·∫±ng gi√° tr·ªã default (v√≠ d·ª•: Benign)\n",
                "        default_label_value = label_mapping_cse.get('Benign', 0)\n",
                "        df_dapt_processed['Label_mapped'] = df_dapt_processed[DAPT_LABEL_COLUMN].map(dapt_to_cse_label_map)\n",
                "        unmapped_count = df_dapt_processed['Label_mapped'].isnull().sum()\n",
                "        if unmapped_count > 0:\n",
                "            print(f\"         ‚ö†Ô∏è {unmapped_count} nh√£n trong DAPT2020 kh√¥ng ƒë∆∞·ª£c √°nh x·∫°, s·∫Ω ƒë∆∞·ª£c g√°n l√† Benign (ho·∫∑c gi√° tr·ªã default). H√£y ki·ªÉm tra dapt_to_cse_label_map!\")\n",
                "            df_dapt_processed['Label_mapped'].fillna(default_label_value, inplace=True)\n",
                "        \n",
                "        df_dapt_processed['Label'] = df_dapt_processed['Label_mapped'].astype(int) # S·ª≠ d·ª•ng c·ªôt Label chu·∫©n\n",
                "        df_dapt_processed.drop(columns=['Label_mapped', DAPT_LABEL_COLUMN], inplace=True, errors='ignore')\n",
                "        print(f\"         C√°c nh√£n sau khi √°nh x·∫° v√† fill: {df_dapt_processed['Label'].unique()}\")\n",
                "    \n",
                "    # --- 3.2.3 ƒê·ªìng b·ªô h√≥a c√°c c·ªôt ƒë·∫∑c tr∆∞ng --- \n",
                "    # Gi·ªØ l·∫°i c√°c c·ªôt 'Label' v√† c√°c c·ªôt c√≥ trong expected_features_cse\n",
                "    # expected_features_cse l√† danh s√°ch c√°c t√™n c·ªôt m√† scaler v√† m√¥ h√¨nh g·ªëc ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n ƒë√≥,\n",
                "    # kh√¥ng bao g·ªìm c·ªôt Protocol g·ªëc, nh∆∞ng bao g·ªìm c√°c c·ªôt Protocol_x ƒë√£ one-hot.\n",
                "    current_dapt_features = df_dapt_processed.columns.difference(['Label'], sort=False).tolist()\n",
                "\n",
                "    final_feature_set_for_dapt = []\n",
                "    for ef_col in expected_features_cse: # Duy·ªát theo th·ª© t·ª± c·ªßa scaler g·ªëc\n",
                "        if ef_col in current_dapt_features:\n",
                "            final_feature_set_for_dapt.append(ef_col)\n",
                "        else:\n",
                "            print(f\"      ‚ö†Ô∏è ƒê·∫∑c tr∆∞ng k·ª≥ v·ªçng '{ef_col}' kh√¥ng c√≥ trong DAPT2020 ƒë√£ x·ª≠ l√Ω. S·∫Ω th√™m c·ªôt n√†y v·ªõi gi√° tr·ªã 0.\")\n",
                "            df_dapt_processed[ef_col] = 0 # Th√™m c·ªôt thi·∫øu v√† fill 0\n",
                "            final_feature_set_for_dapt.append(ef_col)\n",
                "\n",
                "    # Lo·∫°i b·ªè c√°c c·ªôt trong DAPT2020 kh√¥ng c√≥ trong expected_features_cse\n",
                "    cols_to_drop_from_dapt = [col for col in current_dapt_features if col not in expected_features_cse]\n",
                "    if cols_to_drop_from_dapt:\n",
                "        print(f\"      Lo·∫°i b·ªè {len(cols_to_drop_from_dapt)} c·ªôt t·ª´ DAPT2020 kh√¥ng c√≥ trong danh s√°ch ƒë·∫∑c tr∆∞ng k·ª≥ v·ªçng: {cols_to_drop_from_dapt[:5]}...\")\n",
                "        df_dapt_processed.drop(columns=cols_to_drop_from_dapt, inplace=True, errors='ignore')\n",
                "    \n",
                "    # S·∫Øp x·∫øp l·∫°i c√°c c·ªôt c·ªßa DAPT2020 theo ƒë√∫ng th·ª© t·ª± c·ªßa expected_features_cse\n",
                "    # v√† th√™m c·ªôt Label v√†o cu·ªëi\n",
                "    label_column_dapt = df_dapt_processed.pop('Stage')\n",
                "    df_dapt_processed = df_dapt_processed[expected_features_cse] # ƒê·∫£m b·∫£o ƒë√∫ng th·ª© t·ª± v√† ch·ªâ c√°c c·ªôt c·∫ßn thi·∫øt\n",
                "    df_dapt_processed['Stage'] = label_column_dapt\n",
                "\n",
                "    print(f\"      ‚úÖ DAPT2020 ƒë√£ ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ƒë·ªÉ c√≥ {len(expected_features_cse)} ƒë·∫∑c tr∆∞ng, shape: {df_dapt_processed.shape}\")\n",
                "\n",
                "    # --- 3.3 Chu·∫©n h√≥a D·ªØ li·ªáu DAPT2020 b·∫±ng Scaler c·ªßa CSE-CIC-IDS2018 --- \n",
                "    if 'scaler_cse' in locals() and expected_features_cse:\n",
                "        print(\"      Chu·∫©n h√≥a d·ªØ li·ªáu DAPT2020 s·ª≠ d·ª•ng scaler ƒë√£ fit tr√™n CSE-CIC-IDS2018...\")\n",
                "        X_dapt = df_dapt_processed.drop(columns=['Label'])\n",
                "        y_dapt = df_dapt_processed['Label']\n",
                "        \n",
                "        # ƒê·∫£m b·∫£o X_dapt ch·ªâ ch·ª©a c√°c c·ªôt m√† scaler ƒë√£ ƒë∆∞·ª£c fit\n",
                "        X_dapt_scaled_np = scaler_cse.transform(X_dapt[expected_features_cse]) # S·ª≠ d·ª•ng ƒë√∫ng expected_features_cse\n",
                "        X_dapt_scaled = pd.DataFrame(X_dapt_scaled_np, columns=expected_features_cse, index=X_dapt.index)\n",
                "        \n",
                "        df_dapt_final = X_dapt_scaled.copy()\n",
                "        df_dapt_final['Label'] = y_dapt.values\n",
                "        \n",
                "        print(f\"         Ph·∫°m vi ƒë·∫∑c tr∆∞ng DAPT2020 sau khi chu·∫©n h√≥a: {X_dapt_scaled_np.min():.4f} ƒë·∫øn {X_dapt_scaled_np.max():.4f}\")\n",
                "        print(f\"      ‚úÖ Chu·∫©n h√≥a DAPT2020 ho√†n t·∫•t. Shape cu·ªëi c√πng: {df_dapt_final.shape}\")\n",
                "    else:\n",
                "        print(\"      ‚ùå Kh√¥ng th·ªÉ chu·∫©n h√≥a DAPT2020 do thi·∫øu scaler ho·∫∑c danh s√°ch ƒë·∫∑c tr∆∞ng k·ª≥ v·ªçng.\")\n",
                "        df_dapt_final = df_dapt_processed.copy() # S·ª≠ d·ª•ng d·ªØ li·ªáu ch∆∞a chu·∫©n h√≥a n·∫øu l·ªói\n",
                "\n",
                "del df_dapt, df_dapt_processed, X_dapt, X_dapt_scaled # Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
                "gc.collect()\n",
                "check_ram()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Ph√¢n chia D·ªØ li·ªáu DAPT2020 v√† X·ª≠ l√Ω M·∫•t c√¢n b·∫±ng (N·∫øu c·∫ßn)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'df_dapt_final' not in locals() or df_dapt_final.empty:\n",
                "    print(\"‚ùå DataFrame DAPT2020 cu·ªëi c√πng (df_dapt_final) r·ªóng ho·∫∑c ch∆∞a ƒë∆∞·ª£c t·∫°o. Kh√¥ng th·ªÉ ti·∫øp t·ª•c.\")\n",
                "    # exit()\n",
                "else:\n",
                "    print(f\"üîÑ Ph√¢n chia d·ªØ li·ªáu DAPT2020 (shape: {df_dapt_final.shape})...\")\n",
                "    # Ki·ªÉm tra xem c·ªôt 'Label' c√≥ t·ªìn t·∫°i kh√¥ng\n",
                "    if 'Label' not in df_dapt_final.columns:\n",
                "        print(\"‚ùå L·ªói: C·ªôt 'Label' kh√¥ng t·ªìn t·∫°i trong df_dapt_final.\")\n",
                "        # exit()\n",
                "    else:\n",
                "        # Ki·ªÉm tra s·ªë l∆∞·ª£ng m·∫´u c·ªßa m·ªói l·ªõp tr∆∞·ªõc khi stratify\n",
                "        label_counts_dapt = df_dapt_final['Label'].value_counts()\n",
                "        print(f\"   Ph√¢n b·ªï nh√£n trong DAPT2020 tr∆∞·ªõc khi chia: \\n{label_counts_dapt}\")\n",
                "        \n",
                "        # Lo·∫°i b·ªè c√°c l·ªõp c√≥ √≠t h∆°n 2 m·∫´u (ƒëi·ªÅu ki·ªán cho stratify)\n",
                "        valid_labels_for_stratify = label_counts_dapt[label_counts_dapt >= 2].index\n",
                "        df_dapt_stratify_ready = df_dapt_final[df_dapt_final['Label'].isin(valid_labels_for_stratify)]\n",
                "        \n",
                "        if len(df_dapt_stratify_ready) < len(df_dapt_final):\n",
                "            print(f\"   ‚ö†Ô∏è ƒê√£ lo·∫°i b·ªè {len(df_dapt_final) - len(df_dapt_stratify_ready)} m·∫´u thu·ªôc c√°c l·ªõp c√≥ √≠t h∆°n 2 m·∫´u ƒë·ªÉ th·ª±c hi·ªán stratify.\")\n",
                "\n",
                "        if not df_dapt_stratify_ready.empty and len(df_dapt_stratify_ready['Label'].unique()) > 1:\n",
                "            df_train_dapt, df_test_dapt = train_test_split(\n",
                "                df_dapt_stratify_ready, \n",
                "                test_size=0.2, # T·ª∑ l·ªá t·∫≠p ki·ªÉm th·ª≠\n",
                "                random_state=42, \n",
                "                stratify=df_dapt_stratify_ready['Label']\n",
                "            )\n",
                "            print(f\"   ‚úÖ DAPT2020 ƒë√£ chia: Train shape: {df_train_dapt.shape}, Test shape: {df_test_dapt.shape}\")\n",
                "        elif not df_dapt_stratify_ready.empty:\n",
                "             print(\"   ‚ö†Ô∏è D·ªØ li·ªáu DAPT2020 ch·ªâ c√≤n m·ªôt l·ªõp duy nh·∫•t sau khi l·ªçc, kh√¥ng th·ªÉ stratify. S·ª≠ d·ª•ng train_test_split th√¥ng th∆∞·ªùng.\")\n",
                "             df_train_dapt, df_test_dapt = train_test_split(df_dapt_stratify_ready, test_size=0.2, random_state=42)\n",
                "             print(f\"   ‚úÖ DAPT2020 ƒë√£ chia: Train shape: {df_train_dapt.shape}, Test shape: {df_test_dapt.shape}\")\n",
                "        else:\n",
                "            print(\"   ‚ùå Kh√¥ng ƒë·ªß d·ªØ li·ªáu DAPT2020 ƒë·ªÉ chia sau khi l·ªçc c√°c l·ªõp √≠t m·∫´u.\")\n",
                "            df_train_dapt, df_test_dapt = pd.DataFrame(), pd.DataFrame()\n",
                "            # exit()\n",
                "\n",
                "        # (T√πy ch·ªçn) X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng tr√™n df_train_dapt n·∫øu c·∫ßn\n",
                "        print(\"\\n   Ki·ªÉm tra v√† x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng cho t·∫≠p hu·∫•n luy·ªán DAPT2020 (n·∫øu c·∫ßn)...\")\n",
                "        label_counts_train_dapt = df_train_dapt['Label'].value_counts()\n",
                "        print(f\"      Ph√¢n b·ªï nh√£n trong df_train_dapt: \\n{label_counts_train_dapt}\")\n",
                "        if any(label_counts_train_dapt < m·ªôt_ng∆∞·ª°ng_n√†o_ƒë√≥): \n",
                "            print(\"      √Åp d·ª•ng SMOTE cho df_train_dapt...\")\n",
                "            X_train_dapt_features = df_train_dapt.drop(columns=['Label'])\n",
                "            y_train_dapt_labels = df_train_dapt['Label']\n",
                "            smote_dapt = SMOTE(random_state=42) # C·∫•u h√¨nh sampling_strategy n·∫øu c·∫ßn\n",
                "            X_res_dapt, y_res_dapt = smote_dapt.fit_resample(X_train_dapt_features, y_train_dapt_labels)\n",
                "            df_train_dapt = pd.DataFrame(X_res_dapt, columns=X_train_dapt_features.columns)\n",
                "            df_train_dapt['Label'] = y_res_dapt\n",
                "            print(f\"      ‚úÖ SMOTE ho√†n t·∫•t. df_train_dapt shape m·ªõi: {df_train_dapt.shape}\")\n",
                "        else:\n",
                "            print(\"      Kh√¥ng c·∫ßn SMOTE cho df_train_dapt.\")\n",
                "\n",
                "del df_dapt_final, df_dapt_stratify_ready # Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
                "gc.collect()\n",
                "check_ram()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. T·∫£i M√¥ h√¨nh ƒë√£ Hu·∫•n luy·ªán v√† Th·ª±c hi·ªán Finetuning\n",
                "\n",
                "Ch√∫ng ta s·∫Ω t·∫£i m√¥ h√¨nh Random Forest v√† XGBoost ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n CSE-CIC-IDS2018."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üîÑ T·∫£i c√°c m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán t·ª´ CSE-CIC-IDS2018...\")\n",
                "rf_model_pretrained = None\n",
                "xgb_model_pretrained = None\n",
                "\n",
                "try:\n",
                "    RF_MODEL_PATH = os.path.join(PRETRAINED_MODEL_DIR, 'random_forest_model.pkl')\n",
                "    rf_model_pretrained = joblib.load(RF_MODEL_PATH)\n",
                "    print(f\"‚úÖ M√¥ h√¨nh Random Forest ƒë√£ t·∫£i t·ª´: {RF_MODEL_PATH}\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh Random Forest t·∫°i '{RF_MODEL_PATH}'.\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh Random Forest: {e}\")\n",
                "\n",
                "try:\n",
                "    # X√°c ƒë·ªãnh t√™n file model XGBoost (gpu_hist hay hist t√πy thu·ªôc v√†o l·∫ßn ch·∫°y tr∆∞·ªõc)\n",
                "    xgb_model_gpu_path = os.path.join(PRETRAINED_MODEL_DIR, 'xgboost_gpu_hist_model.pkl')\n",
                "    xgb_model_cpu_path = os.path.join(PRETRAINED_MODEL_DIR, 'xgboost_hist_model.pkl') # Gi·∫£ s·ª≠ c√≥ th·ªÉ c√≥ file n√†y\n",
                "    XGB_MODEL_PATH = ''\n",
                "    if os.path.exists(xgb_model_gpu_path):\n",
                "        XGB_MODEL_PATH = xgb_model_gpu_path\n",
                "    elif os.path.exists(xgb_model_cpu_path):\n",
                "        XGB_MODEL_PATH = xgb_model_cpu_path\n",
                "    else:\n",
                "        print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh XGBoost (gpu_hist ho·∫∑c hist) trong '{PRETRAINED_MODEL_DIR}'.\")\n",
                "    \n",
                "    if XGB_MODEL_PATH:\n",
                "        xgb_model_pretrained = joblib.load(XGB_MODEL_PATH)\n",
                "        print(f\"‚úÖ M√¥ h√¨nh XGBoost ƒë√£ t·∫£i t·ª´: {XGB_MODEL_PATH}\")\n",
                "except FileNotFoundError: # B·∫Øt l·ªói c·ª• th·ªÉ n·∫øu XGB_MODEL_PATH v·∫´n r·ªóng\n",
                "     pass # ƒê√£ x·ª≠ l√Ω ·ªü tr√™n\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh XGBoost: {e}\")\n",
                "\n",
                "# Chu·∫©n b·ªã d·ªØ li·ªáu DAPT2020 cho hu·∫•n luy·ªán (n·∫øu df_train_dapt v√† df_test_dapt ƒë√£ ƒë∆∞·ª£c t·∫°o)\n",
                "if 'df_train_dapt' in locals() and not df_train_dapt.empty:\n",
                "    X_train_dapt_np = df_train_dapt.drop(columns=['Label']).values\n",
                "    y_train_dapt_np = df_train_dapt['Label'].values\n",
                "else:\n",
                "    print(\"‚ùå D·ªØ li·ªáu hu·∫•n luy·ªán DAPT2020 (df_train_dapt) ch∆∞a s·∫µn s√†ng.\")\n",
                "    # exit()\n",
                "\n",
                "if 'df_test_dapt' in locals() and not df_test_dapt.empty:\n",
                "    X_test_dapt_np = df_test_dapt.drop(columns=['Label']).values\n",
                "    y_test_dapt_np = df_test_dapt['Label'].values\n",
                "else:\n",
                "    print(\"‚ùå D·ªØ li·ªáu ki·ªÉm th·ª≠ DAPT2020 (df_test_dapt) ch∆∞a s·∫µn s√†ng.\")\n",
                "    # exit()\n",
                "\n",
                "# L·∫•y class_names t·ª´ label_mapping c·ªßa CSE-CIC-IDS2018 (gi·∫£ ƒë·ªãnh DAPT2020 ƒë√£ ƒë∆∞·ª£c map v√†o c√°c l·ªõp n√†y)\n",
                "if 'label_mapping_cse' in locals():\n",
                "    class_names_for_eval = [k for k, v in sorted(label_mapping_cse.items(), key=lambda item: item[1])]\n",
                "    num_classes_for_eval = len(class_names_for_eval)\n",
                "else:\n",
                "    print(\"‚ùå Thi·∫øu label_mapping_cse. Kh√¥ng th·ªÉ x√°c ƒë·ªãnh t√™n l·ªõp ƒë·ªÉ ƒë√°nh gi√°.\")\n",
                "    # T·∫°o class_names m·∫∑c ƒë·ªãnh n·∫øu c·∫ßn ƒë·ªÉ tr√°nh l·ªói, nh∆∞ng k·∫øt qu·∫£ s·∫Ω kh√≥ di·ªÖn gi·∫£i\n",
                "    max_label_val = 0\n",
                "    if 'y_train_dapt_np' in locals() and len(y_train_dapt_np) > 0:\n",
                "        max_label_val = max(max_label_val, y_train_dapt_np.max())\n",
                "    if 'y_test_dapt_np' in locals() and len(y_test_dapt_np) > 0:\n",
                "        max_label_val = max(max_label_val, y_test_dapt_np.max())\n",
                "    class_names_for_eval = [f\"Class_{i}\" for i in range(int(max_label_val) + 1)]\n",
                "    num_classes_for_eval = len(class_names_for_eval)\n",
                "\n",
                "clean_ram()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Finetuning/Hu·∫•n luy·ªán l·∫°i Random Forest tr√™n DAPT2020\n",
                "\n",
                "Random Forest kh√¥ng d·ªÖ ƒë·ªÉ \"finetune\" b·∫±ng c√°ch hu·∫•n luy·ªán ti·∫øp. Thay v√†o ƒë√≥, ch√∫ng ta s·∫Ω hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh RF m·ªõi tr√™n DAPT2020, nh∆∞ng c√≥ th·ªÉ s·ª≠ d·ª•ng l·∫°i c√°c si√™u tham s·ªë t·ªët t·ª´ m√¥ h√¨nh tr∆∞·ªõc ƒë√≥ n·∫øu mu·ªën."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'X_train_dapt_np' in locals() and rf_model_pretrained is not None:\n",
                "    print(\"\\n\" + \"=\"*10 + \" üå≥ Finetuning/Hu·∫•n luy·ªán l·∫°i Random Forest tr√™n DAPT2020 \" + \"=\"*10)\n",
                "    \n",
                "    # L·∫•y c√°c tham s·ªë t·ª´ m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán tr∆∞·ªõc (n·∫øu mu·ªën s·ª≠ d·ª•ng l·∫°i c·∫•u h√¨nh)\n",
                "    rf_params = rf_model_pretrained.get_params()\n",
                "    # C√≥ th·ªÉ b·∫°n mu·ªën gi·∫£m n_estimators ho·∫∑c max_depth khi finetuning tr√™n t·∫≠p nh·ªè h∆°n\n",
                "    # ho·∫∑c n·∫øu DAPT2020 r·∫•t kh√°c bi·ªát v√† b·∫°n mu·ªën m√¥ h√¨nh h·ªçc l·∫°i nhi·ªÅu h∆°n.\n",
                "    # rf_params['n_estimators'] = 50 # V√≠ d·ª•: gi·∫£m s·ªë c√¢y\n",
                "    # rf_params['warm_start'] = True # Cho ph√©p th√™m c√¢y v√†o m√¥ h√¨nh hi·ªán c√≥ - KH√îNG ph·∫£i l√∫c n√†o c≈©ng hi·ªáu qu·∫£\n",
                "                                    # v√† th∆∞·ªùng th√¨ hu·∫•n luy·ªán l·∫°i t·ª´ ƒë·∫ßu v·ªõi c·∫•u h√¨nh t·ªët s·∫Ω ·ªïn ƒë·ªãnh h∆°n.\n",
                "                                    # ƒê·ªëi v·ªõi sklearn Random Forest, warm_start ch·ªâ ho·∫°t ƒë·ªông khi fit l·∫°i tr√™n c√πng d·ªØ li·ªáu\n",
                "                                    # ho·∫∑c khi tƒÉng n_estimators. N√≥ kh√¥ng th·ª±c s·ª± \"h·ªçc ti·∫øp\" tr√™n d·ªØ li·ªáu m·ªõi.\n",
                "                                    # Do ƒë√≥, ch√∫ng ta s·∫Ω hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh m·ªõi v·ªõi c√°c tham s·ªë ƒë√≥.\n",
                "    \n",
                "    rf_model_dapt_tuned = RandomForestClassifier(**rf_params) # S·ª≠ d·ª•ng c√°c tham s·ªë ƒë√£ l·∫•y\n",
                "    # Ho·∫∑c ƒë·ªãnh nghƒ©a l·∫°i ho√†n to√†n:\n",
                "    # rf_model_dapt_tuned = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\n",
                "\n",
                "    print(f\"   - Kh·ªüi t·∫°o Random Forest cho DAPT2020 v·ªõi params: {rf_model_dapt_tuned.get_params().get('n_estimators', 'N/A')} estimators, max_depth {rf_model_dapt_tuned.get_params().get('max_depth', 'N/A')}\")\n",
                "\n",
                "    print(\"   - ƒêang hu·∫•n luy·ªán Random Forest tr√™n DAPT2020...\")\n",
                "    start_rf_dapt_train_time = time.time()\n",
                "    try:\n",
                "        rf_model_dapt_tuned.fit(X_train_dapt_np, y_train_dapt_np)\n",
                "        rf_dapt_train_time = time.time() - start_rf_dapt_train_time\n",
                "        print(f\"   ‚úÖ Hu·∫•n luy·ªán Random Forest tr√™n DAPT2020 ho√†n th√†nh trong {rf_dapt_train_time:.2f} gi√¢y.\")\n",
                "\n",
                "        # L∆∞u m√¥ h√¨nh ƒë√£ finetune\n",
                "        rf_dapt_tuned_model_path = os.path.join(OUTPUT_DIR, \"random_forest_model_finetuned_dapt2020.pkl\")\n",
                "        joblib.dump(rf_model_dapt_tuned, rf_dapt_tuned_model_path)\n",
                "        print(f\"   üíæ M√¥ h√¨nh Random Forest ƒë√£ finetune ƒë∆∞·ª£c l∆∞u v√†o: '{rf_dapt_tuned_model_path}'\")\n",
                "\n",
                "        # ƒê√°nh gi√°\n",
                "        if 'X_test_dapt_np' in locals():\n",
                "            evaluate_model_finetuned(rf_model_dapt_tuned, X_test_dapt_np, y_test_dapt_np, class_names_for_eval, \"Random Forest (Finetuned DAPT2020)\", OUTPUT_DIR)\n",
                "        else:\n",
                "            print(\"   ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ki·ªÉm th·ª≠ DAPT2020 ƒë·ªÉ ƒë√°nh gi√° Random Forest.\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ùå L·ªói khi hu·∫•n luy·ªán/ƒë√°nh gi√° Random Forest tr√™n DAPT2020: {e}\")\n",
                "    \n",
                "    del rf_model_dapt_tuned\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Kh√¥ng th·ªÉ finetune Random Forest: thi·∫øu d·ªØ li·ªáu hu·∫•n luy·ªán DAPT2020 ho·∫∑c m√¥ h√¨nh Random Forest g·ªëc.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Finetuning XGBoost tr√™n DAPT2020\n",
                "\n",
                "XGBoost cho ph√©p hu·∫•n luy·ªán ti·∫øp m·ªôt m√¥ h√¨nh ƒë√£ c√≥ b·∫±ng c√°ch truy·ªÅn m√¥ h√¨nh ƒë√≥ v√†o tham s·ªë `xgb_model` c·ªßa h√†m `fit`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'X_train_dapt_np' in locals() and xgb_model_pretrained is not None:\n",
                "    print(\"\\n\" + \"=\"*10 + \" üöÄ Finetuning XGBoost tr√™n DAPT2020 \" + \"=\"*10)\n",
                "    \n",
                "    # L·∫•y c√°c tham s·ªë t·ª´ m√¥ h√¨nh XGBoost ƒë√£ hu·∫•n luy·ªán tr∆∞·ªõc\n",
                "    # M·ªôt s·ªë tham s·ªë nh∆∞ n_estimators c√≥ th·ªÉ kh√¥ng c·∫ßn thi·∫øt khi hu·∫•n luy·ªán ti·∫øp\n",
                "    xgb_params_pretrained = xgb_model_pretrained.get_params()\n",
                "    print(f\"   - Tham s·ªë c·ªßa XGBoost g·ªëc: {xgb_params_pretrained.get('n_estimators')}, lr: {xgb_params_pretrained.get('learning_rate')}, max_depth: {xgb_params_pretrained.get('max_depth')}\")\n",
                "\n",
                "    # Thi·∫øt l·∫≠p tree_method (gpu_hist ho·∫∑c hist) cho finetuning\n",
                "    tree_method_finetune = 'hist' # M·∫∑c ƒë·ªãnh CPU\n",
                "    if torch.cuda.is_available():\n",
                "        tree_method_finetune = 'gpu_hist'\n",
                "        print(f\"   - S·∫Ω s·ª≠ d·ª•ng 'tree_method': '{tree_method_finetune}' cho finetuning XGBoost.\")\n",
                "    else:\n",
                "        print(f\"   - GPU kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng 'tree_method': '{tree_method_finetune}' (CPU) cho finetuning XGBoost.\")\n",
                "\n",
                "    # T·∫°o m·ªôt ƒë·ªëi t∆∞·ª£ng XGBClassifier m·ªõi ƒë·ªÉ finetune. \n",
                "    # Quan tr·ªçng: num_class ph·∫£i kh·ªõp v·ªõi s·ªë l·ªõp trong y_train_dapt_np sau khi ƒë√£ √°nh x·∫°!\n",
                "    # N·∫øu b·∫°n ƒë√£ √°nh x·∫° nh√£n DAPT2020 v√†o h·ªá th·ªëng N l·ªõp c·ªßa CSE, th√¨ num_class gi·ªØ nguy√™n.\n",
                "    # N·∫øu b·∫°n t·∫°o th√™m l·ªõp m·ªõi cho DAPT2020 (kh√≥ h∆°n), num_class ph·∫£i thay ƒë·ªïi.\n",
                "\n",
                "    current_num_classes_dapt = len(np.unique(y_train_dapt_np)) if 'y_train_dapt_np' in locals() else num_classes_for_eval\n",
                "\n",
                "    xgb_model_dapt_finetuned = xgb.XGBClassifier(\n",
                "        objective='multi:softmax',\n",
                "        num_class=current_num_classes_dapt, # S·ªë l·ªõp trong d·ªØ li·ªáu DAPT2020 ƒë√£ map\n",
                "        # Gi·ªØ l·∫°i c√°c tham s·ªë c·∫•u tr√∫c c√¢y quan tr·ªçng t·ª´ m√¥ h√¨nh g·ªëc, ho·∫∑c ƒëi·ªÅu ch·ªânh:\n",
                "        learning_rate=xgb_params_pretrained.get('learning_rate', 0.1) * 0.5, # C√≥ th·ªÉ gi·∫£m learning rate khi finetuning\n",
                "        max_depth=xgb_params_pretrained.get('max_depth', 6),\n",
                "        subsample=xgb_params_pretrained.get('subsample', 0.8),\n",
                "        colsample_bytree=xgb_params_pretrained.get('colsample_bytree', 0.8),\n",
                "        gamma=xgb_params_pretrained.get('gamma', 0),\n",
                "        n_estimators=100, # S·ªë l∆∞·ª£ng c√¢y m·ªõi ƒë·ªÉ th√™m v√†o khi finetuning (ƒëi·ªÅu ch·ªânh)\n",
                "        random_state=42,\n",
                "        use_label_encoder=False, # ƒê√£ b·ªè t·ª´ XGBoost >1.3\n",
                "        eval_metric='mlogloss',\n",
                "        n_jobs=-1,\n",
                "        tree_method=tree_method_finetune\n",
                "    )\n",
                "\n",
                "    print(f\"   - ƒêang finetuning XGBoost tr√™n DAPT2020 (b·∫Øt ƒë·∫ßu t·ª´ m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán tr√™n CSE-CIC-IDS2018)...\")\n",
                "    print(f\"     Tham s·ªë finetuning: lr={xgb_model_dapt_finetuned.learning_rate}, n_est={xgb_model_dapt_finetuned.n_estimators}\")\n",
                "    start_xgb_dapt_train_time = time.time()\n",
                "    try:\n",
                "        # Hu·∫•n luy·ªán ti·∫øp (finetune)\n",
                "        xgb_model_dapt_finetuned.fit(\n",
                "            X_train_dapt_np, \n",
                "            y_train_dapt_np, \n",
                "            xgb_model=xgb_model_pretrained, # ƒê√¢y l√† ƒëi·ªÉm m·∫•u ch·ªët ƒë·ªÉ finetune\n",
                "            verbose=False\n",
                "        )\n",
                "        xgb_dapt_train_time = time.time() - start_xgb_dapt_train_time\n",
                "        print(f\"   ‚úÖ Finetuning XGBoost tr√™n DAPT2020 ho√†n th√†nh trong {xgb_dapt_train_time:.2f} gi√¢y.\")\n",
                "\n",
                "        # L∆∞u m√¥ h√¨nh ƒë√£ finetune\n",
                "        xgb_dapt_finetuned_model_path = os.path.join(OUTPUT_DIR, f\"xgboost_{tree_method_finetune}_model_finetuned_dapt2020.pkl\")\n",
                "        joblib.dump(xgb_model_dapt_finetuned, xgb_dapt_finetuned_model_path)\n",
                "        print(f\"   üíæ M√¥ h√¨nh XGBoost ƒë√£ finetune ƒë∆∞·ª£c l∆∞u v√†o: '{xgb_dapt_finetuned_model_path}'\")\n",
                "\n",
                "        # ƒê√°nh gi√°\n",
                "        if 'X_test_dapt_np' in locals():\n",
                "            evaluate_model_finetuned(xgb_model_dapt_finetuned, X_test_dapt_np, y_test_dapt_np, class_names_for_eval, f\"XGBoost ({tree_method_finetune}, Finetuned DAPT2020)\", OUTPUT_DIR)\n",
                "        else:\n",
                "            print(\"   ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ki·ªÉm th·ª≠ DAPT2020 ƒë·ªÉ ƒë√°nh gi√° XGBoost.\")\n",
                "\n",
                "    except xgb.core.XGBoostError as e_xgb:\n",
                "        print(f\"   ‚ùå L·ªói XGBoost khi finetuning tr√™n DAPT2020: {e_xgb}\")\n",
                "        if \"CUDA\" in str(e_xgb) or \"GPU\" in str(e_xgb):\n",
                "            print(\"      L·ªói c√≥ th·ªÉ li√™n quan ƒë·∫øn c√†i ƒë·∫∑t CUDA/GPU. Th·ª≠ l·∫°i v·ªõi tree_method='hist' (CPU).\")\n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ùå L·ªói khi finetuning/ƒë√°nh gi√° XGBoost tr√™n DAPT2020: {e}\")\n",
                "\n",
                "    del xgb_model_dapt_finetuned\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Kh√¥ng th·ªÉ finetune XGBoost: thi·∫øu d·ªØ li·ªáu hu·∫•n luy·ªán DAPT2020 ho·∫∑c m√¥ h√¨nh XGBoost g·ªëc.\")\n",
                "\n",
                "clean_ram()\n",
                "print(\"\\nüèÅüèÅüèÅ Qu√° tr√¨nh finetuning ho√†n t·∫•t! üèÅüèÅüèÅ\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
