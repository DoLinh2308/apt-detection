import pandas as pd
import numpy as np
import joblib 
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder # Example scalers/encoders
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve, auc

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

# --- Cáº¥u hÃ¬nh ---
PATH_TO_MODELS = 'D:/Do_an_tot_nghiep/apt-detection/ai_model/dataset/working2/'
RF_MODEL_FILE = 'random_forest_model.pkl'
XGB_MODEL_FILE = 'xgboost_model.pkl'
DAPT2020_DATA_FILE = 'D:/Do_an_tot_nghiep/apt-detection/ai_model/dataset/DAPT-2020/merged_cleaned.csv' 


# TODO: Cáº¤U HÃŒNH Cá»˜T NHÃƒN TRONG DAPT2020
# Cáº§n biáº¿t tÃªn cá»™t chá»©a nhÃ£n táº¥n cÃ´ng trong DAPT2020.
LABEL_COLUMN = 'label' # Example: The column in DAPT2020 that contains attack types/APT label

# TODO: Äá»ŠNH NGHÄ¨A NHÃƒN APT VÃ€ BENIGN
# Báº¡n cáº§n Ã¡nh xáº¡ cÃ¡c giÃ¡ trá»‹ trong LABEL_COLUMN cá»§a DAPT2020 sang 0 (Benign) vÃ  1 (APT)
# VÃ Dá»¤ (THAY THáº¾ Báº°NG CÃC NHÃƒN THáº¬T Cá»¦A Báº N):
print("\nðŸ” Checking for Extra Spaces in Attack Labels...")

APT_LABELS_IN_DAPT2020 = ['Lateral Movement', 'Reconnaissance', 'Establish Foothold', 'Data Exfiltration']
BENIGN_LABELS_IN_DAPT2020 = ['Benign', 'BENIGN']

FEATURE_COLUMNS = [
    'Dst Port',
    'Flow Duration',
    'Tot Fwd Pkts',
    'Tot Bwd Pkts',
    'TotLen Fwd Pkts',
    'TotLen Bwd Pkts',
    'Fwd Pkt Len Max',
    'Fwd Pkt Len Min',
    'Fwd Pkt Len Mean',
    'Fwd Pkt Len Std',
    'Bwd Pkt Len Max',
    'Bwd Pkt Len Min',
    'Bwd Pkt Len Mean',
    'Bwd Pkt Len Std',
    'Flow Byts/s',
    'Flow Pkts/s',
    'Flow IAT Mean',
    'Flow IAT Std',
    'Flow IAT Max',
    'Flow IAT Min',
    'Fwd IAT Tot',
    'Fwd IAT Mean',
    'Fwd IAT Std',
    'Fwd IAT Max',
    'Fwd IAT Min',
    'Bwd IAT Tot',
    'Bwd IAT Mean',
    'Bwd IAT Std',
    'Bwd IAT Max',
    'Bwd IAT Min',
    'Fwd PSH Flags',
    'Fwd URG Flags',
    'Fwd Header Len',
    'Bwd Header Len',
    'Fwd Pkts/s',
    'Bwd Pkts/s',
    'Pkt Len Min',
    'Pkt Len Max',
    'Pkt Len Mean',
    'Pkt Len Std',
    'Pkt Len Var',
    'FIN Flag Cnt',
    'SYN Flag Cnt',
    'RST Flag Cnt',
    'PSH Flag Cnt',
    'ACK Flag Cnt',
    'URG Flag Cnt',
    'CWE Flag Count',
    'ECE Flag Cnt',
    'Down/Up Ratio',
    'Pkt Size Avg',
    'Fwd Seg Size Avg',
    'Bwd Seg Size Avg',
    'Subflow Fwd Pkts',
    'Subflow Fwd Byts',
    'Subflow Bwd Pkts',
    'Subflow Bwd Byts',
    'Init Fwd Win Byts',
    'Init Bwd Win Byts',
    'Fwd Act Data Pkts',
    'Fwd Seg Size Min',
    'Active Mean',
    'Active Std',
    'Active Max',
    'Active Min',
    'Idle Mean',
    'Idle Std',
    'Idle Max',
    'Idle Min',
]

# TODO: XÃC Äá»ŠNH CÃC Cá»˜T Cáº¦N TIá»€N Xá»¬ LÃ TRONG DAPT2020 Dá»°A TRÃŠN CÃCH Báº N ÄÃƒ LÃ€M Vá»šI CIC-IDS2018
# Äáº·c biá»‡t lÆ°u Ã½ cá»™t Protocol. Náº¿u DAPT2020 cÃ³ cá»™t Protocol gá»‘c, báº¡n cáº§n mÃ£ hÃ³a nÃ³.
# Náº¿u DAPT2020 Ä‘Ã£ cÃ³ sáºµn cÃ¡c cá»™t Protocol_0, Protocol_6, Protocol_17, thÃ¬ coi chÃºng lÃ  sá»‘.

# VÃ Dá»¤ (Cáº§n Ä‘iá»u chá»‰nh dá»±a trÃªn tiá»n xá»­ lÃ½ thá»±c táº¿ cá»§a báº¡n):
# Giáº£ Ä‘á»‹nh háº§u háº¿t cÃ¡c cá»™t lÃ  sá»‘ vÃ  Ä‘Ã£ Ä‘Æ°á»£c scale, cÃ²n Protocol_X lÃ  káº¿t quáº£ OHE vÃ  khÃ´ng scale thÃªm.
NUMERICAL_FEATURES_TO_SCALE = [
    'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts',
    'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min',
    'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max',
    'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std',
    'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std',
    'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean',
    'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot',
    'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',
    'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',
    'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std',
    'Pkt Len Var', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg',
    'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts',
    'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts',
    'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min',
    'Active Mean', 'Active Std', 'Active Max', 'Active Min',
    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'
]

# CÃ¡c cá»™t cá» vÃ  Protocol_X thÆ°á»ng lÃ  0/1 hoáº·c Ä‘áº¿m, cÃ³ thá»ƒ khÃ´ng cáº§n scale
# Náº¿u báº¡n Ä‘Ã£ scale chÃºng, thÃªm vÃ o danh sÃ¡ch trÃªn. Náº¿u khÃ´ng, bá» qua.
FEATURES_NOT_SCALED_BUT_NUMERICAL = [
    'Dst Port', # Cá»•ng Ä‘Ã­ch - cÃ³ thá»ƒ coi lÃ  sá»‘ hoáº·c phÃ¢n loáº¡i tÃ¹y cÃ¡ch dÃ¹ng
    'Fwd PSH Flags', 'Fwd URG Flags', 'FIN Flag Cnt', 'SYN Flag Cnt',
    'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',
    'CWE Flag Count', 'ECE Flag Cnt',
    'Protocol_0', 'Protocol_6', 'Protocol_17' # Káº¿t quáº£ cá»§a OHE Protocol
]

# CÃ¡c cá»™t phÃ¢n loáº¡i gá»‘c (náº¿u DAPT2020 cÃ²n cá»™t Protocol gá»‘c, cáº§n mÃ£ hÃ³a)
# Náº¿u DAPT2020 Ä‘Ã£ cÃ³ Protocol_X, thÃ¬ khÃ´ng cáº§n cá»™t phÃ¢n loáº¡i gá»‘c ná»¯a.
# CATEGORICAL_FEATURES_TO_ENCODE = ['Protocol'] # Example if DAPT2020 has original Protocol column

# --- Load MÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n trÃªn CIC-IDS2018 ---
print("Loading existing models...")
try:
    rf_model_cicids = joblib.load(os.path.join(PATH_TO_MODELS, RF_MODEL_FILE))
    xgb_model_cicids = joblib.load(os.path.join(PATH_TO_MODELS, XGB_MODEL_FILE))
    print("Models loaded successfully.")
    # Optional: print some info about loaded models
    # print("RF Model Params:", rf_model_cicids.get_params())
    # print("XGB Model Params:", xgb_model_cicids.get_params())
except FileNotFoundError as e:
    print(f"Error loading model files: {e}")
    print("Please check the file paths and names.")
    exit() # Exit if models can't be loaded

# --- Load Dá»¯ liá»‡u DAPT2020 ---
print(f"Loading DAPT2020 data from {DAPT2020_DATA_FILE}...")
try:
    # TODO: Kiá»ƒm tra encoding cá»§a file CSV DAPT2020 náº¿u gáº·p lá»—i Ä‘á»c file
    df_dapt2020 = pd.read_csv(DAPT2020_DATA_FILE)
    print(f"DAPT2020 data loaded. Shape: {df_dapt2020.shape}")
    # Optional: Display first few rows and info
    # print(df_dapt2020.head())
    # print(df_dapt2020.info())
except FileNotFoundError as e:
    print(f"Error loading DAPT2020 data file: {e}")
    print("Please check the file path.")
    exit()
except Exception as e:
     print(f"Error reading DAPT2020 data file: {e}")
     exit()


# --- Tiá»n xá»­ lÃ½ DAPT2020 vÃ  Chuáº©n bá»‹ NhÃ£n APT ---

# 1. Xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ khÃ´ng xÃ¡c Ä‘á»‹nh/thiáº¿u trong DAPT2020
# TODO: ÃP Dá»¤NG CÃCH Xá»¬ LÃ GIÃ TRá»Š THIáº¾U NHáº¤T QUÃN Vá»šI CIC-IDS2018
# VÃ Dá»¤: Ä‘iá»n báº±ng 0, giÃ¡ trá»‹ trung bÃ¬nh, giÃ¡ trá»‹ mode, hoáº·c loáº¡i bá» hÃ ng
print("Preprocessing DAPT2020 data...")
# Example: fill NaNs with 0 (adjust if needed)
df_dapt2020.fillna(0, inplace=True)

# TODO: Ãp dá»¥ng xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ vÃ´ háº¡n (inf) náº¿u cÃ³
df_dapt2020.replace([np.inf, -np.inf], np.nan, inplace=True)
df_dapt2020.fillna(0, inplace=True) # Fill NaNs created by replacing inf

# 2. Ãnh xáº¡ nhÃ£n gá»‘c sang nhÃ£n Benign (0) vÃ  APT (1)
# TODO: ÃP Dá»¤NG LOGIC GÃN NHÃƒN APT/BENIGN Cá»¦A Báº N
df_dapt2020['APT_Label'] = 0 # Default to Benign (0)
df_dapt2020.loc[df_dapt2020[LABEL_COLUMN].isin(APT_LABELS_IN_DAPT2020), 'APT_Label'] = 1 # Mark APT (1)

# Optional: Remove rows that are neither Benign nor APT if necessary
# print(f"Original number of rows: {df_dapt2020.shape[0]}")
# df_dapt2020 = df_dapt2020[df_dapt2020[LABEL_COLUMN].isin(APT_LABELS_IN_DAPT2020 + BENIGN_LABELS_IN_DAPT2020)].copy()
# print(f"Rows after filtering labels: {df_dapt2020.shape[0]}")


# TÃ¡ch Ä‘áº·c trÆ°ng (X) vÃ  nhÃ£n (y) TRÆ¯á»šC KHI Ã¡p dá»¥ng cÃ¡c transformer
try:
    # Äáº£m báº£o chá»‰ chá»n cÃ¡c cá»™t Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘á»‹nh nghÄ©a
    X_dapt = df_dapt2020[FEATURE_COLUMNS].copy()
    y_dapt = df_dapt2020['APT_Label'].copy()
except KeyError as e:
    print(f"Error: Missing a defined feature or label column in DAPT2020 data: {e}")
    print("Please check if all columns in FEATURE_COLUMNS and LABEL_COLUMN exist in your DAPT2020 data.")
    exit()

print(f"Features shape before transformation: {X_dapt.shape}")
print(f"Labels shape: {y_dapt.shape}")
print(f"Label distribution in DAPT2020: {Counter(y_dapt)}")

# 3. Ãp dá»¥ng tiá»n xá»­ lÃ½ cho cÃ¡c Ä‘áº·c trÆ°ng (Scaling, Encoding, etc.)
# TODO: Táº O VÃ€ FIT TRANSFORMER HOáº¶C ÃP Dá»¤NG SCALER ÄÃƒ FIT Tá»ª CIC-IDS2018
# CÃ¡ch lÃ½ tÆ°á»Ÿng lÃ  sá»­ dá»¥ng láº¡i scaler/encoder Ä‘Ã£ fit trÃªn CIC-IDS2018 Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n.
# Náº¿u khÃ´ng cÃ³ scaler/encoder Ä‘Ã£ lÆ°u, báº¡n cÃ³ thá»ƒ fit má»›i trÃªn DAPT2020 hoáº·c táº­p dá»¯ liá»‡u káº¿t há»£p.
print("Applying feature transformations...")

# --- Example: Create a preprocessor pipeline (adapt based on your actual preprocessing) ---
# Báº¡n cáº§n Ä‘iá»u chá»‰nh nÃ y cho phÃ¹ há»£p vá»›i cÃ¡ch báº¡n Ä‘Ã£ tiá»n xá»­ lÃ½ dá»¯ liá»‡u CIC-IDS2018.
# Náº¿u báº¡n Ä‘Ã£ lÆ°u Ä‘á»‘i tÆ°á»£ng preprocessor tá»« láº§n huáº¥n luyá»‡n trÆ°á»›c, hÃ£y load nÃ³ vÃ  chá»‰ gá»i transform.
# VÃ­ dá»¥: preprocessor = joblib.load('cicids_preprocessor.pkl')
# X_dapt_processed = preprocessor.transform(X_dapt)
# -----------------------------------------------------------------------------------------

# Náº¿u báº¡n cáº§n fit láº¡i preprocessor trÃªn dá»¯ liá»‡u má»›i (Ã­t lÃ½ tÆ°á»Ÿng hÆ¡n nhÆ°ng Ä‘Ã´i khi cáº§n):
# VÃ Dá»¤ NÃ€Y GIáº¢ Äá»ŠNH Báº N SCALE CÃC Cá»˜T TRONG NUMERICAL_FEATURES_TO_SCALE
# VÃ€ Äá»‚ NGUYÃŠN CÃC Cá»˜T KHÃC BAO Gá»’M CÃC Cá»˜T Cá»œ VÃ€ PROTOCOL_X
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), NUMERICAL_FEATURES_TO_SCALE)
        # TODO: Add other transformers if you had categorical features in CIC-IDS2018 besides Protocol
        # ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_TO_ENCODE)
    ],
    # Keep other columns (like flag counts, Dst Port, Protocol_X) as they are
    remainder='passthrough'
)

# Fit and transform the DAPT2020 data
X_dapt_processed = preprocessor.fit_transform(X_dapt)
print(f"Processed data shape: {X_dapt_processed.shape}")
print("DAPT2020 preprocessing complete.")

# --- TÃ¹y chá»n: Káº¿t há»£p dá»¯ liá»‡u (Náº¿u chá»n phÆ°Æ¡ng Ã¡n nÃ y) ---
# TODO: Náº¾U Báº N MUá»N Káº¾T Há»¢P Dá»® LIá»†U CIC-IDS2018 VÃ€ DAPT2020
# Báº¡n cáº§n load vÃ  tiá»n xá»­ lÃ½ má»™t pháº§n dá»¯ liá»‡u CIC-IDS2018 (benign vÃ /hoáº·c cÃ¡c loáº¡i táº¥n cÃ´ng liÃªn quan)
# sá»­ dá»¥ng CÃ™NG Ä‘á»‘i tÆ°á»£ng preprocessor ÄÃƒ FIT (hoáº·c fit má»›i trÃªn toÃ n bá»™ dá»¯ liá»‡u káº¿t há»£p ban Ä‘áº§u).
# Sau Ä‘Ã³ ná»‘i X vÃ  y tá»« hai nguá»“n láº¡i.
# VÃ­ dá»¥ (Conceptual - needs actual CIC-IDS2018 loading and preprocessing):
# X_cicids_subset, y_cicids_subset = load_and_preprocess_cicids_subset(...) # Ensure this also uses the *same* preprocessor
# X_combined = np.vstack((X_dapt_processed, X_cicids_subset))
# y_combined = np.concatenate((y_dapt, y_cicids_subset))
# print(f"Combined data shape: {X_combined.shape}")
# X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined) # Use stratify for imbalance
# print(f"Combined Train/Test shapes: {X_train_final.shape} / {X_test_final.shape}")


# --- Chia táº­p dá»¯ liá»‡u (Náº¿u chá»‰ dÃ¹ng DAPT2020) ---
# Náº¿u chá»‰ sá»­ dá»¥ng DAPT2020 Ä‘á»ƒ huáº¥n luyá»‡n láº¡i
print("Splitting DAPT2020 data into train and test sets...")
X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(
    X_dapt_processed, y_dapt,
    test_size=0.2,      # 20% for testing (adjust as needed)
    random_state=42,    # for reproducibility
    stratify=y_dapt     # Stratify to maintain class distribution (crucial for imbalance)
)
print(f"Train/Test shapes: {X_train_final.shape} / {X_test_final.shape}")
print(f"Train Label Distribution: {Counter(y_train_final)}")
print(f"Test Label Distribution: {Counter(y_test_final)}")


# --- Xá»­ lÃ½ máº¥t cÃ¢n báº±ng lá»›p (trÃªn táº­p huáº¥n luyá»‡n) ---
# Táº¥n cÃ´ng APT (nhÃ£n 1) cÃ³ thá»ƒ ráº¥t Ã­t trong táº­p huáº¥n luyá»‡n.
# CÃ³ nhiá»u cÃ¡ch xá»­ lÃ½:
# 1. DÃ¹ng class_weight trong mÃ´ hÃ¬nh (Ä‘á»‘i vá»›i RF)
# 2. DÃ¹ng scale_pos_weight trong mÃ´ hÃ¬nh (Ä‘á»‘i vá»›i XGBoost binary classification)
# 3. Ãp dá»¥ng ká»¹ thuáº­t láº¥y máº«u (sampling) nhÆ° SMOTE, Undersampling (sá»­ dá»¥ng thÆ° viá»‡n imblearn)
print("Handling class imbalance...")

# PhÆ°Æ¡ng Ã¡n 1/2: Sá»­ dá»¥ng trá»ng sá»‘ lá»›p tÃ­ch há»£p trong mÃ´ hÃ¬nh
# TÃ­nh toÃ¡n trá»ng sá»‘ lá»›p cho RF (sá»­ dá»¥ng 'balanced' hoáº·c dict tÃ¹y chá»n)
# from sklearn.utils.class_weight import compute_class_weight
# classes = np.unique(y_train_final)
# weights = compute_class_weight('balanced', classes=classes, y=y_train_final)
# class_weights_dict = dict(zip(classes, weights))
# print("Computed class weights for RF:", class_weights_dict) # Pass this dict to RF's fit method or use 'balanced'

# TÃ­nh toÃ¡n scale_pos_weight cho XGBoost (phÃ¢n loáº¡i nhá»‹ phÃ¢n 0/1)
apt_count_train = sum(y_train_final == 1)
benign_count_train = sum(y_train_final == 0)
# TrÃ¡nh chia cho 0 náº¿u khÃ´ng cÃ³ máº«u APT trong táº­p huáº¥n luyá»‡n (dÃ¹ stratify cá»‘ gáº¯ng Ä‘áº£m báº£o)
scale_pos_weight_value = benign_count_train / apt_count_train if apt_count_train > 0 else 1
print(f"Computed scale_pos_weight for XGBoost: {scale_pos_weight_value}") # Pass this value to XGBoost's fit method or set as parameter


# PhÆ°Æ¡ng Ã¡n 3 (Sá»­ dá»¥ng imblearn - cáº§n cÃ i Ä‘áº·t `pip install imbalanced-learn`)
# print("Applying SMOTE on training data...")
# smote = SMOTE(random_state=42)
# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_final, y_train_final)
# print(f"Resampled train shape: {X_train_resampled.shape}")
# print(f"Resampled train label distribution: {Counter(y_train_resampled)}")
# # Use X_train_resampled, y_train_resampled for fitting instead of X_train_final, y_train_final


# --- Huáº¥n luyá»‡n láº¡i MÃ´ hÃ¬nh (Retraining) ---
print("Retraining models on the new data...")

# Retrain Random Forest
# Sá»­ dá»¥ng láº¡i cáº¥u trÃºc/siÃªu tham sá»‘ tá»« mÃ´ hÃ¬nh CIC-IDS2018 hoáº·c Ä‘iá»u chá»‰nh náº¿u cáº§n
# Báº¡n cÃ³ thá»ƒ táº¡o láº¡i Ä‘á»‘i tÆ°á»£ng RF hoáº·c sá»­ dá»¥ng láº¡i rf_model_cicids vÃ  gá»i .fit()
# Äá»ƒ sá»­ dá»¥ng láº¡i rf_model_cicids vÃ  cÃ¡c siÃªu tham sá»‘ cÅ©:
rf_model_retrained = rf_model_cicids
# Hoáº·c táº¡o má»›i vá»›i cÃ¡c siÃªu tham sá»‘ mong muá»‘n, Cáº¦N Xá»¬ LÃ class_weight á»Ÿ Ä‘Ã¢y hoáº·c khi táº¡o Ä‘á»‘i tÆ°á»£ng
# from sklearn.ensemble import RandomForestClassifier
# rf_model_retrained = RandomForestClassifier(n_estimators=..., max_depth=..., random_state=42, class_weight='balanced' # or pass class_weights_dict)

# Fit using the training data (original or resampled)
# If using class_weight='balanced' in constructor, no need for sample_weight here
rf_model_retrained.fit(X_train_final, y_train_final)
# If using class_weight dict:
# rf_model_retrained.fit(X_train_final, y_train_final, sample_weight=np.array([class_weights_dict[label] for label in y_train_final]))

print("Random Forest retraining complete.")

# Retrain XGBoost
# Sá»­ dá»¥ng láº¡i cáº¥u trÃºc/siÃªu tham sá»‘ tá»« mÃ´ hÃ¬nh CIC-IDS2018 hoáº·c Ä‘iá»u chá»‰nh náº¿u cáº§n
# Báº¡n cÃ³ thá»ƒ táº¡o láº¡i Ä‘á»‘i tÆ°á»£ng XGBoost hoáº·c sá»­ dá»¥ng láº¡i xgb_model_cicids vÃ  gá»i .fit()
# Äá»ƒ sá»­ dá»¥ng láº¡i xgb_model_cicids vÃ  cÃ¡c siÃªu tham sá»‘ cÅ©:
xgb_model_retrained = xgb_model_cicids
# Hoáº·c táº¡o má»›i vá»›i cÃ¡c siÃªu tham sá»‘ mong muá»‘n. Cáº¦N Xá»¬ LÃ scale_pos_weight á»Ÿ Ä‘Ã¢y hoáº·c khi táº¡o Ä‘á»‘i tÆ°á»£ng
# import xgboost as xgb
# xgb_model_retrained = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, scale_pos_weight=scale_pos_weight_value, # other params)


# Fit using the training data (original or resampled)
# Pass scale_pos_weight for binary classification with imbalance, unless set in constructor
xgb_model_retrained.fit(X_train_final, y_train_final, scale_pos_weight=scale_pos_weight_value)

print("XGBoost retraining complete.")


# --- ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh má»›i ---
print("\nEvaluating retrained models on the test set...")

models_to_evaluate = {
    "Random Forest (Retrained)": rf_model_retrained,
    "XGBoost (Retrained)": xgb_model_retrained
}

for name, model in models_to_evaluate.items():
    print(f"\n--- Evaluation for {name} ---")

    # Dá»± Ä‘oÃ¡n trÃªn táº­p kiá»ƒm tra
    y_pred = model.predict(X_test_final)
    # Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t (Ä‘á»ƒ tÃ­nh AUC)
    # Äáº£m báº£o mÃ´ hÃ¬nh cÃ³ predict_proba (háº§u háº¿t cÃ¡c classifier cá»§a sklearn vÃ  XGBoost Ä‘á»u cÃ³)
    if hasattr(model, "predict_proba"):
       y_pred_proba = model.predict_proba(X_test_final)[:, 1] # Probability of the positive class (APT=1)
    else:
       y_pred_proba = [0] * len(y_test_final) # Placeholder if no predict_proba


    # In bÃ¡o cÃ¡o phÃ¢n loáº¡i chi tiáº¿t (Precision, Recall, F1-score)
    print("Classification Report:")
    # targets = ['Benign', 'APT']
    # print(classification_report(y_test_final, y_pred, target_names=targets, zero_division=0))
    # Sá»­ dá»¥ng labels vÃ  target_names Ä‘á»ƒ kiá»ƒm soÃ¡t thá»© tá»±
    labels = [0, 1]
    target_names = ['Benign', 'APT']
    print(classification_report(y_test_final, y_pred, labels=labels, target_names=target_names, zero_division=0))


    # In Ma tráº­n nháº§m láº«n
    print("Confusion Matrix:")
    print(confusion_matrix(y_test_final, y_pred, labels=labels)) # Ensure order of labels

    # TÃ­nh vÃ  in cÃ¡c metrics quan trá»ng cho dá»¯ liá»‡u máº¥t cÃ¢n báº±ng
    accuracy = accuracy_score(y_test_final, y_pred)
    precision = precision_score(y_test_final, y_pred, pos_label=1, zero_division=0) # Precision for APT (positive class)
    recall = recall_score(y_test_final, y_pred, pos_label=1, zero_division=0)       # Recall for APT (positive class)
    f1 = f1_score(y_test_final, y_pred, pos_label=1, zero_division=0)         # F1-score for APT (positive class)

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision (APT=1): {precision:.4f}")
    print(f"Recall (APT=1): {recall:.4f}")
    print(f"F1-score (APT=1): {f1:.4f}")

    # TÃ­nh vÃ  in AUC-ROC
    try:
        # Kiá»ƒm tra náº¿u táº­p test cÃ³ cáº£ 2 lá»›p
        if len(np.unique(y_test_final)) > 1:
            roc_auc = roc_auc_score(y_test_final, y_pred_proba)
            print(f"AUC-ROC: {roc_auc:.4f}")
        else:
             print("AUC-ROC cannot be calculated as only one class is present in test labels.")
    except Exception as e:
        print(f"Could not calculate AUC-ROC: {e}")


# --- LÆ°u MÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n láº¡i ---
print("\nSaving retrained models...")
NEW_RF_MODEL_FILE = 'rf_dapt2020_apt_retrained.pkl'
NEW_XGB_MODEL_FILE = 'xgb_dapt2020_apt_retrained.pkl'

try:
    joblib.dump(rf_model_retrained, os.path.join(PATH_TO_MODELS, NEW_RF_MODEL_FILE))
    joblib.dump(xgb_model_retrained, os.path.join(PATH_TO_MODELS, NEW_XGB_MODEL_FILE))
    print(f"Retrained models saved as {NEW_RF_MODEL_FILE} and {NEW_XGB_MODEL_FILE}")
except Exception as e:
    print(f"Error saving models: {e}")

print("\nFine-tuning process (retraining) complete.")