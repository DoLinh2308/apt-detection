import pandas as pd
import numpy as np
import joblib 
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder # Example scalers/encoders
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve, auc

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

# --- C·∫•u h√¨nh ---
PATH_TO_MODELS = 'D:/Do_an_tot_nghiep/apt-detection/ai_model/dataset/working2/'
RF_MODEL_FILE = 'random_forest_model.pkl'
XGB_MODEL_FILE = 'xgboost_model.pkl'
DAPT2020_DATA_FILE = 'D:/Do_an_tot_nghiep/apt-detection/ai_model/dataset/DAPT-2020/merged_cleaned.csv' 


# TODO: C·∫§U H√åNH C·ªòT NH√ÉN TRONG DAPT2020
# C·∫ßn bi·∫øt t√™n c·ªôt ch·ª©a nh√£n t·∫•n c√¥ng trong DAPT2020.
LABEL_COLUMN = 'label' # Example: The column in DAPT2020 that contains attack types/APT label

# TODO: ƒê·ªäNH NGHƒ®A NH√ÉN APT V√Ä BENIGN
# B·∫°n c·∫ßn √°nh x·∫° c√°c gi√° tr·ªã trong LABEL_COLUMN c·ªßa DAPT2020 sang 0 (Benign) v√† 1 (APT)
# V√ç D·ª§ (THAY TH·∫æ B·∫∞NG C√ÅC NH√ÉN TH·∫¨T C·ª¶A B·∫†N):
print("\nüîç Checking for Extra Spaces in Attack Labels...")

APT_LABELS_IN_DAPT2020 = ['Lateral Movement', 'Reconnaissance', 'Establish Foothold', 'Data Exfiltration']
BENIGN_LABELS_IN_DAPT2020 = ['Benign', 'BENIGN']

FEATURE_COLUMNS = [
    'Dst Port',
    'Flow Duration',
    'Tot Fwd Pkts',
    'Tot Bwd Pkts',
    'TotLen Fwd Pkts',
    'TotLen Bwd Pkts',
    'Fwd Pkt Len Max',
    'Fwd Pkt Len Min',
    'Fwd Pkt Len Mean',
    'Fwd Pkt Len Std',
    'Bwd Pkt Len Max',
    'Bwd Pkt Len Min',
    'Bwd Pkt Len Mean',
    'Bwd Pkt Len Std',
    'Flow Byts/s',
    'Flow Pkts/s',
    'Flow IAT Mean',
    'Flow IAT Std',
    'Flow IAT Max',
    'Flow IAT Min',
    'Fwd IAT Tot',
    'Fwd IAT Mean',
    'Fwd IAT Std',
    'Fwd IAT Max',
    'Fwd IAT Min',
    'Bwd IAT Tot',
    'Bwd IAT Mean',
    'Bwd IAT Std',
    'Bwd IAT Max',
    'Bwd IAT Min',
    'Fwd PSH Flags',
    'Fwd URG Flags',
    'Fwd Header Len',
    'Bwd Header Len',
    'Fwd Pkts/s',
    'Bwd Pkts/s',
    'Pkt Len Min',
    'Pkt Len Max',
    'Pkt Len Mean',
    'Pkt Len Std',
    'Pkt Len Var',
    'FIN Flag Cnt',
    'SYN Flag Cnt',
    'RST Flag Cnt',
    'PSH Flag Cnt',
    'ACK Flag Cnt',
    'URG Flag Cnt',
    'CWE Flag Count',
    'ECE Flag Cnt',
    'Down/Up Ratio',
    'Pkt Size Avg',
    'Fwd Seg Size Avg',
    'Bwd Seg Size Avg',
    'Subflow Fwd Pkts',
    'Subflow Fwd Byts',
    'Subflow Bwd Pkts',
    'Subflow Bwd Byts',
    'Init Fwd Win Byts',
    'Init Bwd Win Byts',
    'Fwd Act Data Pkts',
    'Fwd Seg Size Min',
    'Active Mean',
    'Active Std',
    'Active Max',
    'Active Min',
    'Idle Mean',
    'Idle Std',
    'Idle Max',
    'Idle Min',
]

# TODO: X√ÅC ƒê·ªäNH C√ÅC C·ªòT C·∫¶N TI·ªÄN X·ª¨ L√ù TRONG DAPT2020 D·ª∞A TR√äN C√ÅCH B·∫†N ƒê√É L√ÄM V·ªöI CIC-IDS2018
# ƒê·∫∑c bi·ªát l∆∞u √Ω c·ªôt Protocol. N·∫øu DAPT2020 c√≥ c·ªôt Protocol g·ªëc, b·∫°n c·∫ßn m√£ h√≥a n√≥.
# N·∫øu DAPT2020 ƒë√£ c√≥ s·∫µn c√°c c·ªôt Protocol_0, Protocol_6, Protocol_17, th√¨ coi ch√∫ng l√† s·ªë.

# V√ç D·ª§ (C·∫ßn ƒëi·ªÅu ch·ªânh d·ª±a tr√™n ti·ªÅn x·ª≠ l√Ω th·ª±c t·∫ø c·ªßa b·∫°n):
# Gi·∫£ ƒë·ªãnh h·∫ßu h·∫øt c√°c c·ªôt l√† s·ªë v√† ƒë√£ ƒë∆∞·ª£c scale, c√≤n Protocol_X l√† k·∫øt qu·∫£ OHE v√† kh√¥ng scale th√™m.
NUMERICAL_FEATURES_TO_SCALE = [
    'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts',
    'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min',
    'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max',
    'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std',
    'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std',
    'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean',
    'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot',
    'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',
    'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',
    'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std',
    'Pkt Len Var', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg',
    'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts',
    'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts',
    'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min',
    'Active Mean', 'Active Std', 'Active Max', 'Active Min',
    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'
]

# C√°c c·ªôt c·ªù v√† Protocol_X th∆∞·ªùng l√† 0/1 ho·∫∑c ƒë·∫øm, c√≥ th·ªÉ kh√¥ng c·∫ßn scale
# N·∫øu b·∫°n ƒë√£ scale ch√∫ng, th√™m v√†o danh s√°ch tr√™n. N·∫øu kh√¥ng, b·ªè qua.
FEATURES_NOT_SCALED_BUT_NUMERICAL = [
    'Dst Port', # C·ªïng ƒë√≠ch - c√≥ th·ªÉ coi l√† s·ªë ho·∫∑c ph√¢n lo·∫°i t√πy c√°ch d√πng
    'Fwd PSH Flags', 'Fwd URG Flags', 'FIN Flag Cnt', 'SYN Flag Cnt',
    'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',
    'CWE Flag Count', 'ECE Flag Cnt',
    'Protocol_0', 'Protocol_6', 'Protocol_17' # K·∫øt qu·∫£ c·ªßa OHE Protocol
]

# C√°c c·ªôt ph√¢n lo·∫°i g·ªëc (n·∫øu DAPT2020 c√≤n c·ªôt Protocol g·ªëc, c·∫ßn m√£ h√≥a)
# N·∫øu DAPT2020 ƒë√£ c√≥ Protocol_X, th√¨ kh√¥ng c·∫ßn c·ªôt ph√¢n lo·∫°i g·ªëc n·ªØa.
# CATEGORICAL_FEATURES_TO_ENCODE = ['Protocol'] # Example if DAPT2020 has original Protocol column

# --- Load M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán tr√™n CIC-IDS2018 ---
print("Loading existing models...")
try:
    rf_model_cicids = joblib.load(os.path.join(PATH_TO_MODELS, RF_MODEL_FILE))
    xgb_model_cicids = joblib.load(os.path.join(PATH_TO_MODELS, XGB_MODEL_FILE))
    print("Models loaded successfully.")
    # Optional: print some info about loaded models
    # print("RF Model Params:", rf_model_cicids.get_params())
    # print("XGB Model Params:", xgb_model_cicids.get_params())
except FileNotFoundError as e:
    print(f"Error loading model files: {e}")
    print("Please check the file paths and names.")
    exit() # Exit if models can't be loaded

# --- Load D·ªØ li·ªáu DAPT2020 ---
print(f"Loading DAPT2020 data from {DAPT2020_DATA_FILE}...")
try:
    # TODO: Ki·ªÉm tra encoding c·ªßa file CSV DAPT2020 n·∫øu g·∫∑p l·ªói ƒë·ªçc file
    df_dapt2020 = pd.read_csv(DAPT2020_DATA_FILE)
    print(f"DAPT2020 data loaded. Shape: {df_dapt2020.shape}")
    # Optional: Display first few rows and info
    # print(df_dapt2020.head())
    # print(df_dapt2020.info())
except FileNotFoundError as e:
    print(f"Error loading DAPT2020 data file: {e}")
    print("Please check the file path.")
    exit()
except Exception as e:
     print(f"Error reading DAPT2020 data file: {e}")
     exit()


# --- Ti·ªÅn x·ª≠ l√Ω DAPT2020 v√† Chu·∫©n b·ªã Nh√£n APT ---

# 1. X·ª≠ l√Ω c√°c gi√° tr·ªã kh√¥ng x√°c ƒë·ªãnh/thi·∫øu trong DAPT2020
# TODO: √ÅP D·ª§NG C√ÅCH X·ª¨ L√ù GI√Å TR·ªä THI·∫æU NH·∫§T QU√ÅN V·ªöI CIC-IDS2018
# V√ç D·ª§: ƒëi·ªÅn b·∫±ng 0, gi√° tr·ªã trung b√¨nh, gi√° tr·ªã mode, ho·∫∑c lo·∫°i b·ªè h√†ng
print("Preprocessing DAPT2020 data...")
# Example: fill NaNs with 0 (adjust if needed)
df_dapt2020.fillna(0, inplace=True)

# TODO: √Åp d·ª•ng x·ª≠ l√Ω c√°c gi√° tr·ªã v√¥ h·∫°n (inf) n·∫øu c√≥
df_dapt2020.replace([np.inf, -np.inf], np.nan, inplace=True)
df_dapt2020.fillna(0, inplace=True) # Fill NaNs created by replacing inf

# 2. √Ånh x·∫° nh√£n g·ªëc sang nh√£n Benign (0) v√† APT (1)
# TODO: √ÅP D·ª§NG LOGIC G√ÅN NH√ÉN APT/BENIGN C·ª¶A B·∫†N
df_dapt2020['APT_Label'] = 0 # Default to Benign (0)
df_dapt2020.loc[df_dapt2020[LABEL_COLUMN].isin(APT_LABELS_IN_DAPT2020), 'APT_Label'] = 1 # Mark APT (1)

# Optional: Remove rows that are neither Benign nor APT if necessary
# print(f"Original number of rows: {df_dapt2020.shape[0]}")
# df_dapt2020 = df_dapt2020[df_dapt2020[LABEL_COLUMN].isin(APT_LABELS_IN_DAPT2020 + BENIGN_LABELS_IN_DAPT2020)].copy()
# print(f"Rows after filtering labels: {df_dapt2020.shape[0]}")


# T√°ch ƒë·∫∑c tr∆∞ng (X) v√† nh√£n (y) TR∆Ø·ªöC KHI √°p d·ª•ng c√°c transformer
try:
    # ƒê·∫£m b·∫£o ch·ªâ ch·ªçn c√°c c·ªôt ƒë·∫∑c tr∆∞ng ƒë√£ ƒë·ªãnh nghƒ©a
    X_dapt = df_dapt2020[FEATURE_COLUMNS].copy()
    y_dapt = df_dapt2020['APT_Label'].copy()
except KeyError as e:
    print(f"Error: Missing a defined feature or label column in DAPT2020 data: {e}")
    print("Please check if all columns in FEATURE_COLUMNS and LABEL_COLUMN exist in your DAPT2020 data.")
    exit()

print(f"Features shape before transformation: {X_dapt.shape}")
print(f"Labels shape: {y_dapt.shape}")
print(f"Label distribution in DAPT2020: {Counter(y_dapt)}")

# 3. √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω cho c√°c ƒë·∫∑c tr∆∞ng (Scaling, Encoding, etc.)
# TODO: T·∫†O V√Ä FIT TRANSFORMER HO·∫∂C √ÅP D·ª§NG SCALER ƒê√É FIT T·ª™ CIC-IDS2018
# C√°ch l√Ω t∆∞·ªüng l√† s·ª≠ d·ª•ng l·∫°i scaler/encoder ƒë√£ fit tr√™n CIC-IDS2018 ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n.
# N·∫øu kh√¥ng c√≥ scaler/encoder ƒë√£ l∆∞u, b·∫°n c√≥ th·ªÉ fit m·ªõi tr√™n DAPT2020 ho·∫∑c t·∫≠p d·ªØ li·ªáu k·∫øt h·ª£p.
print("Applying feature transformations...")

# --- Example: Create a preprocessor pipeline (adapt based on your actual preprocessing) ---
# B·∫°n c·∫ßn ƒëi·ªÅu ch·ªânh n√†y cho ph√π h·ª£p v·ªõi c√°ch b·∫°n ƒë√£ ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu CIC-IDS2018.
# N·∫øu b·∫°n ƒë√£ l∆∞u ƒë·ªëi t∆∞·ª£ng preprocessor t·ª´ l·∫ßn hu·∫•n luy·ªán tr∆∞·ªõc, h√£y load n√≥ v√† ch·ªâ g·ªçi transform.
# V√≠ d·ª•: preprocessor = joblib.load('cicids_preprocessor.pkl')
# X_dapt_processed = preprocessor.transform(X_dapt)
# -----------------------------------------------------------------------------------------

# N·∫øu b·∫°n c·∫ßn fit l·∫°i preprocessor tr√™n d·ªØ li·ªáu m·ªõi (√≠t l√Ω t∆∞·ªüng h∆°n nh∆∞ng ƒë√¥i khi c·∫ßn):
# V√ç D·ª§ N√ÄY GI·∫¢ ƒê·ªäNH B·∫†N SCALE C√ÅC C·ªòT TRONG NUMERICAL_FEATURES_TO_SCALE
# V√Ä ƒê·ªÇ NGUY√äN C√ÅC C·ªòT KH√ÅC BAO G·ªíM C√ÅC C·ªòT C·ªú V√Ä PROTOCOL_X
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), NUMERICAL_FEATURES_TO_SCALE)
        # TODO: Add other transformers if you had categorical features in CIC-IDS2018 besides Protocol
        # ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_TO_ENCODE)
    ],
    # Keep other columns (like flag counts, Dst Port, Protocol_X) as they are
    remainder='passthrough'
)

# Fit and transform the DAPT2020 data
X_dapt_processed = preprocessor.fit_transform(X_dapt)
print(f"Processed data shape: {X_dapt_processed.shape}")
print("DAPT2020 preprocessing complete.")

# --- T√πy ch·ªçn: K·∫øt h·ª£p d·ªØ li·ªáu (N·∫øu ch·ªçn ph∆∞∆°ng √°n n√†y) ---
# TODO: N·∫æU B·∫†N MU·ªêN K·∫æT H·ª¢P D·ªÆ LI·ªÜU CIC-IDS2018 V√Ä DAPT2020
# B·∫°n c·∫ßn load v√† ti·ªÅn x·ª≠ l√Ω m·ªôt ph·∫ßn d·ªØ li·ªáu CIC-IDS2018 (benign v√†/ho·∫∑c c√°c lo·∫°i t·∫•n c√¥ng li√™n quan)
# s·ª≠ d·ª•ng C√ôNG ƒë·ªëi t∆∞·ª£ng preprocessor ƒê√É FIT (ho·∫∑c fit m·ªõi tr√™n to√†n b·ªô d·ªØ li·ªáu k·∫øt h·ª£p ban ƒë·∫ßu).
# Sau ƒë√≥ n·ªëi X v√† y t·ª´ hai ngu·ªìn l·∫°i.
# V√≠ d·ª• (Conceptual - needs actual CIC-IDS2018 loading and preprocessing):
# X_cicids_subset, y_cicids_subset = load_and_preprocess_cicids_subset(...) # Ensure this also uses the *same* preprocessor
# X_combined = np.vstack((X_dapt_processed, X_cicids_subset))
# y_combined = np.concatenate((y_dapt, y_cicids_subset))
# print(f"Combined data shape: {X_combined.shape}")
# X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined) # Use stratify for imbalance
# print(f"Combined Train/Test shapes: {X_train_final.shape} / {X_test_final.shape}")


# --- Chia t·∫≠p d·ªØ li·ªáu (N·∫øu ch·ªâ d√πng DAPT2020) ---
# N·∫øu ch·ªâ s·ª≠ d·ª•ng DAPT2020 ƒë·ªÉ hu·∫•n luy·ªán l·∫°i
print("Splitting DAPT2020 data into train and test sets...")
X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(
    X_dapt_processed, y_dapt,
    test_size=0.2,      # 20% for testing (adjust as needed)
    random_state=42,    # for reproducibility
    stratify=y_dapt     # Stratify to maintain class distribution (crucial for imbalance)
)
print(f"Train/Test shapes: {X_train_final.shape} / {X_test_final.shape}")
print(f"Train Label Distribution: {Counter(y_train_final)}")
print(f"Test Label Distribution: {Counter(y_test_final)}")


# --- X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng l·ªõp (tr√™n t·∫≠p hu·∫•n luy·ªán) ---
# T·∫•n c√¥ng APT (nh√£n 1) c√≥ th·ªÉ r·∫•t √≠t trong t·∫≠p hu·∫•n luy·ªán.
# C√≥ nhi·ªÅu c√°ch x·ª≠ l√Ω:
# 1. D√πng class_weight trong m√¥ h√¨nh (ƒë·ªëi v·ªõi RF)
# 2. D√πng scale_pos_weight trong m√¥ h√¨nh (ƒë·ªëi v·ªõi XGBoost binary classification)
# 3. √Åp d·ª•ng k·ªπ thu·∫≠t l·∫•y m·∫´u (sampling) nh∆∞ SMOTE, Undersampling (s·ª≠ d·ª•ng th∆∞ vi·ªán imblearn)
print("Handling class imbalance...")

# Ph∆∞∆°ng √°n 1/2: S·ª≠ d·ª•ng tr·ªçng s·ªë l·ªõp t√≠ch h·ª£p trong m√¥ h√¨nh
# T√≠nh to√°n tr·ªçng s·ªë l·ªõp cho RF (s·ª≠ d·ª•ng 'balanced' ho·∫∑c dict t√πy ch·ªçn)
# from sklearn.utils.class_weight import compute_class_weight
# classes = np.unique(y_train_final)
# weights = compute_class_weight('balanced', classes=classes, y=y_train_final)
# class_weights_dict = dict(zip(classes, weights))
# print("Computed class weights for RF:", class_weights_dict) # Pass this dict to RF's fit method or use 'balanced'

# T√≠nh to√°n scale_pos_weight cho XGBoost (ph√¢n lo·∫°i nh·ªã ph√¢n 0/1)
apt_count_train = sum(y_train_final == 1)
benign_count_train = sum(y_train_final == 0)
# Tr√°nh chia cho 0 n·∫øu kh√¥ng c√≥ m·∫´u APT trong t·∫≠p hu·∫•n luy·ªán (d√π stratify c·ªë g·∫Øng ƒë·∫£m b·∫£o)
scale_pos_weight_value = benign_count_train / apt_count_train if apt_count_train > 0 else 1
print(f"Computed scale_pos_weight for XGBoost: {scale_pos_weight_value}") # Pass this value to XGBoost's fit method or set as parameter


# Ph∆∞∆°ng √°n 3 (S·ª≠ d·ª•ng imblearn - c·∫ßn c√†i ƒë·∫∑t `pip install imbalanced-learn`)
# print("Applying SMOTE on training data...")
# smote = SMOTE(random_state=42)
# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_final, y_train_final)
# print(f"Resampled train shape: {X_train_resampled.shape}")
# print(f"Resampled train label distribution: {Counter(y_train_resampled)}")
# # Use X_train_resampled, y_train_resampled for fitting instead of X_train_final, y_train_final


# --- Hu·∫•n luy·ªán l·∫°i M√¥ h√¨nh (Retraining) ---
print("Retraining models on the new data...")

# Retrain Random Forest
# S·ª≠ d·ª•ng l·∫°i c·∫•u tr√∫c/si√™u tham s·ªë t·ª´ m√¥ h√¨nh CIC-IDS2018 ho·∫∑c ƒëi·ªÅu ch·ªânh n·∫øu c·∫ßn
# B·∫°n c√≥ th·ªÉ t·∫°o l·∫°i ƒë·ªëi t∆∞·ª£ng RF ho·∫∑c s·ª≠ d·ª•ng l·∫°i rf_model_cicids v√† g·ªçi .fit()
# ƒê·ªÉ s·ª≠ d·ª•ng l·∫°i rf_model_cicids v√† c√°c si√™u tham s·ªë c≈©:
rf_model_retrained = rf_model_cicids
# Ho·∫∑c t·∫°o m·ªõi v·ªõi c√°c si√™u tham s·ªë mong mu·ªën, C·∫¶N X·ª¨ L√ù class_weight ·ªü ƒë√¢y ho·∫∑c khi t·∫°o ƒë·ªëi t∆∞·ª£ng
# from sklearn.ensemble import RandomForestClassifier
# rf_model_retrained = RandomForestClassifier(n_estimators=..., max_depth=..., random_state=42, class_weight='balanced' # or pass class_weights_dict)

# Fit using the training data (original or resampled)
# If using class_weight='balanced' in constructor, no need for sample_weight here
rf_model_retrained.fit(X_train_final, y_train_final)
# If using class_weight dict:
# rf_model_retrained.fit(X_train_final, y_train_final, sample_weight=np.array([class_weights_dict[label] for label in y_train_final]))

print("Random Forest retraining complete.")

# Retrain XGBoost
# S·ª≠ d·ª•ng l·∫°i c·∫•u tr√∫c/si√™u tham s·ªë t·ª´ m√¥ h√¨nh CIC-IDS2018 ho·∫∑c ƒëi·ªÅu ch·ªânh n·∫øu c·∫ßn
# B·∫°n c√≥ th·ªÉ t·∫°o l·∫°i ƒë·ªëi t∆∞·ª£ng XGBoost ho·∫∑c s·ª≠ d·ª•ng l·∫°i xgb_model_cicids v√† g·ªçi .fit()
# ƒê·ªÉ s·ª≠ d·ª•ng l·∫°i xgb_model_cicids v√† c√°c si√™u tham s·ªë c≈©:
xgb_model_retrained = xgb_model_cicids
# Ho·∫∑c t·∫°o m·ªõi v·ªõi c√°c si√™u tham s·ªë mong mu·ªën. C·∫¶N X·ª¨ L√ù scale_pos_weight ·ªü ƒë√¢y ho·∫∑c khi t·∫°o ƒë·ªëi t∆∞·ª£ng
# import xgboost as xgb
# xgb_model_retrained = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, scale_pos_weight=scale_pos_weight_value, # other params)


# Fit using the training data (original or resampled)
# Pass scale_pos_weight for binary classification with imbalance, unless set in constructor
xgb_model_retrained.fit(X_train_final, y_train_final, scale_pos_weight=scale_pos_weight_value)

print("XGBoost retraining complete.")


# --- ƒê√°nh gi√° M√¥ h√¨nh m·ªõi ---
print("\nEvaluating retrained models on the test set...")

models_to_evaluate = {
    "Random Forest (Retrained)": rf_model_retrained,
    "XGBoost (Retrained)": xgb_model_retrained
}

for name, model in models_to_evaluate.items():
    print(f"\n--- Evaluation for {name} ---")

    # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra
    y_pred = model.predict(X_test_final)
    # D·ª± ƒëo√°n x√°c su·∫•t (ƒë·ªÉ t√≠nh AUC)
    # ƒê·∫£m b·∫£o m√¥ h√¨nh c√≥ predict_proba (h·∫ßu h·∫øt c√°c classifier c·ªßa sklearn v√† XGBoost ƒë·ªÅu c√≥)
    if hasattr(model, "predict_proba"):
       y_pred_proba = model.predict_proba(X_test_final)[:, 1] # Probability of the positive class (APT=1)
    else:
       y_pred_proba = [0] * len(y_test_final) # Placeholder if no predict_proba


    # In b√°o c√°o ph√¢n lo·∫°i chi ti·∫øt (Precision, Recall, F1-score)
    print("Classification Report:")
    # targets = ['Benign', 'APT']
    # print(classification_report(y_test_final, y_pred, target_names=targets, zero_division=0))
    # S·ª≠ d·ª•ng labels v√† target_names ƒë·ªÉ ki·ªÉm so√°t th·ª© t·ª±
    labels = [0, 1]
    target_names = ['Benign', 'APT']
    print(classification_report(y_test_final, y_pred, labels=labels, target_names=target_names, zero_division=0))


    # In Ma tr·∫≠n nh·∫ßm l·∫´n
    print("Confusion Matrix:")
    print(confusion_matrix(y_test_final, y_pred, labels=labels)) # Ensure order of labels

    # T√≠nh v√† in c√°c metrics quan tr·ªçng cho d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng
    accuracy = accuracy_score(y_test_final, y_pred)
    precision = precision_score(y_test_final, y_pred, pos_label=1, zero_division=0) # Precision for APT (positive class)
    recall = recall_score(y_test_final, y_pred, pos_label=1, zero_division=0)       # Recall for APT (positive class)
    f1 = f1_score(y_test_final, y_pred, pos_label=1, zero_division=0)         # F1-score for APT (positive class)

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision (APT=1): {precision:.4f}")
    print(f"Recall (APT=1): {recall:.4f}")
    print(f"F1-score (APT=1): {f1:.4f}")

    # T√≠nh v√† in AUC-ROC
    try:
        # Ki·ªÉm tra n·∫øu t·∫≠p test c√≥ c·∫£ 2 l·ªõp
        if len(np.unique(y_test_final)) > 1:
            roc_auc = roc_auc_score(y_test_final, y_pred_proba)
            print(f"AUC-ROC: {roc_auc:.4f}")
        else:
             print("AUC-ROC cannot be calculated as only one class is present in test labels.")
    except Exception as e:
        print(f"Could not calculate AUC-ROC: {e}")


# --- L∆∞u M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán l·∫°i ---
print("\nSaving retrained models...")
NEW_RF_MODEL_FILE = 'rf_dapt2020_apt_retrained.pkl'
NEW_XGB_MODEL_FILE = 'xgb_dapt2020_apt_retrained.pkl'

try:
    joblib.dump(rf_model_retrained, os.path.join(PATH_TO_MODELS, NEW_RF_MODEL_FILE))
    joblib.dump(xgb_model_retrained, os.path.join(PATH_TO_MODELS, NEW_XGB_MODEL_FILE))
    print(f"Retrained models saved as {NEW_RF_MODEL_FILE} and {NEW_XGB_MODEL_FILE}")
except Exception as e:
    print(f"Error saving models: {e}")

print("\nFine-tuning process (retraining) complete.")