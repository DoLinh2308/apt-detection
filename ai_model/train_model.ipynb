{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f97947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/CSE-CIC-IDS2018\\02-14-2018.csv\n",
      "dataset/CSE-CIC-IDS2018\\02-15-2018.csv\n",
      "dataset/CSE-CIC-IDS2018\\02-16-2018.csv\n",
      "dataset/CSE-CIC-IDS2018\\02-20-2018.csv\n",
      "dataset/CSE-CIC-IDS2018\\02-21-2018.csv\n",
      "dataset/CSE-CIC-IDS2018\\02-22-2018.csv\n",
      "dataset/CSE-CIC-IDS2018\\02-23-2018.csv\n",
      "dataset/CSE-CIC-IDS2018\\02-28-2018.csv\n",
      "dataset/CSE-CIC-IDS2018\\03-01-2018.csv\n",
      "dataset/CSE-CIC-IDS2018\\03-02-2018.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('dataset/CSE-CIC-IDS2018'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01dcba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is support GPU: True\n",
      "üöÄ GPU Available: True\n",
      "üñ•Ô∏è GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1951"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch  \n",
    "\n",
    "\n",
    "# ‚úÖ Check GPU Support Availability\n",
    "print(f\"Is support GPU: {torch.cuda.is_available()}\")\n",
    "# ‚úÖ Check GPU Availability\n",
    "print(f\"üöÄ GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üñ•Ô∏è GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU Detected. Running on CPU.\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b13dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Current RAM Usage: 1.97 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import IPython\n",
    "\n",
    "# ‚úÖ Function to Monitor RAM Usage\n",
    "def check_ram():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_usage = process.memory_info().rss / 1024 ** 3  # Convert to GB\n",
    "    print(f\"üîç Current RAM Usage: {ram_usage:.2f} GB\")\n",
    "\n",
    "# ‚úÖ Function to Clean RAM\n",
    "def clean_ram():\n",
    "    gc.collect()  # ‚úÖ Free Unused Memory\n",
    "    IPython.display.clear_output(wait=True)  # ‚úÖ Clear Output (Optional)\n",
    "    check_ram()  # ‚úÖ Show Updated RAM Usage\n",
    "\n",
    "# Example Usage:\n",
    "# Run this after every large operation (like merging datasets, model training)\n",
    "check_ram()  # Before cleaning\n",
    "clean_ram()  # After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f3728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu t·∫£i d·ªØ li·ªáu t·ª´: dataset/CSE-CIC-IDS2018 (theo chunk)\n",
      "   K√≠ch th∆∞·ªõc chunk: 100000\n",
      "   S·∫Ω b·ªè qua file: 02-20-2018.csv\n",
      "   üìÇ ƒêang x·ª≠ l√Ω file: dataset/CSE-CIC-IDS2018\\02-14-2018.csv\n",
      "      ‚úÖ ƒê√£ x·ª≠ l√Ω 11 chunk(s) t·ª´ file 02-14-2018.csv.\n",
      "   üìÇ ƒêang x·ª≠ l√Ω file: dataset/CSE-CIC-IDS2018\\02-15-2018.csv\n",
      "      ‚úÖ ƒê√£ x·ª≠ l√Ω 11 chunk(s) t·ª´ file 02-15-2018.csv.\n",
      "   üìÇ ƒêang x·ª≠ l√Ω file: dataset/CSE-CIC-IDS2018\\02-16-2018.csv\n",
      "      ‚úÖ ƒê√£ x·ª≠ l√Ω 11 chunk(s) t·ª´ file 02-16-2018.csv.\n",
      "   ‚û°Ô∏è B·ªè qua: dataset/CSE-CIC-IDS2018\\02-20-2018.csv\n",
      "   üìÇ ƒêang x·ª≠ l√Ω file: dataset/CSE-CIC-IDS2018\\02-21-2018.csv\n",
      "      ‚úÖ ƒê√£ x·ª≠ l√Ω 11 chunk(s) t·ª´ file 02-21-2018.csv.\n",
      "   üìÇ ƒêang x·ª≠ l√Ω file: dataset/CSE-CIC-IDS2018\\02-22-2018.csv\n",
      "      ‚úÖ ƒê√£ x·ª≠ l√Ω 11 chunk(s) t·ª´ file 02-22-2018.csv.\n",
      "   üìÇ ƒêang x·ª≠ l√Ω file: dataset/CSE-CIC-IDS2018\\02-23-2018.csv\n",
      "      ‚úÖ ƒê√£ x·ª≠ l√Ω 11 chunk(s) t·ª´ file 02-23-2018.csv.\n",
      "   üìÇ ƒêang x·ª≠ l√Ω file: dataset/CSE-CIC-IDS2018\\02-28-2018.csv\n",
      "      ‚úÖ ƒê√£ x·ª≠ l√Ω 7 chunk(s) t·ª´ file 02-28-2018.csv.\n",
      "   üìÇ ƒêang x·ª≠ l√Ω file: dataset/CSE-CIC-IDS2018\\03-01-2018.csv\n",
      "      ‚úÖ ƒê√£ x·ª≠ l√Ω 4 chunk(s) t·ª´ file 03-01-2018.csv.\n",
      "   üìÇ ƒêang x·ª≠ l√Ω file: dataset/CSE-CIC-IDS2018\\03-02-2018.csv\n",
      "      ‚úÖ ƒê√£ x·ª≠ l√Ω 11 chunk(s) t·ª´ file 03-02-2018.csv.\n",
      "\n",
      "‚úÖ Ho√†n th√†nh vi·ªác ƒë·ªçc t·∫•t c·∫£ c√°c file theo chunk.\n",
      "üöÄ B·∫Øt ƒë·∫ßu g·ªôp c√°c chunk th√†nh DataFrame cu·ªëi c√πng...\n",
      "‚úÖ G·ªôp chunk th√†nh c√¥ng!\n",
      "üîπ K√≠ch th∆∞·ªõc DataFrame cu·ªëi c√πng: (8284254, 80)\n",
      "‚úÖ DataFrame 'df_main' ƒë√£ s·∫µn s√†ng.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "chunk_list = [] \n",
    "chunksize = 100000 \n",
    "# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu CSV\n",
    "data_directory = 'dataset/CSE-CIC-IDS2018' \n",
    "# ƒê·ªãnh nghƒ©a t√™n file c·∫ßn b·ªè qua\n",
    "file_to_skip = '02-20-2018.csv'\n",
    "\n",
    "print(f\"B·∫Øt ƒë·∫ßu t·∫£i d·ªØ li·ªáu t·ª´: {data_directory} (theo chunk)\")\n",
    "print(f\"K√≠ch th∆∞·ªõc chunk: {chunksize}\")\n",
    "print(f\"S·∫Ω b·ªè qua file: {file_to_skip}\")\n",
    "\n",
    "# Duy·ªát qua c√¢y th∆∞ m·ª•c\n",
    "for dirname, _, filenames in os.walk(data_directory):\n",
    "    for filename in filenames:\n",
    "        # B·ªè qua file ch·ªâ ƒë·ªãnh\n",
    "        if filename == file_to_skip:\n",
    "            full_path_skipped = os.path.join(dirname, filename)\n",
    "            print(f\"B·ªè qua: {full_path_skipped}\")\n",
    "            continue \n",
    "\n",
    "        # Ch·ªâ x·ª≠ l√Ω c√°c file CSV\n",
    "        if filename.lower().endswith('.csv'):\n",
    "            file_path = os.path.join(dirname, filename)\n",
    "            print(f\"ƒêang x·ª≠ l√Ω file: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                # pd.read_csv v·ªõi chunksize tr·∫£ v·ªÅ m·ªôt iterator\n",
    "                chunk_iterator = pd.read_csv(\n",
    "                    file_path, \n",
    "                    chunksize=chunksize, \n",
    "                    low_memory=False \n",
    "                )\n",
    "                \n",
    "                file_chunk_count = 0 # ƒê·∫øm s·ªë chunk ƒë√£ ƒë·ªçc t·ª´ file hi·ªán t·∫°i\n",
    "                # L·∫∑p qua t·ª´ng chunk trong file\n",
    "                for chunk in chunk_iterator:\n",
    "                    # Ki·ªÉm tra chunk r·ªóng\n",
    "                    if chunk.empty:\n",
    "                        print(f\"-> Chunk r·ªóng trong file {filename}. B·ªè qua.\")\n",
    "                        continue\n",
    "                    chunk_list.append(chunk)\n",
    "                    file_chunk_count += 1\n",
    "                    del chunk\n",
    "                    gc.collect()\n",
    "                    \n",
    "                print(f\"ƒê√£ x·ª≠ l√Ω {file_chunk_count} chunk(s) t·ª´ file {filename}.\")\n",
    "\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"C·∫£nh b√°o: File {filename} tr·ªëng ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu. B·ªè qua.\")\n",
    "            # B·∫Øt c√°c l·ªói li√™n quan ƒë·∫øn b·ªô nh·ªõ c·ª• th·ªÉ h∆°n n·∫øu mu·ªën\n",
    "            except MemoryError:\n",
    "                 print(f\"L·ªói MemoryError khi ƒë·ªçc file {filename}. C√≥ th·ªÉ chunksize v·∫´n qu√° l·ªõn ho·∫∑c RAM qu√° th·∫•p.\")\n",
    "            except Exception as e:\n",
    "                print(f\"L·ªói khi ƒë·ªçc ho·∫∑c x·ª≠ l√Ω file {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"-> B·ªè qua file kh√¥ng ph·∫£i CSV: {filename}\")\n",
    "\n",
    "print(\"\\nHo√†n th√†nh vi·ªác ƒë·ªçc t·∫•t c·∫£ c√°c file theo chunk.\")\n",
    "\n",
    "# B∆∞·ªõc n√†y s·∫Ω di·ªÖn ra M·ªòT L·∫¶N DUY NH·∫§T sau khi ƒë·ªçc xong t·∫•t c·∫£, hi·ªáu qu·∫£ h∆°n v·ªÅ b·ªô nh·ªõ\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu g·ªôp c√°c chunk th√†nh DataFrame cu·ªëi c√πng...\")\n",
    "df_main = pd.DataFrame(data=None) \n",
    "\n",
    "if chunk_list: \n",
    "    try:\n",
    "        df_main = pd.concat(chunk_list, ignore_index=True)\n",
    "        print(\"‚úÖ G·ªôp chunk th√†nh c√¥ng!\")\n",
    "        del chunk_list\n",
    "        gc.collect()\n",
    "    except MemoryError:\n",
    "        print(f\"L·ªói MemoryError khi g·ªôp c√°c chunk! D·ªØ li·ªáu t·ªïng h·ª£p v·∫´n qu√° l·ªõn cho RAM.\")\n",
    "        print(f\"·ªë l∆∞·ª£ng chunk ƒë√£ ƒë·ªçc: {len(chunk_list)}\")\n",
    "    except Exception as e:\n",
    "         print(f\"‚ùå L·ªói khi g·ªôp c√°c chunk: {e}\")\n",
    "else:\n",
    "    print(\"Kh√¥ng c√≥ chunk n√†o ƒë∆∞·ª£c ƒë·ªçc th√†nh c√¥ng. DataFrame 'df_main' s·∫Ω r·ªóng.\")\n",
    "\n",
    "if df_main.empty:\n",
    "    print(\"C·∫¢NH B√ÅO: DataFrame 'df_main' cu·ªëi c√πng v·∫´n r·ªóng ho·∫∑c kh√¥ng th·ªÉ g·ªôp.\")\n",
    "    print(\"Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n, n·ªôi dung file, l·ªói b·ªô nh·ªõ v√† th·ª≠ ƒëi·ªÅu ch·ªânh chunksize ho·∫∑c dtypes.\")\n",
    "else:\n",
    "    print(f\"K√≠ch th∆∞·ªõc DataFrame cu·ªëi c√πng: {df_main.shape}\")\n",
    "    print(\"DataFrame 'df_main' ƒë√£ s·∫µn s√†ng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca727fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ **Final Data Verification Started...** üöÄ\n",
      "\n",
      "üîç Checking for Duplicate Rows...\n",
      "‚úÖ Total Duplicate Rows: 410731\n",
      "\n",
      "üîç Checking for Repeated Headers...\n",
      "‚úÖ No apparent repeated headers found in row 1.\n",
      "\n",
      "üîç Checking for Missing Values (NaN)...\n",
      "Flow Byts/s    22954\n",
      "dtype: int64\n",
      "\n",
      "üîç Checking for Infinite Values...\n",
      "Flow Byts/s    10772\n",
      "Flow Pkts/s    28975\n",
      "dtype: int64\n",
      "\n",
      "üîç Checking for Completely Empty Rows...\n",
      "‚úÖ Total Empty Rows: 0\n",
      "\n",
      "üîç Checking for Extra Spaces in Attack Labels...\n",
      "Unique labels after stripping:\n",
      "['Benign' 'FTP-BruteForce' 'SSH-Bruteforce' 'DoS attacks-GoldenEye'\n",
      " 'DoS attacks-Slowloris' 'DoS attacks-SlowHTTPTest' 'DoS attacks-Hulk'\n",
      " 'Label' 'DDOS attack-LOIC-UDP' 'DDOS attack-HOIC' 'Brute Force -Web'\n",
      " 'Brute Force -XSS' 'SQL Injection' 'Infilteration' 'Bot']\n",
      "\n",
      "üîç Checking for Non-Numeric Values in Numeric Columns...\n",
      "Dst Port         59\n",
      "Protocol         59\n",
      "Flow Duration    59\n",
      "Tot Fwd Pkts     59\n",
      "Tot Bwd Pkts     59\n",
      "                 ..\n",
      "Active Min       59\n",
      "Idle Mean        59\n",
      "Idle Std         59\n",
      "Idle Max         59\n",
      "Idle Min         59\n",
      "Length: 78, dtype: int64\n",
      "\n",
      "üéâ **Final Verification Complete!** üéâ\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\nüöÄ **Final Data Verification Started...** üöÄ\")\n",
    "\n",
    "### ‚úÖ 1. CHECK FOR DUPLICATE ROWS\n",
    "print(\"\\nüîç Checking for Duplicate Rows...\")\n",
    "duplicates = df_main.duplicated().sum()\n",
    "print(f\"‚úÖ Total Duplicate Rows: {duplicates}\")\n",
    "\n",
    "### ‚úÖ 2. CHECK FOR REPEATED HEADERS\n",
    "print(\"\\nüîç Checking for Repeated Headers...\")\n",
    "\n",
    "# --- TH√äM KI·ªÇM TRA N√ÄY ---\n",
    "if df_main.empty:\n",
    "    print(\"‚ö†Ô∏è Warning: DataFrame is empty. Cannot check for repeated headers in row 1.\")\n",
    "    # B·∫°n c√≥ th·ªÉ g√°n header_duplicates = 0 ho·∫∑c x·ª≠ l√Ω ph√π h·ª£p n·∫øu c·∫ßn\n",
    "    header_duplicates = 0\n",
    "else:\n",
    "    try:\n",
    "        # Chuy·ªÉn h√†ng ƒë·∫ßu ti√™n th√†nh chu·ªói ƒë·ªÉ so s√°nh an to√†n h∆°n v·ªõi t√™n c·ªôt (l√† chu·ªói)\n",
    "        first_row_as_str = df_main.iloc[0].astype(str)\n",
    "        header_duplicates = (first_row_as_str == df_main.columns).sum()\n",
    "\n",
    "        # ƒêi·ªÅu ki·ªán ki·ªÉm tra (c√≥ th·ªÉ gi·ªØ nguy√™n ho·∫∑c ƒëi·ªÅu ch·ªânh)\n",
    "        if header_duplicates >= len(df_main.columns) // 2:\n",
    "            print(f\"‚ö†Ô∏è Warning: Dataset might have repeated headers in row 1!\") # C√≥ th·ªÉ c√≥ header l·∫∑p l·∫°i ·ªü h√†ng 1\n",
    "        else:\n",
    "            print(\"‚úÖ No apparent repeated headers found in row 1.\") # Kh√¥ng th·∫•y header l·∫∑p l·∫°i r√µ r√†ng ·ªü h√†ng 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error checking repeated headers in row 1: {e}\")\n",
    "        header_duplicates = 0 # X·ª≠ l√Ω l·ªói\n",
    "\n",
    "### ‚úÖ 3. CHECK FOR MISSING / NULL VALUES\n",
    "print(\"\\nüîç Checking for Missing Values (NaN)...\")\n",
    "missing_values = df_main.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "if not missing_values.empty:\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"‚úÖ No Missing Values Found!\")\n",
    "\n",
    "### ‚úÖ 4. CHECK FOR INFINITE VALUES (INF, -INF)\n",
    "print(\"\\nüîç Checking for Infinite Values...\")\n",
    "inf_values = (df_main == np.inf).sum() + (df_main == -np.inf).sum()\n",
    "inf_values = inf_values[inf_values > 0]\n",
    "if not inf_values.empty:\n",
    "    print(inf_values)\n",
    "else:\n",
    "    print(\"‚úÖ No Infinite Values Found!\")\n",
    "\n",
    "### ‚úÖ 5. CHECK FOR EMPTY ROWS\n",
    "print(\"\\nüîç Checking for Completely Empty Rows...\")\n",
    "empty_rows = (df_main.isnull().all(axis=1)).sum()\n",
    "print(f\"‚úÖ Total Empty Rows: {empty_rows}\")\n",
    "\n",
    "\n",
    "### ‚úÖ 7. CHECK FOR EXTRA SPACES IN ATTACK LABELS\n",
    "print(\"\\nüîç Checking for Extra Spaces in Attack Labels...\")\n",
    "\n",
    "try:\n",
    "    # Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu c·ªßa c·ªôt sang string tr∆∞·ªõc khi strip\n",
    "    df_main[\"Label\"] = df_main[\"Label\"].astype(str).str.strip()\n",
    "    print(\"Unique labels after stripping:\")\n",
    "    print(df_main[\"Label\"].unique())\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error processing 'Label' column: {e}\")\n",
    "\n",
    "### ‚úÖ 8. CHECK FOR NON-NUMERIC VALUES IN NUMERIC COLUMNS\n",
    "print(\"\\nüîç Checking for Non-Numeric Values in Numeric Columns...\")\n",
    "for col in df_main.columns:\n",
    "    if col not in [\"Label\", \"Timestamp\"]:  # Exclude categorical columns\n",
    "        df_main[col] = pd.to_numeric(df_main[col], errors=\"coerce\")\n",
    "\n",
    "non_numeric_counts = df_main.isnull().sum()\n",
    "non_numeric_counts = non_numeric_counts[non_numeric_counts > 0]\n",
    "if not non_numeric_counts.empty:\n",
    "    print(non_numeric_counts)\n",
    "else:\n",
    "    print(\"‚úÖ No Non-Numeric Values Found!\")\n",
    "\n",
    "print(\"\\nüéâ **Final Verification Complete!** üéâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e184fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Current RAM Usage: 4.29 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import IPython\n",
    "\n",
    "# ‚úÖ Function to Monitor RAM Usage\n",
    "def check_ram():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_usage = process.memory_info().rss / 1024 ** 3  # Convert to GB\n",
    "    print(f\"üîç Current RAM Usage: {ram_usage:.2f} GB\")\n",
    "\n",
    "# ‚úÖ Function to Clean RAM\n",
    "def clean_ram():\n",
    "    gc.collect()  # ‚úÖ Free Unused Memory\n",
    "    IPython.display.clear_output(wait=True)  # ‚úÖ Clear Output (Optional)\n",
    "    check_ram()  # ‚úÖ Show Updated RAM Usage\n",
    "\n",
    "# Example Usage:\n",
    "# Run this after every large operation (like merging datasets, model training)\n",
    "check_ram()  # Before cleaning\n",
    "clean_ram()  # After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe034be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Optimized Processing (Without Predefined dtypes)...\n",
      "üìÇ Processing Chunks for: dataset/CSE-CIC-IDS2018\\02-14-2018.csv\n",
      "‚úÖ Header written to dataset/working2/merged_clean(real).csv\n",
      "‚úÖ Finished processing chunks for: 02-14-2018.csv\n",
      "üìÇ Processing Chunks for: dataset/CSE-CIC-IDS2018\\02-15-2018.csv\n",
      "‚úÖ Finished processing chunks for: 02-15-2018.csv\n",
      "üìÇ Processing Chunks for: dataset/CSE-CIC-IDS2018\\02-16-2018.csv\n",
      "‚úÖ Finished processing chunks for: 02-16-2018.csv\n",
      "‚ùå Skipping 02-20-2018.csv (Too Large & Mostly Benign)\n",
      "üìÇ Processing Chunks for: dataset/CSE-CIC-IDS2018\\02-21-2018.csv\n",
      "‚úÖ Finished processing chunks for: 02-21-2018.csv\n",
      "üìÇ Processing Chunks for: dataset/CSE-CIC-IDS2018\\02-22-2018.csv\n",
      "‚úÖ Finished processing chunks for: 02-22-2018.csv\n",
      "üìÇ Processing Chunks for: dataset/CSE-CIC-IDS2018\\02-23-2018.csv\n",
      "‚úÖ Finished processing chunks for: 02-23-2018.csv\n",
      "üìÇ Processing Chunks for: dataset/CSE-CIC-IDS2018\\02-28-2018.csv\n",
      "‚úÖ Finished processing chunks for: 02-28-2018.csv\n",
      "üìÇ Processing Chunks for: dataset/CSE-CIC-IDS2018\\03-01-2018.csv\n",
      "‚úÖ Finished processing chunks for: 03-01-2018.csv\n",
      "üìÇ Processing Chunks for: dataset/CSE-CIC-IDS2018\\03-02-2018.csv\n",
      "‚úÖ Finished processing chunks for: 03-02-2018.csv\n",
      "\n",
      "üéâ **Optimized Processing Complete!** üéâ\n",
      "‚úÖ Cleaned data appended to: dataset/working2/merged_clean(real).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "data_dir = \"dataset/CSE-CIC-IDS2018\"\n",
    "output_path = \"dataset/working2/merged_clean(real).csv\"\n",
    "\n",
    "# K√≠ch th∆∞·ªõc m·ªói chunk (s·ªë d√≤ng ƒë·ªçc m·ªói l·∫ßn, t√πy ch·ªânh theo RAM c·ªßa b·∫°n)\n",
    "chunksize = 80000 \n",
    "\n",
    "# Bi·∫øn c·ªù ƒë·ªÉ ki·ªÉm tra xem ƒë√£ ghi header ch∆∞a\n",
    "header_written = False\n",
    "# Danh s√°ch c√°c c·ªôt th·ª±c t·∫ø s·∫Ω ƒë∆∞·ª£c l·∫•y t·ª´ file ƒë·∫ßu ti√™n\n",
    "actual_columns = None\n",
    "\n",
    "print(\"Starting Optimized Processing (Without Predefined dtypes)...\")\n",
    "\n",
    "# L·∫•y danh s√°ch file .csv\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "\n",
    "for filename in csv_files:\n",
    "    if filename == \"02-20-2018.csv\":\n",
    "        print(f\"Skipping {filename} (Too Large & Mostly Benign)\")\n",
    "        continue\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    print(f\"Processing Chunks for: {file_path}\")\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(\n",
    "            file_path,\n",
    "            chunksize=chunksize,\n",
    "            low_memory=False\n",
    "        )\n",
    "        for i, chunk in enumerate(chunk_iterator):\n",
    "            if not header_written:\n",
    "                actual_columns = chunk.columns \n",
    "                chunk.head(0).to_csv(output_path, mode='w', header=True, index=False)\n",
    "                header_written = True\n",
    "                print(f\"Header written to {output_path}\")\n",
    "            if actual_columns is not None:\n",
    "                try:\n",
    "                    chunk = chunk[actual_columns]\n",
    "                except KeyError as e_col:\n",
    "                    missing_cols = set(actual_columns) - set(chunk.columns)\n",
    "                    print(f\"Warning: Chunk {i} in {filename} missing columns: {missing_cols}. Skipping chunk or filling NaNs might be needed.\")\n",
    "                    for col in missing_cols:\n",
    "                         chunk[col] = pd.NA\n",
    "                    chunk = chunk[actual_columns]\n",
    "            try:\n",
    "                if 'Dst Port' in chunk.columns:\n",
    "                    chunk = chunk[chunk['Dst Port'].astype(str) != 'Dst Port']\n",
    "                else:\n",
    "                     print(f\"Warning: 'Dst Port' column not found in chunk {i} of {filename}. Cannot remove repeated headers based on it.\")\n",
    "            except Exception as e_clean:\n",
    "                 print(f\"Error cleaning header in chunk {i} of {filename}: {e_clean}\")\n",
    "            chunk.to_csv(output_path, mode='a', header=False, index=False)\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "        print(f\"Finished processing for: {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Warning: File is empty - {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred processing {filename}: {e}\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "print(\"\\nOptimized Processing Complete!\")\n",
    "print(f\"Cleaned data appended to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f4af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Current RAM Usage: 4.63 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import IPython\n",
    "\n",
    "def check_ram():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_usage = process.memory_info().rss / 1024 ** 3 \n",
    "    print(f\"üîç Current RAM Usage: {ram_usage:.2f} GB\")\n",
    "\n",
    "def clean_ram():\n",
    "    gc.collect()\n",
    "    IPython.display.clear_output(wait=True)  # ‚úÖ Clear Output (Optional)\n",
    "    check_ram()\n",
    "\n",
    "check_ram() \n",
    "clean_ram() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cab99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ B·∫Øt ƒë·∫ßu qu√° tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu theo t·ª´ng ph·∫ßn (chunking)...\n",
      "   - ƒêang ƒë·ªçc t·ªáp 'dataset/working2/merged_clean(real).csv' v·ªõi k√≠ch th∆∞·ªõc chunk l√† 100000...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 1...\n",
      "     -> Chunk 1 x·ª≠ l√Ω xong trong 0.46 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 2...\n",
      "     -> Chunk 2 x·ª≠ l√Ω xong trong 0.51 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 3...\n",
      "     -> Chunk 3 x·ª≠ l√Ω xong trong 0.54 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 4...\n",
      "     -> Chunk 4 x·ª≠ l√Ω xong trong 0.63 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 5...\n",
      "     -> Chunk 5 x·ª≠ l√Ω xong trong 0.81 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 5...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 6...\n",
      "     -> Chunk 6 x·ª≠ l√Ω xong trong 0.88 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 7...\n",
      "     -> Chunk 7 x·ª≠ l√Ω xong trong 0.86 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 8...\n",
      "     -> Chunk 8 x·ª≠ l√Ω xong trong 0.96 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 9...\n",
      "     -> Chunk 9 x·ª≠ l√Ω xong trong 0.93 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 10...\n",
      "     -> Chunk 10 x·ª≠ l√Ω xong trong 0.93 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 10...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 11...\n",
      "     -> Chunk 11 x·ª≠ l√Ω xong trong 0.96 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 12...\n",
      "     -> Chunk 12 x·ª≠ l√Ω xong trong 0.91 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 13...\n",
      "     -> Chunk 13 x·ª≠ l√Ω xong trong 0.86 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 14...\n",
      "     -> Chunk 14 x·ª≠ l√Ω xong trong 0.90 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 15...\n",
      "     -> Chunk 15 x·ª≠ l√Ω xong trong 0.82 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 15...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 16...\n",
      "     -> Chunk 16 x·ª≠ l√Ω xong trong 0.93 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 17...\n",
      "     -> Chunk 17 x·ª≠ l√Ω xong trong 0.82 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 18...\n",
      "     -> Chunk 18 x·ª≠ l√Ω xong trong 0.83 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 19...\n",
      "     -> Chunk 19 x·ª≠ l√Ω xong trong 0.90 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 20...\n",
      "     -> Chunk 20 x·ª≠ l√Ω xong trong 0.88 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 20...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 21...\n",
      "     -> Chunk 21 x·ª≠ l√Ω xong trong 0.91 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 22...\n",
      "     -> Chunk 22 x·ª≠ l√Ω xong trong 0.47 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 23...\n",
      "     -> Chunk 23 x·ª≠ l√Ω xong trong 0.74 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 24...\n",
      "     -> Chunk 24 x·ª≠ l√Ω xong trong 0.76 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 25...\n",
      "     -> Chunk 25 x·ª≠ l√Ω xong trong 0.75 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 25...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 26...\n",
      "     -> Chunk 26 x·ª≠ l√Ω xong trong 0.78 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 27...\n",
      "     -> Chunk 27 x·ª≠ l√Ω xong trong 0.76 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 28...\n",
      "     -> Chunk 28 x·ª≠ l√Ω xong trong 0.90 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 29...\n",
      "     -> Chunk 29 x·ª≠ l√Ω xong trong 0.85 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 30...\n",
      "     -> Chunk 30 x·ª≠ l√Ω xong trong 0.84 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 30...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 31...\n",
      "     -> Chunk 31 x·ª≠ l√Ω xong trong 0.89 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 32...\n",
      "     -> Chunk 32 x·ª≠ l√Ω xong trong 0.66 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 33...\n",
      "     -> Chunk 33 x·ª≠ l√Ω xong trong 0.71 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 34...\n",
      "     -> Chunk 34 x·ª≠ l√Ω xong trong 0.66 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 35...\n",
      "     -> Chunk 35 x·ª≠ l√Ω xong trong 0.69 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 35...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 36...\n",
      "     -> Chunk 36 x·ª≠ l√Ω xong trong 0.70 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 37...\n",
      "     -> Chunk 37 x·ª≠ l√Ω xong trong 0.76 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 38...\n",
      "     -> Chunk 38 x·ª≠ l√Ω xong trong 0.84 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 39...\n",
      "     -> Chunk 39 x·ª≠ l√Ω xong trong 0.94 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 40...\n",
      "     -> Chunk 40 x·ª≠ l√Ω xong trong 0.80 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 40...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 41...\n",
      "     -> Chunk 41 x·ª≠ l√Ω xong trong 0.93 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 42...\n",
      "     -> Chunk 42 x·ª≠ l√Ω xong trong 0.87 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 43...\n",
      "     -> Chunk 43 x·ª≠ l√Ω xong trong 1.00 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 44...\n",
      "     -> Chunk 44 x·ª≠ l√Ω xong trong 1.01 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 45...\n",
      "     -> Chunk 45 x·ª≠ l√Ω xong trong 0.99 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 45...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 46...\n",
      "     -> Chunk 46 x·ª≠ l√Ω xong trong 0.91 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 47...\n",
      "     -> Chunk 47 x·ª≠ l√Ω xong trong 0.94 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 48...\n",
      "     -> Chunk 48 x·ª≠ l√Ω xong trong 0.92 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 49...\n",
      "     -> Chunk 49 x·ª≠ l√Ω xong trong 1.06 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 50...\n",
      "     -> Chunk 50 x·ª≠ l√Ω xong trong 1.02 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 50...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 51...\n",
      "     -> Chunk 51 x·ª≠ l√Ω xong trong 0.98 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 52...\n",
      "     -> Chunk 52 x·ª≠ l√Ω xong trong 1.01 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 53...\n",
      "     -> Chunk 53 x·ª≠ l√Ω xong trong 1.03 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 54...\n",
      "     -> Chunk 54 x·ª≠ l√Ω xong trong 1.02 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 55...\n",
      "     -> Chunk 55 x·ª≠ l√Ω xong trong 0.96 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 55...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 56...\n",
      "     -> Chunk 56 x·ª≠ l√Ω xong trong 1.04 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 57...\n",
      "     -> Chunk 57 x·ª≠ l√Ω xong trong 1.02 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 58...\n",
      "     -> Chunk 58 x·ª≠ l√Ω xong trong 1.04 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 59...\n",
      "     -> Chunk 59 x·ª≠ l√Ω xong trong 1.02 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 60...\n",
      "     -> Chunk 60 x·ª≠ l√Ω xong trong 0.96 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 60...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 61...\n",
      "     -> Chunk 61 x·ª≠ l√Ω xong trong 0.97 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 62...\n",
      "     -> Chunk 62 x·ª≠ l√Ω xong trong 1.00 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 63...\n",
      "     -> Chunk 63 x·ª≠ l√Ω xong trong 1.03 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 64...\n",
      "     -> Chunk 64 x·ª≠ l√Ω xong trong 0.97 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 65...\n",
      "     -> Chunk 65 x·ª≠ l√Ω xong trong 1.01 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 65...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 66...\n",
      "     -> Chunk 66 x·ª≠ l√Ω xong trong 1.07 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 67...\n",
      "     -> Chunk 67 x·ª≠ l√Ω xong trong 1.00 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 68...\n",
      "     -> Chunk 68 x·ª≠ l√Ω xong trong 0.93 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 69...\n",
      "     -> Chunk 69 x·ª≠ l√Ω xong trong 0.94 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 70...\n",
      "     -> Chunk 70 x·ª≠ l√Ω xong trong 0.97 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 70...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 71...\n",
      "     -> Chunk 71 x·ª≠ l√Ω xong trong 0.96 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 72...\n",
      "     -> Chunk 72 x·ª≠ l√Ω xong trong 1.01 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 73...\n",
      "     -> Chunk 73 x·ª≠ l√Ω xong trong 0.89 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 74...\n",
      "     -> Chunk 74 x·ª≠ l√Ω xong trong 0.86 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 75...\n",
      "     -> Chunk 75 x·ª≠ l√Ω xong trong 0.91 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 75...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 76...\n",
      "     -> Chunk 76 x·ª≠ l√Ω xong trong 0.96 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 77...\n",
      "     -> Chunk 77 x·ª≠ l√Ω xong trong 0.93 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 78...\n",
      "     -> Chunk 78 x·ª≠ l√Ω xong trong 1.02 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 79...\n",
      "     -> Chunk 79 x·ª≠ l√Ω xong trong 0.93 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 80...\n",
      "     -> Chunk 80 x·ª≠ l√Ω xong trong 0.97 gi√¢y.\n",
      "   - ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk 80...\n",
      "   - ƒêang x·ª≠ l√Ω chunk 81...\n",
      "     -> Chunk 81 x·ª≠ l√Ω xong trong 0.89 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 82...\n",
      "     -> Chunk 82 x·ª≠ l√Ω xong trong 0.90 gi√¢y.\n",
      "   - ƒêang x·ª≠ l√Ω chunk 83...\n",
      "     -> Chunk 83 x·ª≠ l√Ω xong trong 0.87 gi√¢y.\n",
      "\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω t·ªïng c·ªông 83 chunk. B·∫Øt ƒë·∫ßu k·∫øt h·ª£p...\n",
      "   - K·∫øt h·ª£p 83 chunk th√†nh c√¥ng trong 1.40 gi√¢y.\n",
      "   - ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ t·ª´ danh s√°ch chunk.\n",
      "\n",
      "üßπ Th·ª±c hi·ªán c√°c b∆∞·ªõc l√†m s·∫°ch cu·ªëi c√πng tr√™n DataFrame ƒë√£ k·∫øt h·ª£p...\n",
      "   - B∆∞·ªõc 4: ƒê√£ x√≥a 1982483 h√†ng tr√πng l·∫∑p t·ªïng th·ªÉ.\n",
      "   - B∆∞·ªõc 7: ƒê√£ x√≥a 8 c·ªôt kh√¥ng ƒë·ªïi: ['Bwd PSH Flags', 'Bwd URG Flags', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg']\n",
      "   - B∆∞·ªõc 8a: ƒê√£ chuy·ªÉn/x√°c nh·∫≠n c·ªôt 'Protocol' sang ki·ªÉu 'category'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linhv\\AppData\\Local\\Temp\\ipykernel_20052\\3323830457.py:141: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if not pd.api.types.is_categorical_dtype(df_main[\"Protocol\"]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - B∆∞·ªõc 8b: ƒê√£ m√£ h√≥a 'Protocol' b·∫±ng One-Hot Encoding (ki·ªÉu int8).\n",
      "   - B∆∞·ªõc 9: ƒê√£ di chuy·ªÉn c·ªôt 'Label' v·ªÅ cu·ªëi.\n",
      "   - B∆∞·ªõc 10: Tinh ch·ªânh ki·ªÉu d·ªØ li·ªáu 'Dst Port'...\n",
      "     -> ƒê√£ chuy·ªÉn ƒë·ªïi 'Dst Port' sang uint16.\n",
      "   - T·ªëi ∆∞u h√≥a l·∫°i ki·ªÉu d·ªØ li·ªáu s·ªë h·ªçc cu·ªëi c√πng...\n",
      "\n",
      "üìä Th√¥ng tin DataFrame cu·ªëi c√πng:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5881823 entries, 0 to 7864305\n",
      "Data columns (total 73 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Dst Port           uint16 \n",
      " 1   Flow Duration      int64  \n",
      " 2   Tot Fwd Pkts       int32  \n",
      " 3   Tot Bwd Pkts       int32  \n",
      " 4   TotLen Fwd Pkts    int32  \n",
      " 5   TotLen Bwd Pkts    float64\n",
      " 6   Fwd Pkt Len Max    int32  \n",
      " 7   Fwd Pkt Len Min    int16  \n",
      " 8   Fwd Pkt Len Mean   float64\n",
      " 9   Fwd Pkt Len Std    float64\n",
      " 10  Bwd Pkt Len Max    int32  \n",
      " 11  Bwd Pkt Len Min    int16  \n",
      " 12  Bwd Pkt Len Mean   float64\n",
      " 13  Bwd Pkt Len Std    float32\n",
      " 14  Flow Byts/s        float64\n",
      " 15  Flow Pkts/s        float64\n",
      " 16  Flow IAT Mean      float64\n",
      " 17  Flow IAT Std       float64\n",
      " 18  Flow IAT Max       float64\n",
      " 19  Flow IAT Min       float64\n",
      " 20  Fwd IAT Tot        float64\n",
      " 21  Fwd IAT Mean       float64\n",
      " 22  Fwd IAT Std        float64\n",
      " 23  Fwd IAT Max        float64\n",
      " 24  Fwd IAT Min        float64\n",
      " 25  Bwd IAT Tot        float64\n",
      " 26  Bwd IAT Mean       float64\n",
      " 27  Bwd IAT Std        float64\n",
      " 28  Bwd IAT Max        float64\n",
      " 29  Bwd IAT Min        float64\n",
      " 30  Fwd PSH Flags      int8   \n",
      " 31  Fwd URG Flags      int8   \n",
      " 32  Fwd Header Len     int32  \n",
      " 33  Bwd Header Len     int32  \n",
      " 34  Fwd Pkts/s         float64\n",
      " 35  Bwd Pkts/s         float64\n",
      " 36  Pkt Len Min        int16  \n",
      " 37  Pkt Len Max        int32  \n",
      " 38  Pkt Len Mean       float32\n",
      " 39  Pkt Len Std        float64\n",
      " 40  Pkt Len Var        float64\n",
      " 41  FIN Flag Cnt       int8   \n",
      " 42  SYN Flag Cnt       int8   \n",
      " 43  RST Flag Cnt       int8   \n",
      " 44  PSH Flag Cnt       int8   \n",
      " 45  ACK Flag Cnt       int8   \n",
      " 46  URG Flag Cnt       int8   \n",
      " 47  CWE Flag Count     int8   \n",
      " 48  ECE Flag Cnt       int8   \n",
      " 49  Down/Up Ratio      int16  \n",
      " 50  Pkt Size Avg       float64\n",
      " 51  Fwd Seg Size Avg   float64\n",
      " 52  Bwd Seg Size Avg   float64\n",
      " 53  Subflow Fwd Pkts   int32  \n",
      " 54  Subflow Fwd Byts   int32  \n",
      " 55  Subflow Bwd Pkts   int32  \n",
      " 56  Subflow Bwd Byts   int32  \n",
      " 57  Init Fwd Win Byts  int32  \n",
      " 58  Init Bwd Win Byts  int32  \n",
      " 59  Fwd Act Data Pkts  int32  \n",
      " 60  Fwd Seg Size Min   int8   \n",
      " 61  Active Mean        float64\n",
      " 62  Active Std         float64\n",
      " 63  Active Max         float64\n",
      " 64  Active Min         float64\n",
      " 65  Idle Mean          float64\n",
      " 66  Idle Std           float64\n",
      " 67  Idle Max           float64\n",
      " 68  Idle Min           float64\n",
      " 69  Protocol_0         int8   \n",
      " 70  Protocol_6         int8   \n",
      " 71  Protocol_17        int8   \n",
      " 72  Label              object \n",
      "dtypes: float32(2), float64(35), int16(4), int32(15), int64(1), int8(14), object(1), uint16(1)\n",
      "memory usage: 2.4 GB\n",
      "\n",
      "üíæ ƒêang l∆∞u DataFrame ƒë√£ x·ª≠ l√Ω v√†o 'dataset/working2/cleaned_optimized_dataset_chunked.csv'...\n",
      "‚úÖ D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω theo chunk ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng trong 207.64 gi√¢y.\n",
      "\n",
      "‚ú® Qu√° tr√¨nh ho√†n t·∫•t trong 453.73 gi√¢y. B·ªô nh·ªõ ƒë√£ ƒë∆∞·ª£c gi·∫£i ph√≥ng.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time # Th√™m th∆∞ vi·ªán time ƒë·ªÉ ƒëo th·ªùi gian\n",
    "\n",
    "# --- H√†m x·ª≠ l√Ω cho m·ªói chunk ---\n",
    "def process_chunk(chunk_df):\n",
    "    \"\"\"√Åp d·ª•ng c√°c b∆∞·ªõc l√†m s·∫°ch c∆° b·∫£n cho m·ªôt DataFrame chunk.\"\"\"\n",
    "    try:\n",
    "        # B∆∞·ªõc 1: X√≥a ti√™u ƒë·ªÅ l·∫∑p l·∫°i\n",
    "        chunk_df.drop(chunk_df[chunk_df[\"Label\"] == \"Label\"].index, inplace=True, errors='ignore')\n",
    "        # B∆∞·ªõc 2: X·ª≠ l√Ω gi√° tr·ªã v√¥ c·ª±c & NaN\n",
    "        chunk_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        # B∆∞·ªõc 3: X√≥a h√†ng ch·ª©a to√†n NaN\n",
    "        chunk_df.dropna(inplace=True)\n",
    "        # B∆∞·ªõc 4: X√≥a h√†ng tr√πng l·∫∑p \n",
    "        chunk_df.drop_duplicates(inplace=True)\n",
    "        # B∆∞·ªõc 5: Chuy·ªÉn ƒë·ªïi c·ªôt s·ªë h·ªçc v√† t·ªëi ∆∞u ki·ªÉu d·ªØ li·ªáu (downcast)\n",
    "        potential_non_numeric = [\"Label\", \"Protocol\", \"Timestamp\"]\n",
    "        numeric_cols = chunk_df.columns.difference(potential_non_numeric, sort=False)\n",
    "        # Ch·ªâ chuy·ªÉn ƒë·ªïi c√°c c·ªôt th·ª±c s·ª± t·ªìn t·∫°i trong chunk\n",
    "        cols_to_convert = [col for col in numeric_cols if col in chunk_df.columns]\n",
    "        if cols_to_convert:\n",
    "            chunk_df[cols_to_convert] = chunk_df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "            # T·ªëi ∆∞u ki·ªÉu d·ªØ li·ªáu (downcasting)\n",
    "            for col in cols_to_convert:\n",
    "                 if col in chunk_df.columns:\n",
    "                    if chunk_df[col].dtype == 'float64':\n",
    "                        chunk_df[col] = pd.to_numeric(chunk_df[col], downcast='float')\n",
    "                    elif chunk_df[col].dtype == 'int64':\n",
    "                        chunk_df[col] = pd.to_numeric(chunk_df[col], downcast='integer')\n",
    "        # B∆∞·ªõc 6: X√≥a c·ªôt \"Timestamp\" n·∫øu t·ªìn t·∫°i\n",
    "        if \"Timestamp\" in chunk_df.columns:\n",
    "            chunk_df.drop(columns=[\"Timestamp\"], inplace=True)\n",
    "        # T·∫°m th·ªùi gi·ªØ l·∫°i c·ªôt Protocol d∆∞·ªõi d·∫°ng category ƒë·ªÉ x·ª≠ l√Ω sau khi k·∫øt h·ª£p\n",
    "        if \"Protocol\" in chunk_df.columns:\n",
    "            chunk_df[\"Protocol\"] = chunk_df[\"Protocol\"].astype('category')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω chunk: {e}. B·ªè qua chunk n√†y ho·∫∑c x·ª≠ l√Ω l·ªói...\")\n",
    "        # Tr·∫£ v·ªÅ DataFrame r·ªóng ho·∫∑c x·ª≠ l√Ω l·ªói ph√π h·ª£p\n",
    "        return pd.DataFrame() # Tr·∫£ v·ªÅ DF r·ªóng ƒë·ªÉ tr√°nh l·ªói khi concat\n",
    "    return chunk_df\n",
    "\n",
    "# --- Ch∆∞∆°ng tr√¨nh ch√≠nh ---\n",
    "start_time = time.time()\n",
    "print(\"üîÑ B·∫Øt ƒë·∫ßu qu√° tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu theo t·ª´ng ph·∫ßn (chunking)...\")\n",
    "\n",
    "chunksize = 100000\n",
    "processed_chunks = []\n",
    "file_path = \"dataset/working2/merged_clean(real).csv\"\n",
    "output_path = \"dataset/working2/cleaned_optimized_dataset_chunked.csv\"\n",
    "try:\n",
    "    chunk_iterator = pd.read_csv(file_path, chunksize=chunksize, low_memory=False)\n",
    "    total_chunks = 0\n",
    "\n",
    "    print(f\"ƒêang ƒë·ªçc t·ªáp '{file_path}' v·ªõi k√≠ch th∆∞·ªõc chunk l√† {chunksize}...\")\n",
    "\n",
    "    for i, chunk in enumerate(chunk_iterator):\n",
    "        total_chunks = i + 1\n",
    "        print(f\"ƒêang x·ª≠ l√Ω chunk {total_chunks}...\")\n",
    "        start_chunk_time = time.time()\n",
    "        processed_chunk = process_chunk(chunk.copy()) \n",
    "        if not processed_chunk.empty:\n",
    "             processed_chunks.append(processed_chunk)\n",
    "\n",
    "        chunk_time = time.time() - start_chunk_time\n",
    "        print(f\"-> Chunk {total_chunks} x·ª≠ l√Ω xong trong {chunk_time:.2f} gi√¢y.\")\n",
    "\n",
    "        # Thu gom r√°c ƒë·ªãnh k·ª≥ ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "        if total_chunks % 5 == 0:\n",
    "            print(f\"ƒêang th·ª±c hi·ªán thu gom r√°c (GC) sau chunk {total_chunks}...\")\n",
    "            gc.collect()\n",
    "\n",
    "    if not processed_chunks:\n",
    "        print(\"Kh√¥ng c√≥ chunk n√†o ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng. Ki·ªÉm tra l·∫°i d·ªØ li·ªáu ƒë·∫ßu v√†o ho·∫∑c h√†m process_chunk.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"\\nƒê√£ x·ª≠ l√Ω t·ªïng c·ªông {total_chunks} chunk. B·∫Øt ƒë·∫ßu k·∫øt h·ª£p...\")\n",
    "    start_concat_time = time.time()\n",
    "\n",
    "    # K·∫øt h·ª£p t·∫•t c·∫£ c√°c chunk ƒë√£ x·ª≠ l√Ω\n",
    "    df_main = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "    concat_time = time.time() - start_concat_time\n",
    "    print(f\"K·∫øt h·ª£p {len(processed_chunks)} chunk th√†nh c√¥ng trong {concat_time:.2f} gi√¢y.\")\n",
    "\n",
    "    # Gi·∫£i ph√≥ng b·ªô nh·ªõ ngay l·∫≠p t·ª©c t·ª´ danh s√°ch chunk\n",
    "    del processed_chunks\n",
    "    gc.collect()\n",
    "    print(\"ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ t·ª´ danh s√°ch chunk.\")\n",
    "\n",
    "    # --- Th·ª±c hi·ªán c√°c b∆∞·ªõc l√†m s·∫°ch cu·ªëi c√πng sau khi k·∫øt h·ª£p ---\n",
    "    print(\"\\nüßπ Th·ª±c hi·ªán c√°c b∆∞·ªõc l√†m s·∫°ch cu·ªëi c√πng tr√™n DataFrame ƒë√£ k·∫øt h·ª£p...\")\n",
    "\n",
    "    # B∆∞·ªõc 4: X√≥a c√°c h√†ng tr√πng l·∫∑p *t·ªïng th·ªÉ*\n",
    "    rows_before_dup_drop = len(df_main)\n",
    "    df_main.drop_duplicates(inplace=True)\n",
    "    print(f\"B∆∞·ªõc 4: ƒê√£ x√≥a {rows_before_dup_drop - len(df_main)} h√†ng tr√πng l·∫∑p t·ªïng th·ªÉ.\")\n",
    "\n",
    "    # B∆∞·ªõc 5: X√≥a c√°c c·ªôt c√≥ gi√° tr·ªã kh√¥ng ƒë·ªïi\n",
    "    numeric_cols_final = df_main.select_dtypes(include=np.number).columns\n",
    "    if not numeric_cols_final.empty:\n",
    "        variances = df_main[numeric_cols_final].var()\n",
    "        constant_columns = variances[variances == 0].index\n",
    "        if len(constant_columns) > 0:\n",
    "            df_main.drop(columns=constant_columns, inplace=True)\n",
    "            print(f\"B∆∞·ªõc 5: ƒê√£ x√≥a {len(constant_columns)} c·ªôt kh√¥ng ƒë·ªïi: {list(constant_columns)}\")\n",
    "        else:\n",
    "            print(\"B∆∞·ªõc 5: Kh√¥ng t√¨m th·∫•y c·ªôt n√†o c√≥ gi√° tr·ªã kh√¥ng ƒë·ªïi.\")\n",
    "    else:\n",
    "        print(\"B∆∞·ªõc 5: Kh√¥ng c√≥ c·ªôt s·ªë h·ªçc n√†o ƒë·ªÉ ki·ªÉm tra ph∆∞∆°ng sai.\")\n",
    "\n",
    "\n",
    "    # B∆∞·ªõc 6: M√£ h√≥a \"Protocol\" b·∫±ng One-Hot Encoding\n",
    "    if \"Protocol\" in df_main.columns:\n",
    "        if not pd.api.types.is_categorical_dtype(df_main[\"Protocol\"]):\n",
    "            df_main[\"Protocol\"] = df_main[\"Protocol\"].astype('category')\n",
    "        print(\"B∆∞·ªõc 6a: ƒê√£ chuy·ªÉn/x√°c nh·∫≠n c·ªôt 'Protocol' sang ki·ªÉu 'category'.\")\n",
    "        df_main = pd.get_dummies(df_main, columns=[\"Protocol\"], prefix=\"Protocol\", dtype=np.int8)\n",
    "        print(\"B∆∞·ªõc 6b: ƒê√£ m√£ h√≥a 'Protocol' b·∫±ng One-Hot Encoding (ki·ªÉu int8).\")\n",
    "    else:\n",
    "        print(\"B∆∞·ªõc 6: Kh√¥ng t√¨m th·∫•y c·ªôt 'Protocol' ƒë·ªÉ m√£ h√≥a.\")\n",
    "\n",
    "    # B∆∞·ªõc 7: ƒê·∫£m b·∫£o c·ªôt \"Label\" l√† c·ªôt cu·ªëi c√πng\n",
    "    if \"Label\" in df_main.columns:\n",
    "        label_col = df_main.pop(\"Label\")\n",
    "        df_main.insert(len(df_main.columns), \"Label\", label_col)\n",
    "        print(\"B∆∞·ªõc 7: ƒê√£ di chuy·ªÉn c·ªôt 'Label' v·ªÅ cu·ªëi.\")\n",
    "    else:\n",
    "        print(\"B∆∞·ªõc 7: Kh√¥ng t√¨m th·∫•y c·ªôt 'Label'.\")\n",
    "\n",
    "    # B∆∞·ªõc 10: Tinh ch·ªânh ki·ªÉu d·ªØ li·ªáu cu·ªëi c√πng cho 'Dst Port'\n",
    "    if \"Dst Port\" in df_main.columns:\n",
    "        print(\"B∆∞·ªõc 10: Tinh ch·ªânh ki·ªÉu d·ªØ li·ªáu 'Dst Port'...\")\n",
    "        # Chuy·ªÉn sang s·ªë h·ªçc n·∫øu ch∆∞a ph·∫£i, √©p l·ªói th√†nh NaN\n",
    "        df_main[\"Dst Port\"] = pd.to_numeric(df_main[\"Dst Port\"], errors='coerce')\n",
    "        # X√≥a c√°c h√†ng m√† 'Dst Port' tr·ªü th√†nh NaN sau khi √©p ki·ªÉu\n",
    "        rows_before_na = len(df_main)\n",
    "        df_main.dropna(subset=[\"Dst Port\"], inplace=True)\n",
    "        if len(df_main) < rows_before_na:\n",
    "             print(f\"-> ƒê√£ x√≥a {rows_before_na - len(df_main)} h√†ng c√≥ 'Dst Port' kh√¥ng h·ª£p l·ªá.\")\n",
    "\n",
    "        # Ki·ªÉm tra min/max ƒë·ªÉ ch·ªçn ki·ªÉu integer ph√π h·ª£p nh·∫•t\n",
    "        min_port = df_main[\"Dst Port\"].min()\n",
    "        max_port = df_main[\"Dst Port\"].max()\n",
    "\n",
    "        if pd.notna(min_port) and pd.notna(max_port):\n",
    "            if min_port >= 0 and max_port <= 65535:\n",
    "                df_main[\"Dst Port\"] = df_main[\"Dst Port\"].astype(np.uint16)\n",
    "                print(f\"-> ƒê√£ chuy·ªÉn ƒë·ªïi 'Dst Port' sang uint16.\")\n",
    "            elif min_port >= 0 and max_port <= 4294967295: # Gi·ªõi h·∫°n c·ªßa uint32\n",
    "                 df_main[\"Dst Port\"] = df_main[\"Dst Port\"].astype(np.uint32)\n",
    "                 print(f\"-> ƒê√£ chuy·ªÉn ƒë·ªïi 'Dst Port' sang uint32.\")\n",
    "            else:\n",
    "                 # S·ª≠ d·ª•ng downcast ƒë·ªÉ Pandas t·ª± ch·ªçn ki·ªÉu int ph√π h·ª£p nh·∫•t\n",
    "                 df_main[\"Dst Port\"] = pd.to_numeric(df_main[\"Dst Port\"], downcast='integer')\n",
    "                 print(f\"-> ƒê√£ chuy·ªÉn ƒë·ªïi 'Dst Port' sang ki·ªÉu integer t·ªëi ∆∞u: {df_main['Dst Port'].dtype}.\")\n",
    "        else:\n",
    "             print(\"-> Kh√¥ng th·ªÉ x√°c ƒë·ªãnh min/max cho 'Dst Port' (c√≥ th·ªÉ do c·ªôt r·ªóng). Gi·ªØ nguy√™n ki·ªÉu hi·ªán t·∫°i.\")\n",
    "    else:\n",
    "        print(\"B∆∞·ªõc 10: Kh√¥ng t√¨m th·∫•y c·ªôt 'Dst Port'.\")\n",
    "\n",
    "    # T·ªëi ∆∞u h√≥a l·∫°i t·∫•t c·∫£ c√°c c·ªôt s·ªë h·ªçc m·ªôt l·∫ßn n·ªØa sau c√°c thay ƒë·ªïi\n",
    "    print(\"   - T·ªëi ∆∞u h√≥a l·∫°i ki·ªÉu d·ªØ li·ªáu s·ªë h·ªçc cu·ªëi c√πng...\")\n",
    "    for col in df_main.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        if col in df_main.columns: # Ki·ªÉm tra c·ªôt c√≤n t·ªìn t·∫°i kh√¥ng\n",
    "            if df_main[col].dtype == 'float64':\n",
    "                df_main[col] = pd.to_numeric(df_main[col], downcast='float')\n",
    "            elif df_main[col].dtype == 'int64':\n",
    "                df_main[col] = pd.to_numeric(df_main[col], downcast='integer')\n",
    "\n",
    "    print(\"\\nüìä Th√¥ng tin DataFrame cu·ªëi c√πng:\")\n",
    "    # B∆∞·ªõc 11: Hi·ªÉn th·ªã th√¥ng tin cu·ªëi c√πng c·ªßa DataFrame\n",
    "    df_main.info(memory_usage='deep')\n",
    "\n",
    "    # B∆∞·ªõc 12: L∆∞u d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch\n",
    "    print(f\"\\nüíæ ƒêang l∆∞u DataFrame ƒë√£ x·ª≠ l√Ω v√†o '{output_path}'...\")\n",
    "    start_save_time = time.time()\n",
    "    df_main.to_csv(output_path, index=False)\n",
    "    save_time = time.time() - start_save_time\n",
    "    print(f\"‚úÖ D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω theo chunk ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng trong {save_time:.2f} gi√¢y.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp '{file_path}'. H√£y ƒë·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "     print(f\"‚ùå L·ªói: T·ªáp '{file_path}' tr·ªëng ho·∫∑c kh√¥ng ch·ª©a d·ªØ li·ªáu.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën trong qu√° tr√¨nh x·ª≠ l√Ω: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc() # In chi ti·∫øt l·ªói ƒë·ªÉ g·ª° r·ªëi\n",
    "\n",
    "finally:\n",
    "    if 'df_main' in locals():\n",
    "        del df_main\n",
    "    if 'processed_chunks' in locals() and processed_chunks is not None:\n",
    "         del processed_chunks\n",
    "    gc.collect()\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\n‚ú® Qu√° tr√¨nh ho√†n t·∫•t trong {total_time:.2f} gi√¢y. B·ªô nh·ªõ ƒë√£ ƒë∆∞·ª£c gi·∫£i ph√≥ng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6525e648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features Used for Training:\n",
      "['Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Protocol_0', 'Protocol_6', 'Protocol_17']\n",
      "\n",
      "‚úÖ Feature names saved to feature_list.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = \"dataset/working2/cleaned_dataset.csv\"  # Change this path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Get the feature names (excluding the label column)\n",
    "feature_names = df.columns.tolist()\n",
    "feature_names.remove(\"Label\")\n",
    "\n",
    "print(\"‚úÖ Features Used for Training:\")\n",
    "print(feature_names)\n",
    "\n",
    "# Save feature names to a text file (to transfer to Ubuntu VM)\n",
    "with open(\"dataset/working2/feature_list.txt\", \"w\") as f:\n",
    "    for feature in feature_names:\n",
    "        f.write(feature + \"\\n\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature names saved to feature_list.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1914df",
   "metadata": {},
   "source": [
    "NORMALIZE DATA AND SPLIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading Dataset: dataset/working2/cleaned_dataset.csv ...\n",
      "‚úÖ Dataset Loaded! Shape: (5881823, 73)\n",
      "‚úÖ Train Shape: (4705458, 73), Test Shape: (1176365, 73)\n",
      "‚úÖ Normalization Complete!\n",
      "Train data range: 0.00 to 1.00\n",
      "Test data range: 0.00 to 3.46\n",
      "‚úÖ Scaler saved at: dataset/working2/scaler.pkl\n",
      "\n",
      "üöÄ Applying SMOTE ONLY on Rare Attack Types (to normalized data)...\n",
      "‚úÖ SMOTE Applied! New Train Shape: (4719765, 73)\n",
      "Post-SMOTE range check: 0.00 to 1.00\n",
      "\n",
      "‚úÖ Train & Test Data Saved Successfully!\n",
      "\n",
      "üîç Label Distribution in Train Set (After SMOTE):\n",
      "Label\n",
      "0     4069824\n",
      "2      173921\n",
      "5      116159\n",
      "1      115628\n",
      "9      111473\n",
      "10      75238\n",
      "4       33125\n",
      "7        7926\n",
      "11       5000\n",
      "12       5000\n",
      "13       5000\n",
      "3        1384\n",
      "6          44\n",
      "8          43\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîç Label Distribution in Test Set (Original Distribution):\n",
      "Label\n",
      "0     1017456\n",
      "2       43480\n",
      "5       29040\n",
      "1       28907\n",
      "9       27868\n",
      "10      18810\n",
      "4        8281\n",
      "7        1982\n",
      "3         346\n",
      "11        111\n",
      "12         46\n",
      "13         17\n",
      "6          11\n",
      "8          10\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# ‚úÖ Load Cleaned Data\n",
    "cleaned_file = \"dataset/working2/cleaned_dataset.csv\"\n",
    "print(f\"üìÇ Loading Dataset: {cleaned_file} ...\")\n",
    "df = pd.read_csv(cleaned_file)\n",
    "print(f\"‚úÖ Dataset Loaded! Shape: {df.shape}\")\n",
    "\n",
    "# ‚úÖ (1) Encode Label Column (Multi-Class)\n",
    "label_mapping = {\n",
    "    \"Benign\": 0,\n",
    "    \"Bot\": 1,\n",
    "    \"DDOS attack-HOIC\": 2,\n",
    "    \"DDOS attack-LOIC-UDP\": 3,\n",
    "    \"DoS attacks-GoldenEye\": 4,\n",
    "    \"DoS attacks-Hulk\": 5,\n",
    "    \"DoS attacks-SlowHTTPTest\": 6,\n",
    "    \"DoS attacks-Slowloris\": 7,\n",
    "    \"FTP-BruteForce\": 8,\n",
    "    \"Infilteration\": 9,\n",
    "    \"SSH-Bruteforce\": 10,\n",
    "    \"Brute Force -Web\": 11,\n",
    "    \"Brute Force -XSS\": 12,\n",
    "    \"SQL Injection\": 13\n",
    "}\n",
    "df[\"Label\"] = df[\"Label\"].map(label_mapping)\n",
    "\n",
    "# ‚úÖ (2) Train-Test Split FIRST (BEFORE Normalization)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=20, stratify=df[\"Label\"])\n",
    "print(f\"‚úÖ Train Shape: {df_train.shape}, Test Shape: {df_test.shape}\")\n",
    "\n",
    "# Chu·∫©n h√≥a c√°c c·ªôt s·ªë\n",
    "numeric_cols = df_train.columns.difference([\"Label\"])  # Exclude Label column\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit m√¥ h√¨nh scaler tr√™n t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán\n",
    "scaler.fit(df_train[numeric_cols])\n",
    "\n",
    "# Bi·∫øn ƒë·ªïi t·∫≠p d·ªØ li·ªáu train, test c√πng 1 m√¥ h√¨nh scaler\n",
    "df_train[numeric_cols] = scaler.transform(df_train[numeric_cols])\n",
    "df_test[numeric_cols] = scaler.transform(df_test[numeric_cols])\n",
    "\n",
    "print(f\"Normalization Complete!\")\n",
    "print(f\"Train data range: {df_train[numeric_cols].min().min():.2f} to {df_train[numeric_cols].max().max():.2f}\")\n",
    "print(f\"Test data range: {df_test[numeric_cols].min().min():.2f} to {df_test[numeric_cols].max().max():.2f}\")\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh scaler\n",
    "SCALER_PATH = \"dataset/working2/scaler.pkl\"\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "print(f\"‚úÖ Scaler saved at: {SCALER_PATH}\")\n",
    "\n",
    "# ‚úÖ (5) Handle Imbalance Using SMOTE (Now applied to NORMALIZED data)\n",
    "X_train = df_train.drop(columns=[\"Label\"]).values\n",
    "y_train = df_train[\"Label\"].values\n",
    "\n",
    "# Define rare attack types\n",
    "rare_classes = [11, 12, 13]  # Brute Force -Web, Brute Force -XSS, SQL Injection\n",
    "\n",
    "print(\"\\nüöÄ Applying SMOTE ONLY on Rare Attack Types (to normalized data)...\")\n",
    "smote = SMOTE(sampling_strategy={cls: 5000 for cls in rare_classes}, \n",
    "              random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_train_balanced = pd.DataFrame(X_resampled, columns=df_train.columns[:-1])\n",
    "df_train_balanced[\"Label\"] = y_resampled\n",
    "print(f\"‚úÖ SMOTE Applied! New Train Shape: {df_train_balanced.shape}\")\n",
    "\n",
    "# Verify SMOTE didn't break normalization\n",
    "print(f\"Post-SMOTE range check: {df_train_balanced.iloc[:, :-1].min().min():.2f} to {df_train_balanced.iloc[:, :-1].max().max():.2f}\")\n",
    "\n",
    "# ‚úÖ (6) Save Processed Data\n",
    "df_train_balanced.to_csv(\"dataset/working2/train_balanced1.csv\", index=False)\n",
    "df_test.to_csv(\"dataset/working2/test1.csv\", index=False)\n",
    "print(\"\\n‚úÖ Train & Test Data Saved Successfully!\")\n",
    "\n",
    "# ‚úÖ (7) Verification\n",
    "print(\"\\nüîç Label Distribution in Train Set (After SMOTE):\")\n",
    "print(df_train_balanced[\"Label\"].value_counts())\n",
    "\n",
    "print(\"\\nüîç Label Distribution in Test Set (Original Distribution):\")\n",
    "print(df_test[\"Label\"].value_counts())\n",
    "\n",
    "# Cleanup\n",
    "del df, df_train, X_train, y_train, X_resampled, y_resampled\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8267e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "label_mapping = {\n",
    "    \"Benign\": 0,\n",
    "    \"Bot\": 1,\n",
    "    \"DDOS attack-HOIC\": 2,\n",
    "    \"DDOS attack-LOIC-UDP\": 3,\n",
    "    \"DoS attacks-GoldenEye\": 4,\n",
    "    \"DoS attacks-Hulk\": 5,\n",
    "    \"DoS attacks-SlowHTTPTest\": 6,\n",
    "    \"DoS attacks-Slowloris\": 7,\n",
    "    \"FTP-BruteForce\": 8,\n",
    "    \"Infilteration\": 9,\n",
    "    \"SSH-Bruteforce\": 10,\n",
    "    \"Brute Force -Web\": 11,\n",
    "    \"Brute Force -XSS\": 12,\n",
    "    \"SQL Injection\": 13\n",
    "}\n",
    "torch.save(label_mapping, \"dataset/working2/label_mapping.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b0a49",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7807f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAM and GPU memory cleared!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Clear Python garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ RAM and GPU memory cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbb04290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Current RAM Usage: 0.20 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import IPython\n",
    "\n",
    "# ‚úÖ Function to Monitor RAM Usage\n",
    "def check_ram():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    ram_usage = process.memory_info().rss / 1024 ** 3  # Convert to GB\n",
    "    print(f\"üîç Current RAM Usage: {ram_usage:.2f} GB\")\n",
    "\n",
    "# ‚úÖ Function to Clean RAM\n",
    "def clean_ram():\n",
    "    gc.collect()  # ‚úÖ Free Unused Memory\n",
    "    IPython.display.clear_output(wait=True)  # ‚úÖ Clear Output (Optional)\n",
    "    check_ram()  # ‚úÖ Show Updated RAM Usage\n",
    "\n",
    "# Example Usage:\n",
    "# Run this after every large operation (like merging datasets, model training)\n",
    "check_ram()  # Before cleaning\n",
    "clean_ram()  # After cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04389d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ ƒêang t·∫£i d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω...\n",
      "   - K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán: (4719765, 73)\n",
      "   - K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm th·ª≠: (1176365, 73)\n",
      "\n",
      "üè∑Ô∏è ƒêang t·∫£i b·∫£n ƒë·ªì nh√£n...\n",
      "   - T·∫£i th√†nh c√¥ng b·∫£n ƒë·ªì nh√£n cho 14 l·ªõp.\n",
      "   - C√°c l·ªõp: ['Benign', 'Bot', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DoS attacks-GoldenEye', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'DoS attacks-Slowloris', 'FTP-BruteForce', 'Infilteration', 'SSH-Bruteforce', 'Brute Force -Web', 'Brute Force -XSS', 'SQL Injection']\n",
      "\n",
      "‚öôÔ∏è ƒêang chu·∫©n b·ªã d·ªØ li·ªáu (chuy·ªÉn ƒë·ªïi sang NumPy)...\n",
      "   - K√≠ch th∆∞·ªõc X_train: (4719765, 72), y_train: (4719765,)\n",
      "   - K√≠ch th∆∞·ªõc X_test: (1176365, 72), y_test: (1176365,)\n",
      "\n",
      "üî• Ki·ªÉm tra ph·∫°m vi gi√° tr·ªã d·ªØ li·ªáu (sau khi t·∫£i):\n",
      "   - Ph·∫°m vi ƒë·∫∑c tr∆∞ng Train: 0.0000 ƒë·∫øn 1.0000\n",
      "   - Ph·∫°m vi ƒë·∫∑c tr∆∞ng Test: 0.0000 ƒë·∫øn 3.4572\n",
      "   - ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ t·ª´ Pandas DataFrames.\n",
      "\n",
      "========== üå≥ B·∫Øt ƒë·∫ßu v·ªõi Random Forest ==========\n",
      "   - Kh·ªüi t·∫°o Random Forest v·ªõi: n_estimators=200, max_depth=30, max_features='sqrt'\n",
      "   - ƒêang hu·∫•n luy·ªán Random Forest...\n",
      "‚úÖ Hu·∫•n luy·ªán Random Forest ho√†n th√†nh trong 1147.40 gi√¢y.\n",
      "üíæ M√¥ h√¨nh Random Forest ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: 'dataset/working2/random_forest_model.pkl'\n",
      "\n",
      "üîç B·∫Øt ƒë·∫ßu ƒë√°nh gi√° m√¥ h√¨nh: Random Forest...\n",
      "   - Th·ªùi gian d·ª± ƒëo√°n: 23.28 gi√¢y\n",
      "\n",
      "üèÜ ƒê·ªô ch√≠nh x√°c cu·ªëi c√πng c·ªßa Random Forest: 97.39%\n",
      "\n",
      "üìä B√°o c√°o ph√¢n lo·∫°i (Classification Report) c·ªßa Random Forest:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.98      0.99      0.99   1017456\n",
      "                     Bot       1.00      1.00      1.00     28907\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00     43480\n",
      "    DDOS attack-LOIC-UDP       1.00      1.00      1.00       346\n",
      "   DoS attacks-GoldenEye       1.00      1.00      1.00      8281\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     29040\n",
      "DoS attacks-SlowHTTPTest       0.17      0.18      0.17        11\n",
      "   DoS attacks-Slowloris       1.00      1.00      1.00      1982\n",
      "          FTP-BruteForce       0.00      0.00      0.00        10\n",
      "           Infilteration       0.32      0.09      0.14     27868\n",
      "          SSH-Bruteforce       1.00      1.00      1.00     18810\n",
      "        Brute Force -Web       0.75      0.82      0.78       111\n",
      "        Brute Force -XSS       0.98      0.93      0.96        46\n",
      "           SQL Injection       0.65      0.65      0.65        17\n",
      "\n",
      "                accuracy                           0.97   1176365\n",
      "               macro avg       0.77      0.76      0.76   1176365\n",
      "            weighted avg       0.96      0.97      0.97   1176365\n",
      "\n",
      "‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: 'dataset/working2/random_forest_results.txt'\n",
      "üìà Ma tr·∫≠n nh·∫ßm l·∫´n ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: 'dataset/working2/random_forest_confusion_matrix.png'\n",
      "\n",
      "========== üöÄ B·∫Øt ƒë·∫ßu v·ªõi XGBoost (Th·ª≠ nghi·ªám GPU) ==========\n",
      "   - Ph√°t hi·ªán GPU NVIDIA v√† CUDA.\n",
      "   - S·∫Ω s·ª≠ d·ª•ng 'tree_method': 'gpu_hist' cho XGBoost.\n",
      "   - ƒêang hu·∫•n luy·ªán XGBoost s·ª≠ d·ª•ng gpu_hist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:40:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:40:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hu·∫•n luy·ªán XGBoost (gpu_hist) ho√†n th√†nh trong 241.06 gi√¢y.\n",
      "üíæ M√¥ h√¨nh XGBoost (gpu_hist) ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: 'dataset/working2/xgboost_gpu_hist_model.pkl'\n",
      "\n",
      "üîç B·∫Øt ƒë·∫ßu ƒë√°nh gi√° m√¥ h√¨nh: XGBoost (gpu_hist)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:44:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "d:\\Do_an_tot_nghiep\\apt-detection\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:44:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Th·ªùi gian d·ª± ƒëo√°n: 3.03 gi√¢y\n",
      "\n",
      "üèÜ ƒê·ªô ch√≠nh x√°c cu·ªëi c√πng c·ªßa XGBoost (gpu_hist): 97.65%\n",
      "\n",
      "üìä B√°o c√°o ph√¢n lo·∫°i (Classification Report) c·ªßa XGBoost (gpu_hist):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.97      1.00      0.99   1017456\n",
      "                     Bot       1.00      1.00      1.00     28907\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00     43480\n",
      "    DDOS attack-LOIC-UDP       1.00      1.00      1.00       346\n",
      "   DoS attacks-GoldenEye       1.00      1.00      1.00      8281\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     29040\n",
      "DoS attacks-SlowHTTPTest       0.20      0.18      0.19        11\n",
      "   DoS attacks-Slowloris       1.00      1.00      1.00      1982\n",
      "          FTP-BruteForce       0.18      0.20      0.19        10\n",
      "           Infilteration       0.59      0.04      0.08     27868\n",
      "          SSH-Bruteforce       1.00      1.00      1.00     18810\n",
      "        Brute Force -Web       0.58      0.68      0.63       111\n",
      "        Brute Force -XSS       0.91      0.91      0.91        46\n",
      "           SQL Injection       0.63      0.71      0.67        17\n",
      "\n",
      "                accuracy                           0.98   1176365\n",
      "               macro avg       0.79      0.77      0.76   1176365\n",
      "            weighted avg       0.97      0.98      0.97   1176365\n",
      "\n",
      "‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: 'dataset/working2/xgboost_(gpu_hist)_results.txt'\n",
      "üìà Ma tr·∫≠n nh·∫ßm l·∫´n ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: 'dataset/working2/xgboost_(gpu_hist)_confusion_matrix.png'\n",
      "\n",
      "üèÅüèÅüèÅ Qu√° tr√¨nh hu·∫•n luy·ªán v√† ƒë√°nh gi√° ho√†n t·∫•t trong 1497.93 gi√¢y! üèÅüèÅüèÅ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "import joblib \n",
    "\n",
    "# C√°c thu·∫≠t to√°n v√† c√¥ng c·ª• ƒë√°nh gi√° t·ª´ Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Th∆∞ vi·ªán XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Th∆∞ vi·ªán PyTorch (ch·ªâ d√πng ƒë·ªÉ t·∫£i label_mapping n·∫øu b·∫°n ƒë√£ l∆∞u b·∫±ng torch.save)\n",
    "import torch\n",
    "\n",
    "# --- H√†m tr·ª£ gi√∫p ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh ---\n",
    "def evaluate_model(model, X_test, y_test, class_names, model_name, results_dir=\"dataset/working2\"):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° m√¥ h√¨nh, in c√°c ch·ªâ s·ªë, l∆∞u k·∫øt qu·∫£ v√† v·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n.\n",
    "    (Gi·ªØ nguy√™n n·ªôi dung h√†m n√†y)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç B·∫Øt ƒë·∫ßu ƒë√°nh gi√° m√¥ h√¨nh: {model_name}...\")\n",
    "    start_eval_time = time.time()\n",
    "\n",
    "    # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm th·ª≠\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    eval_time = time.time() - start_eval_time\n",
    "    print(f\"   - Th·ªùi gian d·ª± ƒëo√°n: {eval_time:.2f} gi√¢y\")\n",
    "\n",
    "    # T√≠nh to√°n c√°c ch·ªâ s·ªë\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    try:\n",
    "        # S·ª≠ d·ª•ng zero_division=0 ƒë·ªÉ tr√°nh l·ªói khi m·ªôt l·ªõp kh√¥ng c√≥ m·∫´u n√†o ƒë∆∞·ª£c d·ª± ƒëo√°n ƒë√∫ng ho·∫∑c sai\n",
    "        report = classification_report(y_test, y_pred, target_names=class_names, zero_division=0)\n",
    "    except ValueError as e:\n",
    "        print(f\"‚ö†Ô∏è C·∫£nh b√°o khi t·∫°o classification report cho {model_name}: {e}\")\n",
    "        # C·ªë g·∫Øng t·∫°o b√°o c√°o kh√¥ng c√≥ t√™n l·ªõp n·∫øu c√≥ l·ªói\n",
    "        try:\n",
    "            report = classification_report(y_test, y_pred, zero_division=0)\n",
    "        except Exception as report_err:\n",
    "            print(f\"‚ùå Kh√¥ng th·ªÉ t·∫°o classification report: {report_err}\")\n",
    "            report = \"Kh√¥ng th·ªÉ t·∫°o b√°o c√°o.\"\n",
    "\n",
    "    print(f\"\\nüèÜ ƒê·ªô ch√≠nh x√°c cu·ªëi c√πng c·ªßa {model_name}: {accuracy:.2f}%\")\n",
    "    print(f\"\\nüìä B√°o c√°o ph√¢n lo·∫°i (Classification Report) c·ªßa {model_name}:\")\n",
    "    print(report)\n",
    "\n",
    "    # L∆∞u k·∫øt qu·∫£ v√†o t·ªáp tin\n",
    "    results_filename = f\"{results_dir}/{model_name.lower().replace(' ', '_')}_results.txt\"\n",
    "    try:\n",
    "        with open(results_filename, 'w', encoding='utf-8') as f: # Th√™m encoding='utf-8'\n",
    "            f.write(f\"{model_name} - Final Accuracy: {accuracy:.2f}%\\n\\n\")\n",
    "            f.write(\"Classification Report:\\n\")\n",
    "            f.write(report)\n",
    "        print(f\"‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: '{results_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi l∆∞u k·∫øt qu·∫£ cho {model_name}: {e}\")\n",
    "\n",
    "    # V·∫Ω v√† l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix)\n",
    "    cm_filename = f\"{results_dir}/{model_name.lower().replace(' ', '_')}_confusion_matrix.png\"\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 12)) # TƒÉng k√≠ch th∆∞·ªõc ƒë·ªÉ d·ªÖ ƒë·ªçc h∆°n\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 8}) # Gi·∫£m c·ª° ch·ªØ annot\n",
    "        plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix) - {model_name}', fontsize=14)\n",
    "        plt.ylabel('Nh√£n Th·∫≠t (True Label)', fontsize=12)\n",
    "        plt.xlabel('Nh√£n D·ª± ƒëo√°n (Predicted Label)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10) # Xoay v√† cƒÉn ch·ªânh nh√£n tr·ª•c X\n",
    "        plt.yticks(rotation=0, fontsize=10)            # CƒÉn ch·ªânh nh√£n tr·ª•c Y\n",
    "        plt.tight_layout() # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh b·ªë c·ª•c\n",
    "        plt.savefig(cm_filename, dpi=150) # TƒÉng dpi cho h√¨nh ·∫£nh ch·∫•t l∆∞·ª£ng h∆°n\n",
    "        print(f\"üìà Ma tr·∫≠n nh·∫ßm l·∫´n ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o t·ªáp: '{cm_filename}'\")\n",
    "        plt.close() # ƒê√≥ng bi·ªÉu ƒë·ªì ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi t·∫°o/l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n cho {model_name}: {e}\")\n",
    "\n",
    "\n",
    "# --- Ch∆∞∆°ng tr√¨nh ch√≠nh ---\n",
    "start_total_time = time.time()\n",
    "\n",
    "# Gi·∫£i ph√≥ng b·ªô nh·ªõ tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu\n",
    "gc.collect()\n",
    "\n",
    "# --- Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n ---\n",
    "data_dir = \"dataset/working2\"\n",
    "results_dir = data_dir\n",
    "train_file = f\"{data_dir}/train_balanced1.csv\"\n",
    "test_file = f\"{data_dir}/test1.csv\"\n",
    "label_map_file = f\"{data_dir}/label_mapping.pth\"\n",
    "\n",
    "# --- T·∫£i d·ªØ li·ªáu ---\n",
    "print(\"\\nüìÇ ƒêang t·∫£i d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω...\")\n",
    "try:\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán: {train_data.shape}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm th·ª≠: {test_data.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp d·ªØ li·ªáu. {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫£i d·ªØ li·ªáu: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- T·∫£i b·∫£n ƒë·ªì nh√£n (Label Mapping) ---\n",
    "print(\"\\nüè∑Ô∏è ƒêang t·∫£i b·∫£n ƒë·ªì nh√£n...\")\n",
    "try:\n",
    "    label_mapping = torch.load(label_map_file)\n",
    "    class_names = [k for k, v in sorted(label_mapping.items(), key=lambda item: item[1])]\n",
    "    num_classes = len(class_names)\n",
    "    print(f\"   - T·∫£i th√†nh c√¥ng b·∫£n ƒë·ªì nh√£n cho {num_classes} l·ªõp.\")\n",
    "    print(f\"   - C√°c l·ªõp: {class_names}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp b·∫£n ƒë·ªì nh√£n '{label_map_file}'.\")\n",
    "    max_label_train = train_data['Label'].max() if 'Label' in train_data else -1\n",
    "    max_label_test = test_data['Label'].max() if 'Label' in test_data else -1\n",
    "    max_label = int(max(max_label_train, max_label_test))\n",
    "    if max_label >= 0:\n",
    "        class_names = [f\"Lop_{i}\" for i in range(max_label + 1)]\n",
    "        num_classes = len(class_names)\n",
    "        print(f\"‚ö†Ô∏è ƒêang s·ª≠ d·ª•ng t√™n l·ªõp m·∫∑c ƒë·ªãnh: {class_names}\")\n",
    "    else:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ x√°c ƒë·ªãnh s·ªë l·ªõp t·ª´ d·ªØ li·ªáu.\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi t·∫£i b·∫£n ƒë·ªì nh√£n: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh ---\n",
    "print(\"\\n‚öôÔ∏è ƒêang chu·∫©n b·ªã d·ªØ li·ªáu (chuy·ªÉn ƒë·ªïi sang NumPy)...\")\n",
    "try:\n",
    "    X_train_np = train_data.drop(columns=[\"Label\"]).values\n",
    "    y_train_np = train_data[\"Label\"].values\n",
    "    X_test_np = test_data.drop(columns=[\"Label\"]).values\n",
    "    y_test_np = test_data[\"Label\"].values\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc X_train: {X_train_np.shape}, y_train: {y_train_np.shape}\")\n",
    "    print(f\"   - K√≠ch th∆∞·ªõc X_test: {X_test_np.shape}, y_test: {y_test_np.shape}\")\n",
    "except KeyError as e:\n",
    "    print(f\"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y c·ªôt 'Label' trong DataFrame. {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå L·ªói khi chu·∫©n b·ªã d·ªØ li·ªáu NumPy: {e}\")\n",
    "    exit()\n",
    "\n",
    "# X√°c minh ph·∫°m vi d·ªØ li·ªáu\n",
    "print(\"\\nüî• Ki·ªÉm tra ph·∫°m vi gi√° tr·ªã d·ªØ li·ªáu (sau khi t·∫£i):\")\n",
    "min_train, max_train = X_train_np.min(), X_train_np.max()\n",
    "min_test, max_test = X_test_np.min(), X_test_np.max()\n",
    "print(f\"   - Ph·∫°m vi ƒë·∫∑c tr∆∞ng Train: {min_train:.4f} ƒë·∫øn {max_train:.4f}\")\n",
    "print(f\"   - Ph·∫°m vi ƒë·∫∑c tr∆∞ng Test: {min_test:.4f} ƒë·∫øn {max_test:.4f}\")\n",
    "\n",
    "\n",
    "# Gi·∫£i ph√≥ng b·ªô nh·ªõ t·ª´ DataFrames kh√¥ng c√≤n d√πng\n",
    "del train_data, test_data\n",
    "gc.collect()\n",
    "print(\"   - ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ t·ª´ Pandas DataFrames.\")\n",
    "\n",
    "\n",
    "# --- 1. Hu·∫•n luy·ªán v√† ƒê√°nh gi√° Random Forest ---\n",
    "print(\"\\n\" + \"=\"*10 + \" üå≥ B·∫Øt ƒë·∫ßu v·ªõi Random Forest \" + \"=\"*10)\n",
    "\n",
    "# --- Kh·ªüi t·∫°o m√¥ h√¨nh Random Forest ---\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,        \n",
    "    criterion='gini',       \n",
    "    max_depth=30,          \n",
    "    min_samples_split=2,      \n",
    "    min_samples_leaf=1,      \n",
    "    max_features='sqrt',      \n",
    "    bootstrap=True,        \n",
    "    oob_score=False,        \n",
    "    random_state=42,         \n",
    "    n_jobs=-1,                \n",
    "    class_weight=None         \n",
    ")\n",
    "print(f\"   - Kh·ªüi t·∫°o Random Forest v·ªõi: n_estimators={rf_model.n_estimators}, max_depth={rf_model.max_depth}, max_features='{rf_model.max_features}'\")\n",
    "\n",
    "print(\"   - ƒêang hu·∫•n luy·ªán Random Forest...\")\n",
    "start_rf_train_time = time.time()\n",
    "try:\n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "    rf_model.fit(X_train_np, y_train_np)\n",
    "    rf_train_time = time.time() - start_rf_train_time\n",
    "    print(f\"‚úÖ Hu·∫•n luy·ªán Random Forest ho√†n th√†nh trong {rf_train_time:.2f} gi√¢y.\")\n",
    "\n",
    "    # L∆∞u m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
    "    rf_model_path = f\"{results_dir}/random_forest_model.pkl\"\n",
    "    joblib.dump(rf_model, rf_model_path)\n",
    "    print(f\"üíæ M√¥ h√¨nh Random Forest ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: '{rf_model_path}'\")\n",
    "\n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "    evaluate_model(rf_model, X_test_np, y_test_np, class_names, \"Random Forest\", results_dir=results_dir)\n",
    "\n",
    "except MemoryError:\n",
    "     print(f\"‚ùå L·ªói b·ªô nh·ªõ khi hu·∫•n luy·ªán/ƒë√°nh gi√° Random Forest. H√£y th·ª≠ gi·∫£m n_estimators ho·∫∑c gi·ªõi h·∫°n max_depth.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω Random Forest: {e}\")\n",
    "\n",
    "# Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "del rf_model\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# --- 2. Hu·∫•n luy·ªán v√† ƒê√°nh gi√° XGBoost ---\n",
    "print(\"\\n\" + \"=\"*10 + \" üöÄ B·∫Øt ƒë·∫ßu v·ªõi XGBoost (Th·ª≠ nghi·ªám GPU) \" + \"=\"*10)\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"   - Ph√°t hi·ªán GPU NVIDIA v√† CUDA.\")\n",
    "        tree_method_setting = 'gpu_hist' # S·ª≠ d·ª•ng GPU\n",
    "        print(f\"   - S·∫Ω s·ª≠ d·ª•ng 'tree_method': '{tree_method_setting}' cho XGBoost.\")\n",
    "    else:\n",
    "        print(\"   - Kh√¥ng ph√°t hi·ªán GPU NVIDIA ho·∫∑c CUDA.\")\n",
    "        tree_method_setting = 'hist' # S·ª≠ d·ª•ng CPU\n",
    "        print(f\"   - S·∫Ω s·ª≠ d·ª•ng 'tree_method': '{tree_method_setting}' (CPU) cho XGBoost.\")\n",
    "except ImportError:\n",
    "    print(\"   - Kh√¥ng t√¨m th·∫•y PyTorch ƒë·ªÉ ki·ªÉm tra CUDA, gi·∫£ ƒë·ªãnh s·ª≠ d·ª•ng CPU.\")\n",
    "    tree_method_setting = 'hist' # S·ª≠ d·ª•ng CPU m·∫∑c ƒë·ªãnh\n",
    "    print(f\"   - S·∫Ω s·ª≠ d·ª•ng 'tree_method': '{tree_method_setting}' (CPU) cho XGBoost.\")\n",
    "\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=num_classes,\n",
    "    n_estimators=200,         \n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,               \n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    n_jobs=-1,\n",
    "    tree_method=tree_method_setting\n",
    ")\n",
    "\n",
    "print(f\"   - ƒêang hu·∫•n luy·ªán XGBoost s·ª≠ d·ª•ng {tree_method_setting}...\")\n",
    "start_xgb_train_time = time.time()\n",
    "try:\n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "    xgb_model.fit(X_train_np, y_train_np, verbose=False)\n",
    "    xgb_train_time = time.time() - start_xgb_train_time\n",
    "    print(f\"‚úÖ Hu·∫•n luy·ªán XGBoost ({tree_method_setting}) ho√†n th√†nh trong {xgb_train_time:.2f} gi√¢y.\")\n",
    "\n",
    "    # L∆∞u m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
    "    xgb_model_path = f\"{results_dir}/xgboost_{tree_method_setting}_model.pkl\"\n",
    "    joblib.dump(xgb_model, xgb_model_path)\n",
    "    print(f\"üíæ M√¥ h√¨nh XGBoost ({tree_method_setting}) ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: '{xgb_model_path}'\")\n",
    "\n",
    "    # ƒê√°nh gi√° m√¥ h√¨nh\n",
    "    evaluate_model(xgb_model, X_test_np, y_test_np, class_names, f\"XGBoost ({tree_method_setting})\", results_dir=results_dir)\n",
    "\n",
    "except xgb.core.XGBoostError as e:\n",
    "    print(f\"‚ùå L·ªói XGBoost: {e}\")\n",
    "    if \"CUDA\" in str(e) or \"GPU\" in str(e):\n",
    "        print(\"   - L·ªói c√≥ th·ªÉ li√™n quan ƒë·∫øn c√†i ƒë·∫∑t CUDA ho·∫∑c GPU.\")\n",
    "    print(\"   - Th·ª≠ ch·∫°y l·∫°i v·ªõi tree_method='hist' (CPU) n·∫øu l·ªói li√™n quan ƒë·∫øn GPU.\")\n",
    "except MemoryError:\n",
    "     print(f\"‚ùå L·ªói b·ªô nh·ªõ khi hu·∫•n luy·ªán/ƒë√°nh gi√° XGBoost ({tree_method_setting}).\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω XGBoost ({tree_method_setting}): {e}\")\n",
    "\n",
    "\n",
    "# --- Ho√†n t·∫•t ---\n",
    "del X_train_np, y_train_np, X_test_np, y_test_np\n",
    "del rf_model\n",
    "del xgb_model\n",
    "gc.collect()\n",
    "\n",
    "end_total_time = time.time()\n",
    "total_script_time = end_total_time - start_total_time\n",
    "print(f\"\\nüèÅüèÅüèÅ Qu√° tr√¨nh hu·∫•n luy·ªán v√† ƒë√°nh gi√° ho√†n t·∫•t trong {total_script_time:.2f} gi√¢y! üèÅüèÅüèÅ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
