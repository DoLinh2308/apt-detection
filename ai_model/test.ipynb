{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da977a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib \n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder # Example scalers/encoders\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683147fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODELS = 'D:/Do_an_tot_nghiep/apt-detection/ai_model/dataset/working2/'\n",
    "RF_MODEL_FILE = 'random_forest_model.pkl'\n",
    "XGB_MODEL_FILE = 'xgboost_model.pkl'\n",
    "PATH_TO_FILES = 'D:/Do_an_tot_nghiep/apt-detection/ai_model/dataset/DAPT-2020/'\n",
    "DAPT2020_DATA_FILE = 'merged_cleaned.csv' \n",
    "SCALER_FILE = 'scaler.pkl'\n",
    "\n",
    "LABEL_COLUMN = 'Stage'\n",
    "\n",
    "APT_LABELS_IN_DAPT2020 = ['Lateral Movement', 'Reconnaissance', 'Establish Foothold', 'Data Exfiltration']\n",
    "BENIGN_LABELS_IN_DAPT2020 = ['Benign', 'BENIGN']\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    'Dst Port',\n",
    "    'Flow Duration',\n",
    "    'Tot Fwd Pkts',\n",
    "    'Tot Bwd Pkts',\n",
    "    'TotLen Fwd Pkts',\n",
    "    'TotLen Bwd Pkts',\n",
    "    'Fwd Pkt Len Max',\n",
    "    'Fwd Pkt Len Min',\n",
    "    'Fwd Pkt Len Mean',\n",
    "    'Fwd Pkt Len Std',\n",
    "    'Bwd Pkt Len Max',\n",
    "    'Bwd Pkt Len Min',\n",
    "    'Bwd Pkt Len Mean',\n",
    "    'Bwd Pkt Len Std',\n",
    "    'Flow Byts/s',\n",
    "    'Flow Pkts/s',\n",
    "    'Flow IAT Mean',\n",
    "    'Flow IAT Std',\n",
    "    'Flow IAT Max',\n",
    "    'Flow IAT Min',\n",
    "    'Fwd IAT Tot',\n",
    "    'Fwd IAT Mean',\n",
    "    'Fwd IAT Std',\n",
    "    'Fwd IAT Max',\n",
    "    'Fwd IAT Min',\n",
    "    'Bwd IAT Tot',\n",
    "    'Bwd IAT Mean',\n",
    "    'Bwd IAT Std',\n",
    "    'Bwd IAT Max',\n",
    "    'Bwd IAT Min',\n",
    "    'Fwd PSH Flags',\n",
    "    'Fwd URG Flags',\n",
    "    'Fwd Header Len',\n",
    "    'Bwd Header Len',\n",
    "    'Fwd Pkts/s',\n",
    "    'Bwd Pkts/s',\n",
    "    'Pkt Len Min',\n",
    "    'Pkt Len Max',\n",
    "    'Pkt Len Mean',\n",
    "    'Pkt Len Std',\n",
    "    'Pkt Len Var',\n",
    "    'FIN Flag Cnt',\n",
    "    'SYN Flag Cnt',\n",
    "    'RST Flag Cnt',\n",
    "    'PSH Flag Cnt',\n",
    "    'ACK Flag Cnt',\n",
    "    'URG Flag Cnt',\n",
    "    'ECE Flag Cnt',\n",
    "    'CWE Flag Count',\n",
    "    'Down/Up Ratio',\n",
    "    'Pkt Size Avg',\n",
    "    'Fwd Seg Size Avg',\n",
    "    'Bwd Seg Size Avg',\n",
    "    'Subflow Fwd Pkts',\n",
    "    'Subflow Fwd Byts',\n",
    "    'Subflow Bwd Pkts',\n",
    "    'Subflow Bwd Byts',\n",
    "    'Init Fwd Win Byts',\n",
    "    'Init Bwd Win Byts',\n",
    "    'Fwd Act Data Pkts',\n",
    "    'Fwd Seg Size Min',\n",
    "    'Active Mean',\n",
    "    'Active Std',\n",
    "    'Active Max',\n",
    "    'Active Min',\n",
    "    'Idle Mean',\n",
    "    'Idle Std',\n",
    "    'Idle Max',\n",
    "    'Idle Min',\n",
    "    'Protocol_0',\n",
    "    'Protocol_6',\n",
    "    'Protocol_17',\n",
    "]\n",
    "\n",
    "NUMERICAL_FEATURES_TO_SCALE = [\n",
    "    'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts',\n",
    "    'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min',\n",
    "    'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max',\n",
    "    'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std',\n",
    "    'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "    'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean',\n",
    "    'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot',\n",
    "    'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
    "    'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',\n",
    "    'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std',\n",
    "    'Pkt Len Var', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg',\n",
    "    'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts',\n",
    "    'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts',\n",
    "    'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min',\n",
    "    'Active Mean', 'Active Std', 'Active Max', 'Active Min',\n",
    "    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
    "]\n",
    "\n",
    "FEATURES_NOT_SCALED_BUT_NUMERICAL = [\n",
    "    'Dst Port', # Cổng đích - có thể coi là số hoặc phân loại tùy cách dùng\n",
    "    'Fwd PSH Flags', 'Fwd URG Flags', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
    "    'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
    "    'CWE Flag Count', 'ECE Flag Cnt',\n",
    "    'Protocol_0', 'Protocol_6', 'Protocol_17' # Kết quả của OHE Protocol\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9dda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing models and scaler...\n",
      "Models and scaler loaded successfully.\n",
      "Loading DAPT2020 data from D:/Do_an_tot_nghiep/apt-detection/ai_model/dataset/DAPT-2020/merged_cleaned.csv...\n",
      "DAPT2020 data loaded. Shape: (165154, 85)\n",
      "                                  Flow ID        Src IP  Src Port  \\\n",
      "0                   8.0.6.4-8.6.0.1-0-0-0       8.6.0.1         0   \n",
      "1  192.168.3.10-239.2.11.71-53569-8662-17  192.168.3.10     53569   \n",
      "2        255.255.255.255-0.0.0.0-67-68-17       0.0.0.0        68   \n",
      "3  192.168.3.30-192.168.3.31-40504-9200-6  192.168.3.30     40504   \n",
      "4              0.87.248.248-3.0.0.0-0-0-0  0.87.248.248         0   \n",
      "\n",
      "            Dst IP  Dst Port  Protocol               Timestamp  Flow Duration  \\\n",
      "0          8.0.6.4         0         0  15/07/2019 01:55:21 PM      119998944   \n",
      "1      239.2.11.71      8662        17  15/07/2019 01:55:22 PM      109235816   \n",
      "2  255.255.255.255        67        17  15/07/2019 01:55:22 PM      119764062   \n",
      "3     192.168.3.31      9200         6  15/07/2019 01:55:23 PM      117030424   \n",
      "4          3.0.0.0         0         0  15/07/2019 01:55:58 PM      119999703   \n",
      "\n",
      "   Tot Fwd Pkts  Tot Bwd Pkts  ...  Active Mean    Active Std  Active Max  \\\n",
      "0           242             1  ...       0.0000       0.00000           0   \n",
      "1            21             1  ...  819535.5000   78517.84409      875056   \n",
      "2            88             1  ...       0.0000       0.00000           0   \n",
      "3            18            17  ...  192329.3333  436593.12330     1083374   \n",
      "4             2             1  ...       0.0000       0.00000           0   \n",
      "\n",
      "   Active Min    Idle Mean      Idle Std  Idle Max  Idle Min  Activity   Stage  \n",
      "0           0         0.00  0.000000e+00         0         0    Normal  Benign  \n",
      "1      764015  15175318.57  6.349189e+06  20019201   5202524    Normal  Benign  \n",
      "2           0         0.00  0.000000e+00         0         0    Normal  Benign  \n",
      "3        7236  19311305.17  1.177830e+07  34978598   5147962    Normal  Benign  \n",
      "4           0  59999851.50  8.478210e+02  60000451  59999252    Normal  Benign  \n",
      "\n",
      "[5 rows x 85 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 165154 entries, 0 to 165153\n",
      "Data columns (total 85 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   Flow ID              165154 non-null  object \n",
      " 1   Src IP               165154 non-null  object \n",
      " 2   Src Port             165154 non-null  int64  \n",
      " 3   Dst IP               165154 non-null  object \n",
      " 4   Dst Port             165154 non-null  int64  \n",
      " 5   Protocol             165154 non-null  int64  \n",
      " 6   Timestamp            165154 non-null  object \n",
      " 7   Flow Duration        165154 non-null  int64  \n",
      " 8   Tot Fwd Pkts         165154 non-null  int64  \n",
      " 9   Tot Bwd Pkts         165154 non-null  int64  \n",
      " 10  TotLen Fwd Pkts      165154 non-null  int64  \n",
      " 11  TotLen Bwd Pkts      165154 non-null  int64  \n",
      " 12  Fwd Pkt Len Max      165154 non-null  int64  \n",
      " 13  Fwd Pkt Len Min      165154 non-null  int64  \n",
      " 14  Fwd Pkt Len Mean     165154 non-null  float64\n",
      " 15  Fwd Pkt Len Std      165154 non-null  float64\n",
      " 16  Bwd Pkt Len Max      165154 non-null  int64  \n",
      " 17  Bwd Pkt Len Min      165154 non-null  int64  \n",
      " 18  Bwd Pkt Len Mean     165154 non-null  float64\n",
      " 19  Bwd Pkt Len Std      165154 non-null  float64\n",
      " 20  Flow Byts/s          165154 non-null  float64\n",
      " 21  Flow Pkts/s          165154 non-null  float64\n",
      " 22  Flow IAT Mean        165154 non-null  float64\n",
      " 23  Flow IAT Std         165154 non-null  float64\n",
      " 24  Flow IAT Max         165154 non-null  int64  \n",
      " 25  Flow IAT Min         165154 non-null  int64  \n",
      " 26  Fwd IAT Tot          165154 non-null  int64  \n",
      " 27  Fwd IAT Mean         165154 non-null  float64\n",
      " 28  Fwd IAT Std          165154 non-null  float64\n",
      " 29  Fwd IAT Max          165154 non-null  int64  \n",
      " 30  Fwd IAT Min          165154 non-null  int64  \n",
      " 31  Bwd IAT Tot          165154 non-null  int64  \n",
      " 32  Bwd IAT Mean         165154 non-null  float64\n",
      " 33  Bwd IAT Std          165154 non-null  float64\n",
      " 34  Bwd IAT Max          165154 non-null  int64  \n",
      " 35  Bwd IAT Min          165154 non-null  int64  \n",
      " 36  Fwd PSH Flags        165154 non-null  int64  \n",
      " 37  Bwd PSH Flags        165154 non-null  int64  \n",
      " 38  Fwd URG Flags        165154 non-null  int64  \n",
      " 39  Bwd URG Flags        165154 non-null  int64  \n",
      " 40  Fwd Header Len       165154 non-null  int64  \n",
      " 41  Bwd Header Len       165154 non-null  int64  \n",
      " 42  Fwd Pkts/s           165154 non-null  float64\n",
      " 43  Bwd Pkts/s           165154 non-null  float64\n",
      " 44  Pkt Len Min          165154 non-null  int64  \n",
      " 45  Pkt Len Max          165154 non-null  int64  \n",
      " 46  Pkt Len Mean         165154 non-null  float64\n",
      " 47  Pkt Len Std          165154 non-null  float64\n",
      " 48  Pkt Len Var          165154 non-null  float64\n",
      " 49  FIN Flag Cnt         165154 non-null  int64  \n",
      " 50  SYN Flag Cnt         165154 non-null  int64  \n",
      " 51  RST Flag Cnt         165154 non-null  int64  \n",
      " 52  PSH Flag Cnt         165154 non-null  int64  \n",
      " 53  ACK Flag Cnt         165154 non-null  int64  \n",
      " 54  URG Flag Cnt         165154 non-null  int64  \n",
      " 55  CWR Flag Cnt         165154 non-null  int64  \n",
      " 56  ECE Flag Cnt         165154 non-null  int64  \n",
      " 57  Down/Up Ratio        165154 non-null  int64  \n",
      " 58  Pkt Size Avg         165154 non-null  float64\n",
      " 59  Fwd Seg Size Avg     165154 non-null  float64\n",
      " 60  Bwd Seg Size Avg     165154 non-null  float64\n",
      " 61  Fwd Bytes/Bulk Avg   165154 non-null  int64  \n",
      " 62  Fwd Packet/Bulk Avg  165154 non-null  int64  \n",
      " 63  Fwd Bulk Rate Avg    165154 non-null  int64  \n",
      " 64  Bwd Bytes/Bulk Avg   165154 non-null  int64  \n",
      " 65  Bwd Packet/Bulk Avg  165154 non-null  int64  \n",
      " 66  Bwd Bulk Rate Avg    165154 non-null  int64  \n",
      " 67  Subflow Fwd Pkts     165154 non-null  int64  \n",
      " 68  Subflow Fwd Byts     165154 non-null  int64  \n",
      " 69  Subflow Bwd Pkts     165154 non-null  int64  \n",
      " 70  Subflow Bwd Byts     165154 non-null  int64  \n",
      " 71  Init Fwd Win Byts    165154 non-null  int64  \n",
      " 72  Init Bwd Win Byts    165154 non-null  int64  \n",
      " 73  Fwd Act Data Pkts    165154 non-null  int64  \n",
      " 74  Fwd Seg Size Min     165154 non-null  int64  \n",
      " 75  Active Mean          165154 non-null  float64\n",
      " 76  Active Std           165154 non-null  float64\n",
      " 77  Active Max           165154 non-null  int64  \n",
      " 78  Active Min           165154 non-null  int64  \n",
      " 79  Idle Mean            165154 non-null  float64\n",
      " 80  Idle Std             165154 non-null  float64\n",
      " 81  Idle Max             165154 non-null  int64  \n",
      " 82  Idle Min             165154 non-null  int64  \n",
      " 83  Activity             165154 non-null  object \n",
      " 84  Stage                165154 non-null  object \n",
      "dtypes: float64(24), int64(55), object(6)\n",
      "memory usage: 107.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "# --- Load Mô hình và Scaler đã huấn luyện trên CIC-IDS2018 ---\n",
    "print(\"Loading existing models and scaler...\")\n",
    "try:\n",
    "    rf_model_cicids = joblib.load(os.path.join(PATH_TO_MODELS, RF_MODEL_FILE))\n",
    "    xgb_model_cicids = joblib.load(os.path.join(PATH_TO_MODELS, XGB_MODEL_FILE))\n",
    "    data_scaler = joblib.load(os.path.join(PATH_TO_MODELS, SCALER_FILE)) # Load your scaler here\n",
    "    print(\"Models and scaler loaded successfully.\")\n",
    "    # Optional: print some info about loaded models/scaler\n",
    "    print(\"RF Model Params:\", rf_model_cicids.get_params())\n",
    "    print(\"XGB Model Params:\", xgb_model_cicids.get_params())\n",
    "    print(\"Scaler object type:\", type(data_scaler)) # Should be StandardScaler, MinMaxScaler, ColumnTransformer, etc.\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading required files: {e}\")\n",
    "    print(\"Please check the file paths and names for models and scaler.\")\n",
    "    exit() # Exit if files can't be loaded\n",
    "except Exception as e:\n",
    "     print(f\"Error loading .pkl files: {e}\")\n",
    "     exit()\n",
    "\n",
    "\n",
    "# --- Load Dữ liệu DAPT2020 ---\n",
    "print(f\"Loading DAPT2020 data from {os.path.join(PATH_TO_FILES, DAPT2020_DATA_FILE)}...\")\n",
    "try:\n",
    "    # TODO: Kiểm tra encoding của file CSV DAPT2020 nếu gặp lỗi đọc file\n",
    "    df_dapt2020 = pd.read_csv(os.path.join(PATH_TO_FILES, DAPT2020_DATA_FILE))\n",
    "    print(f\"DAPT2020 data loaded. Shape: {df_dapt2020.shape}\")\n",
    "    # Optional: Display first few rows and info\n",
    "    print(df_dapt2020.head())\n",
    "    print(df_dapt2020.info())\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading DAPT2020 data file: {e}\")\n",
    "    print(\"Please check the file path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "     print(f\"Error reading DAPT2020 data file: {e}\")\n",
    "     exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5720efe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing DAPT2020 data (handling NaNs and infs)...\n",
      "NaNs and infs handled.\n"
     ]
    }
   ],
   "source": [
    "# 1. Xử lý các giá trị không xác định/thiếu và vô hạn trong DAPT2020\n",
    "# TODO: ÁP DỤNG CÁCH XỬ LÝ GIÁ TRỊ THIẾU NHẤT QUÁN VỚI CIC-IDS2018\n",
    "print(\"Preprocessing DAPT2020 data (handling NaNs and infs)...\")\n",
    "# Example: fill NaNs with 0 (adjust if needed)\n",
    "df_dapt2020.fillna(0, inplace=True)\n",
    "# Handle potential infinite values\n",
    "df_dapt2020.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_dapt2020.fillna(0, inplace=True) # Fill NaNs created by replacing inf\n",
    "print(\"NaNs and infs handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping DAPT2020 labels to Benign/APT...\n",
      "Label mapping done. Distribution: Counter({0: 123824, 1: 41330})\n",
      "\n",
      "Checking for missing feature columns in DAPT2020...\n",
      "Missing original feature columns found: ['CWE Flag Count', 'Protocol_0', 'Protocol_6', 'Protocol_17']\n",
      "Adding missing columns to DAPT2020 and filling with 0...\n",
      "Missing columns added.\n",
      "\n",
      "Separating features (X) and labels (y)...\n",
      "Features shape (X_dapt): (165154, 72)\n",
      "Labels shape (y_dapt): (165154,)\n",
      "\n",
      "--- Pre-scaling Data Cleaning for X_dapt ---\n",
      "Attempting data type conversion (to numeric where possible)...\n",
      "No object columns found in features.\n",
      "Checking for and replacing Infinity values...\n",
      "No Infinity values found.\n",
      "Checking for and filling NaN values...\n",
      "No NaN values found.\n",
      "\n",
      "Final check before scaling:\n",
      "X_dapt shape: (165154, 72)\n",
      "NaN count: 0\n",
      "Infinity count: 0\n",
      "All feature columns in X_dapt are now numeric.\n",
      "--- End Pre-scaling Data Cleaning ---\n",
      "\n",
      "--- Comparing Feature Lists ---\n",
      "Actual features in X_dapt (72):\n",
      "['Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s'] ...\n",
      "\n",
      "Scaler EXPECTS features via 'feature_names_in_' (72):\n",
      "['ACK Flag Cnt', 'Active Max', 'Active Mean', 'Active Min', 'Active Std', 'Bwd Header Len', 'Bwd IAT Max', 'Bwd IAT Mean', 'Bwd IAT Min', 'Bwd IAT Std', 'Bwd IAT Tot', 'Bwd Pkt Len Max', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Min', 'Bwd Pkt Len Std'] ...\n",
      "\n",
      "[ERROR] Feature names and/or order MISMATCH!\n",
      "  - First mismatch at index 0: Actual='Dst Port', Expected='ACK Flag Cnt'\n",
      "--- End Comparing Feature Lists ---\n",
      "\n",
      "Applying loaded scaler to cleaned DAPT2020 features...\n",
      "\n",
      "FATAL ERROR during scaler.transform: The feature names should match those that were passed during fit.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "This 'ValueError' often relates to incompatible values (e.g., NaN/Inf not handled) or incorrect number of features.\n",
      "Recommendations:\n",
      "  - Review the NaN/Inf counts printed just before scaling.\n",
      "  - Ensure X_dapt has the exact number of columns the scaler expects.\n",
      "\n",
      "--- Traceback ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\linhv\\AppData\\Local\\Temp\\ipykernel_13928\\3205208479.py\", line 240, in <module>\n",
      "    X_dapt_processed = data_scaler.transform(X_dapt)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 532, in transform\n",
      "    X = validate_data(\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2919, in validate_data\n",
      "    _check_feature_names(_estimator, X, reset=reset)\n",
      "  File \"d:\\Do_an_tot_nghiep\\apt-detection\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2777, in _check_feature_names\n",
      "    raise ValueError(message)\n",
      "ValueError: The feature names should match those that were passed during fit.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "\n",
      "--- End Traceback ---\n",
      "\n",
      "Skipping train_test_split because data scaling failed or X_dapt_processed was not created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Ánh xạ nhãn gốc sang nhãn Benign (0) và APT (1)\n",
    "print(\"\\nMapping DAPT2020 labels to Benign/APT...\")\n",
    "if LABEL_COLUMN not in df_dapt2020.columns:\n",
    "    print(f\"Error: Label column '{LABEL_COLUMN}' not found in DAPT2020 data.\")\n",
    "    print(\"Please check the LABEL_COLUMN definition.\")\n",
    "    exit()\n",
    "\n",
    "df_dapt2020['APT_Label'] = 0 # Default to Benign (0)\n",
    "df_dapt2020.loc[df_dapt2020[LABEL_COLUMN].isin(APT_LABELS_IN_DAPT2020), 'APT_Label'] = 1 # Mark APT (1)\n",
    "print(f\"Label mapping done. Distribution: {Counter(df_dapt2020['APT_Label'])}\")\n",
    "\n",
    "\n",
    "# --- Thêm các cột đặc trưng bị thiếu vào DAPT2020 và điền giá trị 0 ---\n",
    "print(\"\\nChecking for missing feature columns in DAPT2020...\")\n",
    "missing_feature_columns = [col for col in FEATURE_COLUMNS if col not in df_dapt2020.columns]\n",
    "\n",
    "if missing_feature_columns:\n",
    "    print(f\"Missing original feature columns found: {missing_feature_columns}\")\n",
    "    print(\"Adding missing columns to DAPT2020 and filling with 0...\")\n",
    "    for col in missing_feature_columns:\n",
    "        df_dapt2020[col] = 0 # Add the column and fill with 0\n",
    "    print(\"Missing columns added.\")\n",
    "else:\n",
    "    print(\"All original feature columns are present in DAPT2020.\")\n",
    "\n",
    "# --- Sắp xếp lại các cột của DAPT2020 theo thứ tự của FEATURE_COLUMNS ---\n",
    "print(\"\\nReordering DAPT2020 columns to match original feature order...\")\n",
    "try:\n",
    "    # Lấy danh sách các cột không phải là đặc trưng và không phải là nhãn mới tạo\n",
    "    other_cols = [col for col in df_dapt2020.columns if col not in FEATURE_COLUMNS and col != 'APT_Label']\n",
    "    # Sắp xếp lại: đặc trưng trước, nhãn mới, rồi đến các cột khác\n",
    "    df_dapt2020 = df_dapt2020[FEATURE_COLUMNS + ['APT_Label'] + other_cols].copy()\n",
    "    print(f\"Columns reordered. New shape: {df_dapt2020.shape}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error reordering columns: {e}. Ensure all columns in FEATURE_COLUMNS exist after adding missing ones.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during column reordering: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Tách đặc trưng (X) và nhãn (y) ---\n",
    "print(\"\\nSeparating features (X) and labels (y)...\")\n",
    "try:\n",
    "    # Chọn các cột đặc trưng THEO ĐÚNG THỨ TỰ trong FEATURE_COLUMNS\n",
    "    X_dapt = df_dapt2020[FEATURE_COLUMNS].copy()\n",
    "    y_dapt = df_dapt2020['APT_Label'].copy()\n",
    "    print(f\"Features shape (X_dapt): {X_dapt.shape}\")\n",
    "    print(f\"Labels shape (y_dapt): {y_dapt.shape}\")\n",
    "    # Kiểm tra xem số cột của X_dapt có khớp với FEATURE_COLUMNS không\n",
    "    if X_dapt.shape[1] != len(FEATURE_COLUMNS):\n",
    "         print(f\"WARNING: X_dapt has {X_dapt.shape[1]} columns, but FEATURE_COLUMNS has {len(FEATURE_COLUMNS)} items.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error selecting columns for X_dapt or y_dapt: {e}\")\n",
    "    print(\"Please ensure FEATURE_COLUMNS and 'APT_Label' exist after reordering.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during X/y separation: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- TIỀN XỬ LÝ VÀ LÀM SẠCH X_dapt TRƯỚC KHI SCALING ---\n",
    "print(\"\\n--- Pre-scaling Data Cleaning for X_dapt ---\")\n",
    "\n",
    "# 1. Chuyển đổi kiểu dữ liệu và xử lý lỗi tiềm ẩn\n",
    "print(\"Attempting data type conversion (to numeric where possible)...\")\n",
    "potential_object_cols = X_dapt.select_dtypes(include=['object']).columns.tolist()\n",
    "if potential_object_cols:\n",
    "    print(f\"Columns with object dtype found: {potential_object_cols}. Attempting pd.to_numeric...\")\n",
    "    for col in potential_object_cols:\n",
    "        original_dtype = X_dapt[col].dtype\n",
    "        try:\n",
    "            # Cố gắng chuyển đổi, lỗi sẽ thành NaN\n",
    "            X_dapt[col] = pd.to_numeric(X_dapt[col], errors='coerce')\n",
    "            if X_dapt[col].isnull().any():\n",
    "                print(f\"  - Column '{col}': Conversion done, some values became NaN.\")\n",
    "            # else:\n",
    "            #     print(f\"  - Column '{col}': Conversion successful.\")\n",
    "        except Exception as conv_err:\n",
    "            print(f\"  - Error converting column '{col}' (original dtype {original_dtype}): {conv_err}\")\n",
    "            # Quyết định xem có nên dừng lại nếu chuyển đổi thất bại không\n",
    "            # exit()\n",
    "else:\n",
    "    print(\"No object columns found in features.\")\n",
    "\n",
    "# 2. Xử lý giá trị Vô cực (Infinity)\n",
    "print(\"Checking for and replacing Infinity values...\")\n",
    "numeric_cols_check = X_dapt.select_dtypes(include=np.number)\n",
    "inf_check = np.isinf(numeric_cols_check).any().any() # Kiểm tra nhanh hơn\n",
    "if inf_check:\n",
    "    print(\"Infinity values found. Replacing with NaN...\")\n",
    "    # Chỉ thay thế trên các cột số để tránh lỗi dtype\n",
    "    for col in numeric_cols_check.columns:\n",
    "         # Kiểm tra từng cột để thay thế hiệu quả hơn\n",
    "         if np.isinf(X_dapt[col]).any():\n",
    "              X_dapt[col] = X_dapt[col].replace([np.inf, -np.inf], np.nan)\n",
    "    print(\"Infinity replacement done.\")\n",
    "else:\n",
    "    print(\"No Infinity values found.\")\n",
    "\n",
    "# 3. Xử lý giá trị Thiếu (NaN) - Sau khi chuyển đổi và thay thế Inf\n",
    "print(\"Checking for and filling NaN values...\")\n",
    "nan_check = X_dapt.isnull().any().any()\n",
    "if nan_check:\n",
    "    nan_counts = X_dapt.isnull().sum()\n",
    "    print(f\"NaN values found. Total before fill: {nan_counts.sum()}\")\n",
    "    # print(\"NaN counts per column:\\n\", nan_counts[nan_counts > 0]) # In chi tiết nếu cần\n",
    "    print(\"Filling NaN values with 0...\") # Hoặc chiến lược khác: mean(), median()\n",
    "    X_dapt = X_dapt.fillna(0)\n",
    "    print(\"NaN filling done.\")\n",
    "else:\n",
    "    print(\"No NaN values found.\")\n",
    "\n",
    "# 4. Kiểm tra cuối cùng trước khi scale\n",
    "print(\"\\nFinal check before scaling:\")\n",
    "print(f\"X_dapt shape: {X_dapt.shape}\")\n",
    "final_nan_count = X_dapt.isnull().sum().sum()\n",
    "final_inf_count = np.isinf(X_dapt.select_dtypes(include=np.number).to_numpy()).sum()\n",
    "print(f\"NaN count: {final_nan_count}\")\n",
    "print(f\"Infinity count: {final_inf_count}\")\n",
    "\n",
    "# >>> KIỂM TRA QUAN TRỌNG NHẤT: Đảm bảo tất cả cột đặc trưng là số <<<\n",
    "non_numeric_final = X_dapt.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric_final:\n",
    "    print(f\"\\nCRITICAL WARNING: Non-numeric columns still present in X_dapt after cleaning: {non_numeric_final}\")\n",
    "    print(\"These columns likely cause the scaler to fail. Please investigate!\")\n",
    "    # Xem xét việc dừng lại ở đây nếu scaler chắc chắn yêu cầu đầu vào số\n",
    "    # exit()\n",
    "else:\n",
    "    print(\"All feature columns in X_dapt are now numeric.\")\n",
    "\n",
    "print(\"--- End Pre-scaling Data Cleaning ---\")\n",
    "\n",
    "# --- SO SÁNH FEATURE LISTS ---\n",
    "print(\"\\n--- Comparing Feature Lists ---\")\n",
    "# 1. Lấy danh sách features thực tế từ X_dapt\n",
    "actual_features = X_dapt.columns.tolist()\n",
    "print(f\"Actual features in X_dapt ({len(actual_features)}):\")\n",
    "# In ra một phần để dễ nhìn\n",
    "print(actual_features[:15], \"...\" if len(actual_features) > 15 else \"\")\n",
    "\n",
    "# 2. Cố gắng lấy danh sách features mong đợi từ scaler\n",
    "expected_features = None\n",
    "expected_feature_count = None\n",
    "\n",
    "if hasattr(data_scaler, 'feature_names_in_'):\n",
    "    try:\n",
    "        expected_features = list(data_scaler.feature_names_in_)\n",
    "        expected_feature_count = len(expected_features)\n",
    "        print(f\"\\nScaler EXPECTS features via 'feature_names_in_' ({expected_feature_count}):\")\n",
    "        print(expected_features[:15], \"...\" if len(expected_features) > 15 else \"\")\n",
    "\n",
    "        # So sánh trực tiếp\n",
    "        if actual_features == expected_features:\n",
    "            print(\"\\n[SUCCESS] Feature names and order MATCH perfectly!\")\n",
    "        else:\n",
    "            print(\"\\n[ERROR] Feature names and/or order MISMATCH!\")\n",
    "            if len(actual_features) != expected_feature_count:\n",
    "                print(f\"  - Length mismatch: Actual={len(actual_features)}, Expected={expected_feature_count}\")\n",
    "            # Tìm điểm khác biệt đầu tiên (ví dụ)\n",
    "            for i, (actual, expected) in enumerate(zip(actual_features, expected_features)):\n",
    "                if actual != expected:\n",
    "                    print(f\"  - First mismatch at index {i}: Actual='{actual}', Expected='{expected}'\")\n",
    "                    break\n",
    "            else: # Nếu vòng lặp hoàn thành mà không break (chỉ xảy ra nếu độ dài khác nhau)\n",
    "                if len(actual_features) > expected_feature_count:\n",
    "                     print(f\"  - Actual list has extra features starting from index {expected_feature_count}: '{actual_features[expected_feature_count]}'\")\n",
    "                elif len(actual_features) < expected_feature_count:\n",
    "                     print(f\"  - Expected list has extra features starting from index {len(actual_features)}: '{expected_features[len(actual_features)]}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not retrieve or process 'feature_names_in_': {e}\")\n",
    "        # Thử lấy số lượng features nếu có\n",
    "        if hasattr(data_scaler, 'n_features_in_'):\n",
    "            try:\n",
    "                 expected_feature_count = data_scaler.n_features_in_\n",
    "                 print(f\"Scaler has 'n_features_in_': {expected_feature_count}\")\n",
    "                 if len(actual_features) == expected_feature_count:\n",
    "                      print(f\"[INFO] Number of features MATCHES ({expected_feature_count}), but names/order could not be verified via 'feature_names_in_'.\")\n",
    "                 else:\n",
    "                      print(f\"[ERROR] Number of features MISMATCH: Actual={len(actual_features)}, Expected={expected_feature_count}\")\n",
    "            except Exception as e2:\n",
    "                 print(f\"Could not retrieve 'n_features_in_': {e2}\")\n",
    "        else:\n",
    "            print(\"Scaler does not have 'n_features_in_' attribute either.\")\n",
    "\n",
    "elif hasattr(data_scaler, 'n_features_in_'):\n",
    "     try:\n",
    "        expected_feature_count = data_scaler.n_features_in_\n",
    "        print(f\"\\nScaler only has 'n_features_in_': {expected_feature_count}\")\n",
    "        if len(actual_features) == expected_feature_count:\n",
    "             print(f\"[INFO] Number of features MATCHES ({expected_feature_count}), but names/order cannot be verified automatically.\")\n",
    "             print(\"       Manual check needed based on training code or ColumnTransformer definition.\")\n",
    "        else:\n",
    "             print(f\"[ERROR] Number of features MISMATCH: Actual={len(actual_features)}, Expected={expected_feature_count}\")\n",
    "     except Exception as e:\n",
    "         print(f\"Could not retrieve 'n_features_in_': {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n[WARNING] Could not automatically retrieve expected features from the scaler object.\")\n",
    "    print(\"          Please manually verify feature names and order based on how the scaler was trained.\")\n",
    "    # Nếu là ColumnTransformer, cấu trúc của nó có thể giúp suy ra thứ tự\n",
    "    if hasattr(data_scaler, 'transformers_'):\n",
    "        print(\"          Since it might be a ColumnTransformer, inspect its 'transformers_' attribute:\")\n",
    "        try:\n",
    "            inferred_order = []\n",
    "            total_cols = 0\n",
    "            for name, trans, cols in data_scaler.transformers_:\n",
    "                 # 'cols' có thể là list tên hoặc list chỉ số\n",
    "                 print(f\"          - Step: '{name}', Columns specified: {cols[:15]}...\")\n",
    "                 # Chỉ thêm tên cột nếu nó là danh sách tên\n",
    "                 if all(isinstance(c, str) for c in cols):\n",
    "                     inferred_order.extend(cols)\n",
    "                 total_cols += len(cols) # Đếm tổng số cột được xử lý\n",
    "            print(f\"          - Total columns processed by transformers: {total_cols}\")\n",
    "            # Lưu ý: Cách này không hoàn hảo nếu có cột bị drop hoặc remainder='passthrough'\n",
    "        except Exception as inspect_err:\n",
    "            print(f\"          - Could not inspect 'transformers_': {inspect_err}\")\n",
    "\n",
    "\n",
    "print(\"--- End Comparing Feature Lists ---\")\n",
    "\n",
    "# --- Áp dụng scaler đã load ---\n",
    "print(\"\\nApplying loaded scaler to cleaned DAPT2020 features...\")\n",
    "\n",
    "# In thêm thông tin về scaler nếu là ColumnTransformer\n",
    "if hasattr(data_scaler, 'transformers_'):\n",
    "    print(\"\\nInspecting Scaler (ColumnTransformer) Structure:\")\n",
    "    try:\n",
    "        for name, trans, cols in data_scaler.transformers_:\n",
    "            print(f\"  - Step: '{name}', Transformer: {trans}, Columns: {cols[:15]}...\" if len(cols)>15 else cols) # Rút gọn nếu quá dài\n",
    "        if hasattr(data_scaler, 'feature_names_in_'):\n",
    "             print(f\"  - Scaler expected features ({len(data_scaler.feature_names_in_)}): {list(data_scaler.feature_names_in_)[:15]}...\")\n",
    "             print(f\"  - Actual X_dapt columns ({len(X_dapt.columns)}): {X_dapt.columns.tolist()[:15]}...\")\n",
    "             if list(data_scaler.feature_names_in_) != X_dapt.columns.tolist():\n",
    "                  print(\"  - WARNING: Actual column names/order DO NOT MATCH scaler's expected features!\")\n",
    "    except Exception as inspect_err:\n",
    "        print(f\"  - Could not fully inspect scaler structure: {inspect_err}\")\n",
    "\n",
    "try:\n",
    "    # Gọi transform trên X_dapt ĐÃ ĐƯỢC LÀM SẠCH KỸ\n",
    "    X_dapt_processed = data_scaler.transform(X_dapt)\n",
    "\n",
    "    print(f\"\\nScaling successful!\")\n",
    "    print(f\"Processed data shape after scaling: {X_dapt_processed.shape}\")\n",
    "    # Kiểm tra kiểu dữ liệu của output (thường là numpy array)\n",
    "    print(f\"Type of processed data: {type(X_dapt_processed)}\")\n",
    "    if isinstance(X_dapt_processed, np.ndarray):\n",
    "         print(f\"Processed data dtype: {X_dapt_processed.dtype}\") # Thường là float64\n",
    "         # Kiểm tra NaN/Inf trong kết quả cuối cùng (rất hiếm nhưng có thể)\n",
    "         if np.isnan(X_dapt_processed).any() or np.isinf(X_dapt_processed).any():\n",
    "              print(\"WARNING: NaN or Infinity found in the SCALED data!\")\n",
    "\n",
    "except TypeError as te: # Bắt cụ thể lỗi TypeError\n",
    "     print(f\"\\nFATAL ERROR during scaler.transform: {te}\")\n",
    "     print(\"This 'TypeError' strongly suggests a data type mismatch.\")\n",
    "     print(\"Even after cleaning, some column(s) likely have a type incompatible with the scaler.\")\n",
    "     print(\"Possible Causes:\")\n",
    "     print(\"  1. A column intended for numeric scaling is still 'object' or another non-numeric type.\")\n",
    "     print(\"  2. The scaler (especially ColumnTransformer) definition mismatches X_dapt columns (names, order, or expected type).\")\n",
    "     print(\"  3. A specific transformer within the scaler failed on its input.\")\n",
    "     print(\"Recommendations:\")\n",
    "     print(\"  - Carefully review the 'CRITICAL WARNING' about non-numeric columns above.\")\n",
    "     print(\"  - Double-check the FEATURE_COLUMNS list and the column reordering step.\")\n",
    "     print(\"  - Inspect the scaler structure printed above - do the columns match?\")\n",
    "     print(\"  - Consider retraining the scaler on data preprocessed IDENTICALLY to DAPT2020.\")\n",
    "     import traceback\n",
    "     print(\"\\n--- Traceback ---\")\n",
    "     print(traceback.format_exc())\n",
    "     print(\"--- End Traceback ---\")\n",
    "     exit() # Dừng lại vì không thể tiếp tục\n",
    "except ValueError as ve:\n",
    "     print(f\"\\nFATAL ERROR during scaler.transform: {ve}\")\n",
    "     print(\"This 'ValueError' often relates to incompatible values (e.g., NaN/Inf not handled) or incorrect number of features.\")\n",
    "     print(\"Recommendations:\")\n",
    "     print(\"  - Review the NaN/Inf counts printed just before scaling.\")\n",
    "     print(\"  - Ensure X_dapt has the exact number of columns the scaler expects.\")\n",
    "     import traceback\n",
    "     print(\"\\n--- Traceback ---\")\n",
    "     print(traceback.format_exc())\n",
    "     print(\"--- End Traceback ---\")\n",
    "     exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\nUNEXPECTED FATAL ERROR during scaler.transform: {e}\")\n",
    "    import traceback\n",
    "    print(\"\\n--- Traceback ---\")\n",
    "    print(traceback.format_exc())\n",
    "    print(\"--- End Traceback ---\")\n",
    "    exit()\n",
    "\n",
    "# --- Code tiếp theo (ví dụ: train_test_split) ---\n",
    "# Chỉ chạy nếu X_dapt_processed được tạo thành công\n",
    "if 'X_dapt_processed' in locals() and X_dapt_processed is not None:\n",
    "    print(\"\\nSplitting DAPT2020 data into train and test sets...\")\n",
    "    try:\n",
    "        X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "            X_dapt_processed,  # Dữ liệu đặc trưng đã được scale\n",
    "            y_dapt,            # Nhãn (0 hoặc 1)\n",
    "            test_size=0.2,     # Tỷ lệ dữ liệu cho tập kiểm tra (ví dụ: 20%)\n",
    "            random_state=42,   # Để đảm bảo kết quả chia có thể lặp lại\n",
    "            stratify=y_dapt    # Giữ nguyên tỷ lệ phân phối lớp\n",
    "        )\n",
    "        # ... (in kích thước và phân phối nhãn như code gốc) ...\n",
    "        print(f\"Shape of X_train_final: {X_train_final.shape}\")\n",
    "        print(f\"Shape of X_test_final: {X_test_final.shape}\")\n",
    "        print(f\"Train Label Distribution: {Counter(y_train_final)}\")\n",
    "        print(f\"Test Label Distribution: {Counter(y_test_final)}\")\n",
    "        print(\"Data splitting complete.\")\n",
    "\n",
    "    except Exception as split_error:\n",
    "        print(f\"An error occurred during train_test_split: {split_error}\")\n",
    "else:\n",
    "    print(\"\\nSkipping train_test_split because data scaling failed or X_dapt_processed was not created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa7efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying loaded scaler to DAPT2020 features...\n",
      "Error applying the loaded scaler: The feature names should match those that were passed during fit.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "Please ensure the loaded scaler is compatible with the shape and type of X_dapt.\n",
      "Check the original preprocessing steps and the columns in FEATURE_COLUMNS.\n",
      "Splitting DAPT2020 data into train and test sets...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_dapt_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# --- Chia tập dữ liệu ---\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSplitting DAPT2020 data into train and test sets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mX_dapt_processed\u001b[49m, y_dapt,\n\u001b[32m     18\u001b[39m     test_size=\u001b[32m0.2\u001b[39m,      \u001b[38;5;66;03m# 20% for testing (adjust as needed)\u001b[39;00m\n\u001b[32m     19\u001b[39m     random_state=\u001b[32m42\u001b[39m,    \u001b[38;5;66;03m# for reproducibility\u001b[39;00m\n\u001b[32m     20\u001b[39m     stratify=y_dapt     \u001b[38;5;66;03m# Stratify to maintain class distribution (crucial for imbalance)\u001b[39;00m\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain/Test shapes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_final.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test_final.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Label Distribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCounter(y_train_final)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_dapt_processed' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 3. Áp dụng scaler đã load cho các đặc trưng\n",
    "# SỬ DỤNG SCALER ĐÃ LOAD TỪ CIC-IDS2018 ĐỂ TRANSFORM DỮ LIỆU DAPT2020\n",
    "print(\"Applying loaded scaler to DAPT2020 features...\")\n",
    "try:\n",
    "    X_dapt_processed = data_scaler.transform(X_dapt)\n",
    "    print(f\"Processed data shape after scaling: {X_dapt_processed.shape}\")\n",
    "    print(\"DAPT2020 preprocessing complete using loaded scaler.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error applying the loaded scaler: {e}\")\n",
    "    print(\"Please ensure the loaded scaler is compatible with the shape and type of X_dapt.\")\n",
    "    print(\"Check the original preprocessing steps and the columns in FEATURE_COLUMNS.\")\n",
    "    exit()\n",
    "\n",
    "# --- Chia tập dữ liệu ---\n",
    "print(\"Splitting DAPT2020 data into train and test sets...\")\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_dapt_processed, y_dapt,\n",
    "    test_size=0.2,      # 20% for testing (adjust as needed)\n",
    "    random_state=42,    # for reproducibility\n",
    "    stratify=y_dapt     # Stratify to maintain class distribution (crucial for imbalance)\n",
    ")\n",
    "print(f\"Train/Test shapes: {X_train_final.shape} / {X_test_final.shape}\")\n",
    "print(f\"Train Label Distribution: {Counter(y_train_final)}\")\n",
    "print(f\"Test Label Distribution: {Counter(y_test_final)}\")\n",
    "\n",
    "\n",
    "# --- Xử lý mất cân bằng lớp (trên tập huấn luyện) ---\n",
    "# Tấn công APT (nhãn 1) có thể rất ít trong tập huấn luyện.\n",
    "# Có nhiều cách xử lý:\n",
    "# 1. Dùng class_weight trong mô hình (đối với RF)\n",
    "# 2. Dùng scale_pos_weight trong mô hình (đối với XGBoost binary classification)\n",
    "# 3. Áp dụng kỹ thuật lấy mẫu (sampling) như SMOTE, Undersampling (sử dụng thư viện imblearn)\n",
    "print(\"Handling class imbalance...\")\n",
    "\n",
    "# Phương án 1/2: Sử dụng trọng số lớp tích hợp trong mô hình\n",
    "# Tính toán trọng số lớp cho RF (sử dụng 'balanced' hoặc dict tùy chọn)\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# classes = np.unique(y_train_final)\n",
    "# weights = compute_class_weight('balanced', classes=classes, y=y_train_final)\n",
    "# class_weights_dict = dict(zip(classes, weights))\n",
    "# print(\"Computed class weights for RF:\", class_weights_dict) # Pass this dict to RF's fit method or use 'balanced'\n",
    "\n",
    "# Tính toán scale_pos_weight cho XGBoost (phân loại nhị phân 0/1)\n",
    "apt_count_train = sum(y_train_final == 1)\n",
    "benign_count_train = sum(y_train_final == 0)\n",
    "# Tránh chia cho 0 nếu không có mẫu APT trong tập huấn luyện (dù stratify cố gắng đảm bảo)\n",
    "scale_pos_weight_value = benign_count_train / apt_count_train if apt_count_train > 0 else 1\n",
    "print(f\"Computed scale_pos_weight for XGBoost: {scale_pos_weight_value}\") # Pass this value to XGBoost's fit method or set as parameter\n",
    "\n",
    "\n",
    "# Phương án 3 (Sử dụng imblearn - cần cài đặt `pip install imbalanced-learn`)\n",
    "print(\"Applying SMOTE on training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_final, y_train_final)\n",
    "print(f\"Resampled train shape: {X_train_resampled.shape}\")\n",
    "print(f\"Resampled train label distribution: {Counter(y_train_resampled)}\")\n",
    "# Use X_train_resampled, y_train_resampled for fitting instead of X_train_final, y_train_final\n",
    "\n",
    "\n",
    "# --- Huấn luyện lại Mô hình ---\n",
    "print(\"Retraining models on the new data...\")\n",
    "\n",
    "# Retrain Random Forest\n",
    "# Sử dụng lại cấu trúc/siêu tham số từ mô hình CIC-IDS2018 hoặc điều chỉnh nếu cần\n",
    "# Bạn có thể tạo lại đối tượng RF hoặc sử dụng lại rf_model_cicids và gọi .fit()\n",
    "# Để sử dụng lại rf_model_cicids và các siêu tham số cũ:\n",
    "rf_model_retrained = rf_model_cicids\n",
    "# Hoặc tạo mới với các siêu tham số mong muốn, CẦN XỬ LÝ class_weight ở đây hoặc khi tạo đối tượng\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rf_model_retrained = RandomForestClassifier(n_estimators=..., max_depth=..., random_state=42, class_weight='balanced' # or pass class_weights_dict)\n",
    "\n",
    "# Fit using the training data (original or resampled)\n",
    "# If using class_weight='balanced' in constructor, no need for sample_weight here\n",
    "rf_model_retrained.fit(X_train_final, y_train_final)\n",
    "# If using class_weight dict:\n",
    "# rf_model_retrained.fit(X_train_final, y_train_final, sample_weight=np.array([class_weights_dict[label] for label in y_train_final]))\n",
    "\n",
    "print(\"Random Forest retraining complete.\")\n",
    "\n",
    "# Retrain XGBoost\n",
    "# Sử dụng lại cấu trúc/siêu tham số từ mô hình CIC-IDS2018 hoặc điều chỉnh nếu cần\n",
    "# Bạn có thể tạo lại đối tượng XGBoost hoặc sử dụng lại xgb_model_cicids và gọi .fit()\n",
    "# Để sử dụng lại xgb_model_cicids và các siêu tham số cũ:\n",
    "xgb_model_retrained = xgb_model_cicids\n",
    "# Hoặc tạo mới với các siêu tham số mong muốn. CẦN XỬ LỬ scale_pos_weight ở đây hoặc khi tạo đối tượng\n",
    "# import xgboost as xgb\n",
    "# xgb_model_retrained = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, scale_pos_weight=scale_pos_weight_value, # other params)\n",
    "\n",
    "\n",
    "# Fit using the training data (original or resampled)\n",
    "# Pass scale_pos_weight for binary classification with imbalance, unless set in constructor\n",
    "xgb_model_retrained.fit(X_train_final, y_train_final, scale_pos_weight=scale_pos_weight_value)\n",
    "\n",
    "print(\"XGBoost retraining complete.\")\n",
    "\n",
    "\n",
    "# --- Đánh giá Mô hình mới ---\n",
    "print(\"\\nEvaluating retrained models on the test set...\")\n",
    "\n",
    "models_to_evaluate = {\n",
    "    \"Random Forest (Retrained)\": rf_model_retrained,\n",
    "    \"XGBoost (Retrained)\": xgb_model_retrained\n",
    "}\n",
    "\n",
    "for name, model in models_to_evaluate.items():\n",
    "    print(f\"\\n--- Evaluation for {name} ---\")\n",
    "\n",
    "    # Dự đoán trên tập kiểm tra\n",
    "    y_pred = model.predict(X_test_final)\n",
    "    # Dự đoán xác suất (để tính AUC)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "       y_pred_proba = model.predict_proba(X_test_final)[:, 1] # Probability of the positive class (APT=1)\n",
    "    else:\n",
    "       y_pred_proba = [0] * len(y_test_final) # Placeholder if no predict_proba\n",
    "\n",
    "\n",
    "    # In báo cáo phân loại chi tiết (Precision, Recall, F1-score)\n",
    "    print(\"Classification Report:\")\n",
    "    labels = [0, 1]\n",
    "    target_names = ['Benign', 'APT']\n",
    "    print(classification_report(y_test_final, y_pred, labels=labels, target_names=target_names, zero_division=0))\n",
    "\n",
    "\n",
    "    # In Ma trận nhầm lẫn\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test_final, y_pred, labels=labels)) # Ensure order of labels\n",
    "\n",
    "    # Tính và in các metrics quan trọng cho dữ liệu mất cân bằng\n",
    "    accuracy = accuracy_score(y_test_final, y_pred)\n",
    "    precision = precision_score(y_test_final, y_pred, pos_label=1, zero_division=0) # Precision for APT (positive class)\n",
    "    recall = recall_score(y_test_final, y_pred, pos_label=1, zero_division=0)       # Recall for APT (positive class)\n",
    "    f1 = f1_score(y_test_final, y_pred, pos_label=1, zero_division=0)         # F1-score for APT (positive class)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (APT=1): {precision:.4f}\")\n",
    "    print(f\"Recall (APT=1): {recall:.4f}\")\n",
    "    print(f\"F1-score (APT=1): {f1:.4f}\")\n",
    "\n",
    "    # Tính và in AUC-ROC\n",
    "    try:\n",
    "        if len(np.unique(y_test_final)) > 1:\n",
    "            roc_auc = roc_auc_score(y_test_final, y_pred_proba)\n",
    "            print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "        else:\n",
    "             print(\"AUC-ROC cannot be calculated as only one class is present in test labels.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate AUC-ROC: {e}\")\n",
    "\n",
    "\n",
    "# --- Lưu Mô hình đã huấn luyện lại ---\n",
    "print(\"\\nSaving retrained models...\")\n",
    "NEW_RF_MODEL_FILE = 'rf_dapt2020_apt_retrained.pkl'\n",
    "NEW_XGB_MODEL_FILE = 'xgb_dapt2020_apt_retrained.pkl'\n",
    "\n",
    "try:\n",
    "    joblib.dump(rf_model_retrained, os.path.join(PATH_TO_FILES, NEW_RF_MODEL_FILE))\n",
    "    joblib.dump(xgb_model_retrained, os.path.join(PATH_TO_FILES, NEW_XGB_MODEL_FILE))\n",
    "    print(f\"Retrained models saved as {NEW_RF_MODEL_FILE} and {NEW_XGB_MODEL_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving models: {e}\")\n",
    "\n",
    "print(\"\\nFine-tuning process (retraining) complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
